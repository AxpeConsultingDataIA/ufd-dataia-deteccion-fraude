{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abbaa0dc-5ede-415d-a210-4d421d21bbd9",
   "metadata": {},
   "source": [
    "# 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5084a81a-243f-47ad-b7a3-0c2f42e26c8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (5.28.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from neo4j) (2024.1)\n",
      "Requirement already satisfied: torch in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: torch_geometric in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch_geometric) (3.11.10)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch_geometric) (2025.3.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch_geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch_geometric) (3.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch_geometric) (2.32.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp->torch_geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch_geometric) (3.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from requests->torch_geometric) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from requests->torch_geometric) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from requests->torch_geometric) (2025.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->torch_geometric) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install neo4j\n",
    "!pip install torch\n",
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a9af8b-d261-456d-959c-d5f1e7b0ab9b",
   "metadata": {},
   "source": [
    "# 2. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbdcc8bb-cfac-432d-8c07-8571b15d198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "import logging\n",
    "from datetime import datetime, timedelta, date\n",
    "import random\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "import os\n",
    "from neo4j.time import Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7ddd7de-f4e9-41d9-aed8-501e08f26710",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0138ae9e-4040-4ff5-99a4-15748514a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c718520-2ce9-4d3b-b98f-a240183d0bb2",
   "metadata": {},
   "source": [
    "# 3. Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b4b2c4-a3c8-41bc-8b64-ff344a404ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = \"neo4j://localhost:7687\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"password\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2fe2c15-5acf-467c-89de-92adf8ad6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff0660c-cbeb-445d-9f88-f82de5460bf0",
   "metadata": {},
   "source": [
    "# 4. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b57e12b0-d347-4adb-b668-2461419062c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_neo4j_date(x):\n",
    "    if isinstance(x, Date):\n",
    "        return date(x.year, x.month, x.day)\n",
    "    elif isinstance(x, datetime):\n",
    "        return x.date()\n",
    "    elif isinstance(x, date):\n",
    "        return x\n",
    "    else:\n",
    "        return pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "471b719a-a9c6-4e0d-87cd-22c73dd25d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heterogeneous_results(history, test_results, attention_analysis):\n",
    "    \"\"\"\n",
    "    Visualiza los resultados del modelo heterogéneo\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Historial de entrenamiento\n",
    "    axes[0, 0].plot(history['train_losses'], label='Train Loss', alpha=0.7)\n",
    "    axes[0, 0].plot(history['val_losses'], label='Val Loss', alpha=0.7)\n",
    "    axes[0, 0].set_title('Pérdida durante el entrenamiento')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracies\n",
    "    axes[0, 1].plot(history['train_accuracies'], label='Train Accuracy', alpha=0.7)\n",
    "    axes[0, 1].plot(history['val_accuracies'], label='Val Accuracy', alpha=0.7)\n",
    "    axes[0, 1].set_title('Precisión durante el entrenamiento')\n",
    "    axes[0, 1].set_xlabel('Época')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Matriz de confusión\n",
    "    sns.heatmap(test_results['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Fraude', 'Irregularidad'],\n",
    "                yticklabels=['Normal', 'Fraude', 'Irregularidad'], ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Matriz de Confusión')\n",
    "    axes[1, 0].set_xlabel('Predicción')\n",
    "    axes[1, 0].set_ylabel('Verdadero')\n",
    "    \n",
    "    # 3. Pesos de atención\n",
    "    if attention_analysis:\n",
    "        info_types = list(attention_analysis.keys())\n",
    "        weights = list(attention_analysis.values())\n",
    "        \n",
    "        bars = axes[1, 1].bar(range(len(info_types)), weights, alpha=0.7, color='skyblue')\n",
    "        axes[1, 1].set_title('Pesos de atención por tipo de información')\n",
    "        axes[1, 1].set_xlabel('Tipo de información')\n",
    "        axes[1, 1].set_ylabel('Peso de atención')\n",
    "        axes[1, 1].set_xticks(range(len(info_types)))\n",
    "        axes[1, 1].set_xticklabels(info_types, rotation=45, ha='right')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Agregar valores a las barras\n",
    "        for bar, weight in zip(bars, weights):\n",
    "            height = bar.get_height()\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                            f'{weight:.3f}', ha='center', va='bottom')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No hay análisis de atención disponible', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('Análisis de Atención')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a9eb43-7da9-40c8-b182-7530577a9fb3",
   "metadata": {},
   "source": [
    "# 5. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787ea602-b4b3-4a20-826c-44cdd2c7334c",
   "metadata": {},
   "source": [
    "## 5.1. Load data from Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f218b0-b73a-46bb-a87e-0406392fe152",
   "metadata": {},
   "source": [
    "We will be writing the code in order to load the data from neo4j when the time comes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "852aeab5-b980-409b-8901-8dbb84e0ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jDataLoader:\n",
    "    \"\"\"\n",
    "    Loads heterogeneous data from Neo4j.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, uri: str, username: str, password: str):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Used to close the connection\n",
    "        \"\"\"\n",
    "        self.driver.close()\n",
    "    \n",
    "    def load_heterogeneous_data(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Loads all the heterogeneous data from Neo4j. The idea is to use all the defined functions for each node.\n",
    "        \"\"\"\n",
    "        print(\"🔌 Connecting to Neo4j and loading data ...\")\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            # Load each type of node\n",
    "            node_data = {\n",
    "                'contador': self._load_contador_nodes(session),\n",
    "                'suministro': self._load_suministro_nodes(session),\n",
    "                'comercializadora': self._load_comercializadora_nodes(session),\n",
    "                'ubicacion': self._load_ubicacion_nodes(session),\n",
    "                'concentrador': self._load_concentrador_nodes(session),\n",
    "                'expediente_fraude': self._load_expediente_fraude_nodes(session)\n",
    "            }\n",
    "            \n",
    "            # Load relations\n",
    "            edge_data = self._load_heterogeneous_edges(session, node_data)\n",
    "            \n",
    "            # Assign fraud labels based on expedientes.\n",
    "            node_data['contador'] = self._assign_fraud_labels_from_expedientes(\n",
    "                node_data['contador'], node_data['expediente_fraude'], session\n",
    "            )\n",
    "        \n",
    "        # Let's show now some statistics of the graph\n",
    "        total_nodes = sum(len(nodes) for nodes in node_data.values())\n",
    "        total_edges = sum(len(edges) for edges in edge_data.values())\n",
    "        print(f\"✅ Cargados {total_nodes} nodos y {total_edges} relaciones desde Neo4j\")\n",
    "        \n",
    "        for node_type, df in node_data.items():\n",
    "            print(f\"   - {node_type}: {len(df)} nodos\")\n",
    "        \n",
    "        return {\n",
    "            'nodes': node_data,\n",
    "            'edges': edge_data\n",
    "        }\n",
    "    \n",
    "    def _load_contador_nodes(self, session) -> pd.DataFrame:\n",
    "        \"\"\"Loads CONTADOR nodes from Neo4j\"\"\"\n",
    "        \n",
    "        query = \"\"\"\n",
    "        MATCH (c:CONTADOR)\n",
    "        OPTIONAL MATCH (c)-[:GENERA_MEDICION]->(m:MEDICION)\n",
    "        WITH c, \n",
    "             COUNT(m) as num_mediciones,\n",
    "             AVG(m.energia_activa) as consumo_promedio,\n",
    "             MAX(m.energia_activa) as consumo_maximo,\n",
    "             MIN(m.energia_activa) as consumo_minimo,\n",
    "             STDEV(m.energia_activa) as variabilidad_consumo\n",
    "        RETURN c.nis_rad as nis_rad,\n",
    "               c.numero_contador as numero_contador,\n",
    "               c.marca_contador as marca,\n",
    "               c.modelo_contador as modelo,\n",
    "               c.tipo_aparato as tipo_aparato,\n",
    "               c.telegest_activo as telegest_activo,\n",
    "               c.estado_tg as estado_comunicacion,\n",
    "               c.tension as tension_nominal,\n",
    "               c.fases_contador as fases,\n",
    "               c.potencia_maxima as potencia_maxima,\n",
    "               c.fecha_instalacion as fecha_instalacion,\n",
    "               c.version_firmware as version_firmware,\n",
    "               c.estado_contrato as estado_contrato,\n",
    "               COALESCE(num_mediciones, 0) as num_mediciones,\n",
    "               COALESCE(consumo_promedio, 0) as consumo_promedio_diario,\n",
    "               COALESCE(consumo_maximo, 0) as consumo_maximo_registrado,\n",
    "               COALESCE(consumo_minimo, 0) as consumo_minimo_registrado,\n",
    "               COALESCE(variabilidad_consumo, 0) as variabilidad_consumo\n",
    "        ORDER BY c.nis_rad\n",
    "        \"\"\"\n",
    "        # Run the query just defined:\n",
    "        result = session.run(query)\n",
    "        records = [dict(record) for record in result]\n",
    "        \n",
    "        if not records:\n",
    "            print(\"⚠️ We couldn't find CONTADORES.\")\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        \n",
    "        # Add calculated features\n",
    "        df[\"fecha_instalacion\"] = df[\"fecha_instalacion\"].map(convert_neo4j_date)\n",
    "        df[\"fecha_instalacion\"] = pd.to_datetime(df[\"fecha_instalacion\"])\n",
    "\n",
    "        # Calcula los días como enteros\n",
    "        df[\"dias_desde_instalacion\"] = (\n",
    "            pd.Timestamp.now().normalize() - df[\"fecha_instalacion\"]\n",
    "        ).dt.days.fillna(0)\n",
    "        \n",
    "        # Let's create a unique ID for the mapping.\n",
    "        df['node_id'] = df['nis_rad']\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _load_suministro_nodes(self, session) -> pd.DataFrame:\n",
    "        \"\"\"Loads SUMINISTRO nodes from Neo4j\"\"\"\n",
    "        \n",
    "        query = \"\"\"\n",
    "        MATCH (s:SUMINISTRO)\n",
    "        RETURN s.nis_rad as nis_rad,\n",
    "               s.fecha_alta_suministro as fecha_alta,\n",
    "               s.estado_contrato as estado_contrato,\n",
    "               s.tipo_punto as tipo_suministro,\n",
    "               s.potencia_contratada as potencia_contratada,\n",
    "               s.potencia_maxima as potencia_maxima_demandada,\n",
    "               s.tarifa_activa as tarifa_activa,\n",
    "               s.tension_suministro as tension_suministro,\n",
    "               s.fases_suministro as fases_suministro,\n",
    "               s.cnae as cnae,\n",
    "               s.comercializadora_codigo as comercializadora_codigo\n",
    "        ORDER BY s.nis_rad\n",
    "        \"\"\"\n",
    "        \n",
    "        result = session.run(query)\n",
    "        records = [dict(record) for record in result]\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        df['node_id'] = df['nis_rad']\n",
    "        \n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def _load_comercializadora_nodes(self, session) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads COMERCIALIZADORA nodes from Neo4j\n",
    "        \"\"\"\n",
    "        \n",
    "        query = \"\"\"\n",
    "        MATCH (c:COMERCIALIZADORA)\n",
    "        RETURN c.codigo_comercializadora as codigo_comercializadora,\n",
    "               c.nombre_comercializadora as nombre_comercializadora\n",
    "        ORDER BY c.codigo_comercializadora\n",
    "        \"\"\"\n",
    "        \n",
    "        result = session.run(query)\n",
    "        records = [dict(record) for record in result]\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        df['node_id'] = df['codigo_comercializadora']\n",
    "        \n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def _load_ubicacion_nodes(self, session) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads UBICACION nodes from Neo4j\n",
    "        \"\"\"\n",
    "        \n",
    "        query = \"\"\"\n",
    "        MATCH (u:UBICACION)\n",
    "        RETURN u.coordenada_x as coordenada_x,\n",
    "               u.coordenada_y as coordenada_y,\n",
    "               u.codigo_postal as codigo_postal,\n",
    "               u.area_ejecucion as area_ejecucion,\n",
    "               toString(u.coordenada_x) + '_' + toString(u.coordenada_y) as node_id\n",
    "        ORDER BY u.coordenada_x, u.coordenada_y\n",
    "        \"\"\"\n",
    "        \n",
    "        result = session.run(query)\n",
    "        records = [dict(record) for record in result]\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        \n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def _load_concentrador_nodes(self, session) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads CONCENTRADOR nodes from Neo4j\n",
    "        \"\"\"\n",
    "        \n",
    "        query = \"\"\"\n",
    "        MATCH (c:CONCENTRADOR)\n",
    "        RETURN c.concentrador_id as concentrador_id,\n",
    "               c.version_concentrador as version_concentrador,\n",
    "               c.estado_comunicacion as estado_comunicacion,\n",
    "               c.tipo_reporte as tipo_reporte\n",
    "        ORDER BY c.concentrador_id\n",
    "        \"\"\"\n",
    "        \n",
    "        result = session.run(query)\n",
    "        records = [dict(record) for record in result]\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        df['node_id'] = df['concentrador_id']\n",
    "        \n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def _load_expediente_fraude_nodes(self, session) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads EXPEDIENTE_FRAUDE nodes from Neo4j\n",
    "        \"\"\"\n",
    "        \n",
    "        query = \"\"\"\n",
    "        MATCH (e:EXPEDIENTE_FRAUDE)\n",
    "        RETURN e.nis_expediente as nis_expediente,\n",
    "               e.clasificacion_fraude as clasificacion_fraude,\n",
    "               e.tipo_anomalia as tipo_anomalia,\n",
    "               e.estado_expediente as estado_expediente,\n",
    "               e.fecha_acta as fecha_acta,\n",
    "               e.fecha_inicio_anomalia as fecha_inicio_anomalia,\n",
    "               e.fecha_fin_anomalia as fecha_fin_anomalia,\n",
    "               e.energia_liquidable as energia_liquidable,\n",
    "               e.valoracion_total as valoracion_total,\n",
    "               e.dias_liquidables as dias_liquidables,\n",
    "               e.porcentaje_liquidable as porcentaje_liquidable\n",
    "        ORDER BY e.nis_expediente\n",
    "        \"\"\"\n",
    "        \n",
    "        result = session.run(query)\n",
    "        records = [dict(record) for record in result]\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        if not df.empty:\n",
    "            df['node_id'] = df['nis_expediente']\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _load_heterogeneous_edges(self, session, node_data: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Loads all the heterogeneous relations from Neo4j\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"🔗 Loading relations from Neo4j...\")\n",
    "        \n",
    "        edge_data = {}\n",
    "        \n",
    "        # 1. CONTADOR -> SUMINISTRO (relation MIDE_CONSUMO_DE)\n",
    "        edge_data[('contador', 'mide', 'suministro')] = self._load_edges_from_query(\n",
    "            session,\n",
    "            \"\"\"\n",
    "            MATCH (c:CONTADOR)-[:MIDE_CONSUMO_DE]->(s:SUMINISTRO)\n",
    "            RETURN c.nis_rad as source, s.nis_rad as target\n",
    "            \"\"\",\n",
    "            node_data['contador'], node_data['suministro']\n",
    "        )\n",
    "        \n",
    "        # 2. CONTADOR -> CONCENTRADOR (relation CONECTADO_A)\n",
    "        edge_data[('contador', 'comunica_via', 'concentrador')] = self._load_edges_from_query(\n",
    "            session,\n",
    "            \"\"\"\n",
    "            MATCH (c:CONTADOR)-[:CONECTADO_A]->(con:CONCENTRADOR)\n",
    "            RETURN c.nis_rad as source, con.concentrador_id as target\n",
    "            \"\"\",\n",
    "            node_data['contador'], node_data['concentrador']\n",
    "        )\n",
    "        \n",
    "        # 3. CONTADOR -> UBICACION (relation INSTALADO_EN)\n",
    "        edge_data[('contador', 'ubicado_en', 'ubicacion')] = self._load_edges_from_query(\n",
    "            session,\n",
    "            \"\"\"\n",
    "            MATCH (c:CONTADOR)-[:INSTALADO_EN]->(u:UBICACION)\n",
    "            RETURN c.nis_rad as source, \n",
    "                   toString(u.coordenada_x) + '_' + toString(u.coordenada_y) as target\n",
    "            \"\"\",\n",
    "            node_data['contador'], node_data['ubicacion']\n",
    "        )\n",
    "        \n",
    "        # 4. CONTADOR -> EXPEDIENTE_FRAUDE (relation INVOLUCRADO_EN_FRAUDE)\n",
    "        if not node_data['expediente_fraude'].empty:\n",
    "            edge_data[('contador', 'involucrado_en', 'expediente_fraude')] = self._load_edges_from_query(\n",
    "                session,\n",
    "                \"\"\"\n",
    "                MATCH (c:CONTADOR)-[:INVOLUCRADO_EN_FRAUDE]->(e:EXPEDIENTE_FRAUDE)\n",
    "                RETURN c.nis_rad as source, e.nis_expediente as target\n",
    "                \"\"\",\n",
    "                node_data['contador'], node_data['expediente_fraude']\n",
    "            )\n",
    "        \n",
    "        # 5. SUMINISTRO -> COMERCIALIZADORA (basado en código comercializadora)\n",
    "        edge_data[('suministro', 'contratado_con', 'comercializadora')] = self._create_suministro_comercializadora_edges(\n",
    "            node_data['suministro'], node_data['comercializadora']\n",
    "        )\n",
    "        \n",
    "        # 6. Proximity relations between CONTADORES (These ones are simulated)\n",
    "        edge_data[('contador', 'cerca_de', 'contador')] = self._create_contador_proximidad_edges(\n",
    "            node_data['contador'], node_data['ubicacion']\n",
    "        )\n",
    "        \n",
    "        # 7. Similar CONTADORES in terms of MARCA/MODELO\n",
    "        edge_data[('contador', 'similar_a', 'contador')] = self._create_contador_similar_edges(\n",
    "            node_data['contador']\n",
    "        )\n",
    "        \n",
    "        return edge_data\n",
    "    \n",
    "    def _load_edges_from_query(self, session, query: str, source_df: pd.DataFrame, \n",
    "                             target_df: pd.DataFrame) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Carga relaciones desde Neo4j usando una query específica\n",
    "        \"\"\"\n",
    "        \n",
    "        result = session.run(query)\n",
    "        edges = []\n",
    "        \n",
    "        # Crear mapeos de node_id a índice\n",
    "        source_map = {node_id: idx for idx, node_id in enumerate(source_df['node_id'])}\n",
    "        target_map = {node_id: idx for idx, node_id in enumerate(target_df['node_id'])}\n",
    "        \n",
    "        for record in result:\n",
    "            source_id = record['source']\n",
    "            target_id = record['target']\n",
    "            \n",
    "            if source_id in source_map and target_id in target_map:\n",
    "                source_idx = source_map[source_id]\n",
    "                target_idx = target_map[target_id]\n",
    "                edges.append([source_idx, target_idx])\n",
    "        \n",
    "        # If there are no relations in the db, we create some basic ones for testing\n",
    "        if not edges and len(source_df) > 0 and len(target_df) > 0:\n",
    "            print(f\"   ⚠️ We couldn't find real relations, creating test relations...\")\n",
    "            for i in range(min(len(source_df), len(target_df))):\n",
    "                target_idx = i % len(target_df)\n",
    "                edges.append([i, target_idx])\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def _create_suministro_comercializadora_edges(self, suministros: pd.DataFrame, \n",
    "                                                comercializadoras: pd.DataFrame) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Creates relations SUMINISTRO -> COMERCIALIZADORA based on the code\n",
    "        \"\"\"\n",
    "        \n",
    "        edges = []\n",
    "        com_map = {row['codigo_comercializadora']: idx \n",
    "                  for idx, row in comercializadoras.iterrows()}\n",
    "        \n",
    "        for idx, row in suministros.iterrows():\n",
    "            com_codigo = row.get('comercializadora_codigo', 'COM_001')\n",
    "            if com_codigo in com_map:\n",
    "                edges.append([idx, com_map[com_codigo]])\n",
    "            else:\n",
    "                # Assign randomly if the COMERCIALIZADORA does not exist\n",
    "                com_idx = np.random.randint(0, len(comercializadoras))\n",
    "                edges.append([idx, com_idx])\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def _create_contador_proximidad_edges(self, contadores: pd.DataFrame, \n",
    "                                        ubicaciones: pd.DataFrame) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Crea relaciones de proximidad entre contadores\n",
    "        \"\"\"\n",
    "        \n",
    "        edges = []\n",
    "        # Simulamos proximidad: cada contador se conecta con 2-4 vecinos\n",
    "        for i in range(len(contadores)):\n",
    "            num_neighbors = np.random.randint(2, 5)\n",
    "            for _ in range(num_neighbors):\n",
    "                neighbor_idx = np.random.randint(0, len(contadores))\n",
    "                if neighbor_idx != i:\n",
    "                    edges.append([i, neighbor_idx])\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def _create_contador_similar_edges(self, contadores: pd.DataFrame) -> List[List[int]]:\n",
    "        \"\"\"Crea relaciones entre contadores similares (misma marca/modelo)\"\"\"\n",
    "        \n",
    "        edges = []\n",
    "        \n",
    "        # Agrupar por marca y modelo\n",
    "        for marca in contadores['marca'].unique():\n",
    "            for modelo in contadores['modelo'].unique():\n",
    "                subset_indices = contadores[\n",
    "                    (contadores['marca'] == marca) & (contadores['modelo'] == modelo)\n",
    "                ].index.tolist()\n",
    "                \n",
    "                # Conectar contadores del mismo tipo\n",
    "                for i in range(len(subset_indices)):\n",
    "                    for j in range(i + 1, min(i + 4, len(subset_indices))):\n",
    "                        edges.append([subset_indices[i], subset_indices[j]])\n",
    "                        edges.append([subset_indices[j], subset_indices[i]])\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def _assign_fraud_labels_from_expedientes(self, contadores: pd.DataFrame, \n",
    "                                           expedientes: pd.DataFrame, session) -> pd.DataFrame:\n",
    "        \"\"\"Asigna etiquetas de fraude basadas en expedientes reales\"\"\"\n",
    "        \n",
    "        contadores = contadores.copy()\n",
    "        \n",
    "        # Inicializar todas las etiquetas como NORMAL\n",
    "        contadores['label'] = 'NORMAL'\n",
    "        \n",
    "        if not expedientes.empty:\n",
    "            # Obtener relaciones contador -> expediente\n",
    "            query = \"\"\"\n",
    "            MATCH (c:CONTADOR)-[:INVOLUCRADO_EN_FRAUDE]->(e:EXPEDIENTE_FRAUDE)\n",
    "            RETURN c.nis_rad as contador_nis, e.clasificacion_fraude as clasificacion\n",
    "            \"\"\"\n",
    "            \n",
    "            result = session.run(query)\n",
    "            fraud_relations = [dict(record) for record in result]\n",
    "            \n",
    "            # Crear mapeo de NIS a índice\n",
    "            nis_to_idx = {row['nis_rad']: idx for idx, row in contadores.iterrows()}\n",
    "            \n",
    "            # Asignar etiquetas basadas en clasificación de fraude\n",
    "            fraud_count = 0\n",
    "            irregularity_count = 0\n",
    "            \n",
    "            for relation in fraud_relations:\n",
    "                contador_nis = relation['contador_nis']\n",
    "                clasificacion = relation['clasificacion']\n",
    "                \n",
    "                if contador_nis in nis_to_idx:\n",
    "                    idx = nis_to_idx[contador_nis]\n",
    "                    \n",
    "                    if clasificacion and 'FRAUDE' in clasificacion.upper():\n",
    "                        contadores.loc[idx, 'label'] = 'FRAUDE'\n",
    "                        fraud_count += 1\n",
    "                    elif clasificacion and any(term in clasificacion.upper() \n",
    "                                             for term in ['IRREGULARIDAD', 'ANOMALIA']):\n",
    "                        contadores.loc[idx, 'label'] = 'IRREGULARIDAD'\n",
    "                        irregularity_count += 1\n",
    "            \n",
    "            normal_count = len(contadores) - fraud_count - irregularity_count\n",
    "            print(f\"✅ Etiquetas desde BD: {normal_count} Normal, {fraud_count} Fraude, {irregularity_count} Irregularidad\")\n",
    "        \n",
    "        else:\n",
    "            # Si no hay expedientes, simular algunas etiquetas para prueba\n",
    "            print(\"⚠️ No se encontraron expedientes de fraude, simulando etiquetas...\")\n",
    "            num_fraudes = max(1, int(len(contadores) * 0.05))  # 5% fraudes\n",
    "            num_irregularidades = max(1, int(len(contadores) * 0.03))  # 3% irregularidades\n",
    "            \n",
    "            fraud_indices = np.random.choice(len(contadores), num_fraudes, replace=False)\n",
    "            remaining_indices = [i for i in range(len(contadores)) if i not in fraud_indices]\n",
    "            irregularity_indices = np.random.choice(remaining_indices, num_irregularidades, replace=False)\n",
    "            \n",
    "            for idx in fraud_indices:\n",
    "                contadores.loc[idx, 'label'] = 'FRAUDE'\n",
    "            \n",
    "            for idx in irregularity_indices:\n",
    "                contadores.loc[idx, 'label'] = 'IRREGULARIDAD'\n",
    "            \n",
    "            normal_count = len(contadores) - num_fraudes - num_irregularidades\n",
    "            print(f\"✅ Etiquetas simuladas: {normal_count} Normal, {num_fraudes} Fraude, {num_irregularidades} Irregularidad\")\n",
    "        \n",
    "        return contadores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3afc1fa-8c41-41ce-83b4-df3dc53ff4ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.2. Simualte the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a5635a-a75d-44dd-9afc-f7da9a6845bb",
   "metadata": {},
   "source": [
    "We will use a simulator of data as done in the GNN_dummy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "884f0f9f-52eb-4a48-85c9-549b0cbb24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousDataSimulator:\n",
    "    \"\"\"\n",
    "    Simulate the data for an heterogeneous complete graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        random.seed(RANDOM_SEED)\n",
    "        \n",
    "    def generate_heterogeneous_data(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Generates a complete heterogeneous dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"🎲 Generando datos heterogéneos complejos...\")\n",
    "        \n",
    "        # We generate nodes for each type:\n",
    "        node_data = {\n",
    "            'contador': self._generate_contador_nodes(),\n",
    "            'suministro': self._generate_suministro_nodes(),\n",
    "            'comercializadora': self._generate_comercializadora_nodes(),\n",
    "            'ubicacion': self._generate_ubicacion_nodes(),\n",
    "            'concentrador': self._generate_concentrador_nodes(),\n",
    "            'cliente': self._generate_cliente_nodes(),\n",
    "            'transformador': self._generate_transformador_nodes(),\n",
    "            'zona': self._generate_zona_nodes()\n",
    "        }\n",
    "        \n",
    "        # We generate heterogeneous relations also making use of the _generate_heterogeneous_edges function:\n",
    "        edge_data = self._generate_heterogeneous_edges(node_data)\n",
    "        \n",
    "        # We add labels of fraud to each contador.\n",
    "        node_data['contador'] = self._assign_fraud_labels(node_data['contador'])\n",
    "\n",
    "        # Show how many nodes and relations we have:\n",
    "        print(f\"✅ Generados {sum(len(nodes) for nodes in node_data.values())} nodos\")\n",
    "        print(f\"✅ Generadas {sum(len(edges) for edges in edge_data.values())} relaciones\")\n",
    "        \n",
    "        return {\n",
    "            'nodes': node_data,\n",
    "            'edges': edge_data\n",
    "        }\n",
    "    \n",
    "    def _generate_contador_nodes(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generates contador nodes with detailed features.\n",
    "        \"\"\"\n",
    "        \n",
    "        data = {\n",
    "            'node_id': [f\"CNT_{i:06d}\" for i in range(NUM_CONTADORES_ADV)],\n",
    "            \n",
    "            # Características técnicas\n",
    "            'marca': np.random.choice(['LANDIS', 'ITRON', 'CIRCUTOR', 'SCHNEIDER', 'ABB'], NUM_CONTADORES_ADV),\n",
    "            'modelo': np.random.choice(['E350', 'E450', 'A1140', 'A1800', 'B23'], NUM_CONTADORES_ADV),\n",
    "            'tipo_aparato': np.random.choice(['ELECTRONICO', 'ELECTROMECANICO', 'HIBRIDO'], \n",
    "                                           NUM_CONTADORES_ADV, p=[0.7, 0.2, 0.1]),\n",
    "            'numero_serie': [f\"SN{random.randint(100000, 999999)}\" for _ in range(NUM_CONTADORES_ADV)],\n",
    "            'fecha_fabricacion': pd.to_datetime([\n",
    "                datetime.now() - timedelta(days=random.randint(30, 3650)) \n",
    "                for _ in range(NUM_CONTADORES_ADV)\n",
    "            ]),\n",
    "            'fecha_instalacion': pd.to_datetime([\n",
    "                datetime.now() - timedelta(days=random.randint(1, 1825)) \n",
    "                for _ in range(NUM_CONTADORES_ADV)\n",
    "            ]),\n",
    "            \n",
    "            # Características eléctricas\n",
    "            'potencia_maxima': np.random.lognormal(8.5, 0.8, NUM_CONTADORES_ADV),\n",
    "            'tension_nominal': np.random.choice([230, 400], NUM_CONTADORES_ADV, p=[0.6, 0.4]),\n",
    "            'fases': np.random.choice([1, 3], NUM_CONTADORES_ADV, p=[0.7, 0.3]),\n",
    "            'clase_precision': np.random.choice([1, 2, 0.5], NUM_CONTADORES_ADV, p=[0.6, 0.3, 0.1]),\n",
    "            'intensidad_maxima': np.random.lognormal(3.0, 0.5, NUM_CONTADORES_ADV),\n",
    "            \n",
    "            # Telegestión y comunicaciones\n",
    "            'telegest_activo': np.random.choice([True, False], NUM_CONTADORES_ADV, p=[0.85, 0.15]),\n",
    "            'protocolo_comunicacion': np.random.choice(['PLC', 'RF', 'GPRS', 'ETHERNET'], \n",
    "                                                     NUM_CONTADORES_ADV, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "            'version_firmware': [f\"v{random.randint(1,5)}.{random.randint(0,9)}.{random.randint(0,9)}\" \n",
    "                               for _ in range(NUM_CONTADORES_ADV)],\n",
    "            'estado_comunicacion': np.random.choice(['ACTIVO', 'INTERMITENTE', 'PERDIDO'], \n",
    "                                                  NUM_CONTADORES_ADV, p=[0.8, 0.15, 0.05]),\n",
    "            \n",
    "            # Métricas operacionales\n",
    "            'dias_desde_instalacion': [(datetime.now() - pd.to_datetime([\n",
    "                datetime.now() - timedelta(days=random.randint(1, 1825)) \n",
    "                for _ in range(NUM_CONTADORES_ADV)\n",
    "            ])[i]).days for i in range(NUM_CONTADORES_ADV)],\n",
    "            'num_reinicios': np.random.poisson(2, NUM_CONTADORES_ADV),\n",
    "            'eventos_alarma': np.random.poisson(1, NUM_CONTADORES_ADV),\n",
    "            'calidad_senal': np.random.beta(8, 2, NUM_CONTADORES_ADV) * 100,\n",
    "            \n",
    "            # Consumo y patrones\n",
    "            'consumo_promedio_diario': np.random.lognormal(3.5, 1.2, NUM_CONTADORES_ADV),\n",
    "            'consumo_maximo_registrado': np.random.lognormal(4.0, 1.0, NUM_CONTADORES_ADV),\n",
    "            'consumo_minimo_registrado': np.random.exponential(5, NUM_CONTADORES_ADV),\n",
    "            'variabilidad_consumo': np.random.exponential(20, NUM_CONTADORES_ADV),\n",
    "            'patron_horario': np.random.choice(['RESIDENCIAL', 'COMERCIAL', 'INDUSTRIAL'], \n",
    "                                             NUM_CONTADORES_ADV, p=[0.6, 0.3, 0.1]),\n",
    "            'dias_sin_consumo': np.random.poisson(3, NUM_CONTADORES_ADV),\n",
    "            'picos_consumo_anormales': np.random.poisson(0.5, NUM_CONTADORES_ADV),\n",
    "            \n",
    "            # Mantenimiento y incidencias\n",
    "            'ultima_inspeccion': pd.to_datetime([\n",
    "                datetime.now() - timedelta(days=random.randint(0, 730)) \n",
    "                for _ in range(NUM_CONTADORES_ADV)\n",
    "            ]),\n",
    "            'resultado_ultima_inspeccion': np.random.choice(['CORRECTO', 'ANOMALIA_MENOR', 'ANOMALIA_MAYOR'], \n",
    "                                                          NUM_CONTADORES_ADV, p=[0.85, 0.12, 0.03]),\n",
    "            'num_averias': np.random.poisson(0.3, NUM_CONTADORES_ADV),\n",
    "            'dias_fuera_servicio': np.random.poisson(1, NUM_CONTADORES_ADV),\n",
    "            \n",
    "            # Indicadores de riesgo\n",
    "            'score_fiabilidad': np.random.beta(7, 3, NUM_CONTADORES_ADV) * 100,\n",
    "            'anomalias_detectadas': np.random.poisson(0.8, NUM_CONTADORES_ADV),\n",
    "            'tendencia_consumo': np.random.choice(['ESTABLE', 'CRECIENTE', 'DECRECIENTE'], \n",
    "                                                NUM_CONTADORES_ADV, p=[0.7, 0.2, 0.1])\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _generate_suministro_nodes(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generates nodes of suministros/contratos\n",
    "        \"\"\"\n",
    "        \n",
    "        data = {\n",
    "            'node_id': [f\"SUM_{i:06d}\" for i in range(NUM_SUMINISTROS_ADV)],\n",
    "            \n",
    "            # Datos contractuales\n",
    "            'fecha_alta': pd.to_datetime([\n",
    "                datetime.now() - timedelta(days=random.randint(1, 2555)) \n",
    "                for _ in range(NUM_SUMINISTROS_ADV)\n",
    "            ]),\n",
    "            'fecha_baja': pd.to_datetime([\n",
    "                datetime.now() + timedelta(days=random.randint(-30, 365)) if random.random() < 0.05 else None\n",
    "                for _ in range(NUM_SUMINISTROS_ADV)\n",
    "            ]),\n",
    "            'estado_contrato': np.random.choice(['ACTIVO', 'SUSPENDIDO', 'BAJA'], \n",
    "                                              NUM_SUMINISTROS_ADV, p=[0.92, 0.05, 0.03]),\n",
    "            'tipo_suministro': np.random.choice(['RESIDENCIAL', 'COMERCIAL', 'INDUSTRIAL', 'AGRICOLA'], \n",
    "                                              NUM_SUMINISTROS_ADV, p=[0.6, 0.25, 0.1, 0.05]),\n",
    "            \n",
    "            # Potencia y tarifa\n",
    "            'potencia_contratada': np.random.lognormal(8.0, 1.0, NUM_SUMINISTROS_ADV),\n",
    "            'potencia_maxima_demandada': np.random.lognormal(8.2, 0.8, NUM_SUMINISTROS_ADV),\n",
    "            'tarifa_activa': np.random.choice(['2.0TD', '3.0TD', '6.1TD', '6.2TD'], \n",
    "                                            NUM_SUMINISTROS_ADV, p=[0.7, 0.2, 0.05, 0.05]),\n",
    "            'discriminacion_horaria': np.random.choice([True, False], NUM_SUMINISTROS_ADV, p=[0.4, 0.6]),\n",
    "            \n",
    "            # CNAE y actividad\n",
    "            'cnae': np.random.choice(['4711', '5210', '9820', '1071', '4540'], \n",
    "                                   NUM_SUMINISTROS_ADV, p=[0.3, 0.25, 0.2, 0.15, 0.1]),\n",
    "            'actividad_declarada': np.random.choice(['VIVIENDA', 'COMERCIO', 'OFICINA', 'INDUSTRIA', 'OTROS'], \n",
    "                                                  NUM_SUMINISTROS_ADV, p=[0.6, 0.2, 0.1, 0.05, 0.05]),\n",
    "            \n",
    "            # Facturación y pagos\n",
    "            'facturacion_mensual_promedio': np.random.lognormal(4.5, 1.0, NUM_SUMINISTROS_ADV),\n",
    "            'dias_morosidad': np.random.exponential(5, NUM_SUMINISTROS_ADV),\n",
    "            'num_impagos': np.random.poisson(0.2, NUM_SUMINISTROS_ADV),\n",
    "            'metodo_pago': np.random.choice(['DOMICILIACION', 'TRANSFERENCIA', 'EFECTIVO'], \n",
    "                                          NUM_SUMINISTROS_ADV, p=[0.8, 0.15, 0.05]),\n",
    "            \n",
    "            # Consumo contractual\n",
    "            'consumo_anual_estimado': np.random.lognormal(7.0, 1.2, NUM_SUMINISTROS_ADV),\n",
    "            'consumo_anual_real': np.random.lognormal(7.1, 1.1, NUM_SUMINISTROS_ADV),\n",
    "            'desviacion_consumo_estimado': np.random.normal(0, 25, NUM_SUMINISTROS_ADV),\n",
    "            \n",
    "            # Modificaciones contractuales\n",
    "            'num_cambios_potencia': np.random.poisson(0.3, NUM_SUMINISTROS_ADV),\n",
    "            'num_cambios_tarifa': np.random.poisson(0.2, NUM_SUMINISTROS_ADV),\n",
    "            'ultima_modificacion': pd.to_datetime([\n",
    "                datetime.now() - timedelta(days=random.randint(30, 730)) if random.random() < 0.3 else None\n",
    "                for _ in range(NUM_SUMINISTROS_ADV)\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _generate_comercializadora_nodes(self) -> pd.DataFrame:\n",
    "        \"\"\"Genera nodos de comercializadoras\"\"\"\n",
    "        \n",
    "        nombres = ['ENDESA', 'IBERDROLA', 'NATURGY', 'VIESGO', 'EDP', 'HOLALUZ', 'FACTOR', 'LUCERA', \n",
    "                  'PEPEENERGY', 'SOMA', 'BASER', 'ALDRO', 'REPSOL', 'TOTALENERGIES', 'GALP', \n",
    "                  'GESTERNOVA', 'NEXUS', 'ACCIONA', 'CIDE', 'COMPETENCIA']\n",
    "        \n",
    "        data = {\n",
    "            'node_id': [f\"COM_{i:03d}\" for i in range(NUM_COMERCIALIZADORAS_ADV)],\n",
    "            'nombre': nombres[:NUM_COMERCIALIZADORAS_ADV],\n",
    "            'tipo_empresa': np.random.choice(['GRAN_EMPRESA', 'MEDIANA_EMPRESA', 'STARTUP'], \n",
    "                                           NUM_COMERCIALIZADORAS_ADV, p=[0.6, 0.3, 0.1]),\n",
    "            'NUM_CLIENTES_ADV_total': np.random.lognormal(10, 2, NUM_COMERCIALIZADORAS_ADV),\n",
    "            'cuota_mercado': np.random.exponential(5, NUM_COMERCIALIZADORAS_ADV),\n",
    "            'anos_operando': np.random.randint(1, 25, NUM_COMERCIALIZADORAS_ADV),\n",
    "            'rating_financiero': np.random.choice(['AAA', 'AA', 'A', 'BBB', 'BB'], \n",
    "                                                NUM_COMERCIALIZADORAS_ADV, p=[0.1, 0.2, 0.4, 0.2, 0.1]),\n",
    "            'num_reclamaciones': np.random.poisson(50, NUM_COMERCIALIZADORAS_ADV),\n",
    "            'tiempo_respuesta_promedio': np.random.exponential(3, NUM_COMERCIALIZADORAS_ADV),\n",
    "            'score_satisfaccion': np.random.beta(6, 2, NUM_COMERCIALIZADORAS_ADV) * 10,\n",
    "            'precios_competitivos': np.random.beta(5, 3, NUM_COMERCIALIZADORAS_ADV),\n",
    "            'productos_verdes': np.random.choice([True, False], NUM_COMERCIALIZADORAS_ADV, p=[0.7, 0.3]),\n",
    "            'servicios_digitales': np.random.choice([True, False], NUM_COMERCIALIZADORAS_ADV, p=[0.8, 0.2])\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _generate_ubicacion_nodes(self) -> pd.DataFrame:\n",
    "        \"\"\"Genera nodos de ubicaciones geográficas\"\"\"\n",
    "        \n",
    "        # Simular coordenadas en España\n",
    "        lat_base, lon_base = 40.4168, -3.7038  # Madrid centro\n",
    "        \n",
    "        data = {\n",
    "            'node_id': [f\"UBI_{i:06d}\" for i in range(NUM_UBICACIONES_ADV)],\n",
    "            'latitud': np.random.normal(lat_base, 2.5, NUM_UBICACIONES_ADV),\n",
    "            'longitud': np.random.normal(lon_base, 3.0, NUM_UBICACIONES_ADV),\n",
    "            'coordenada_utm_x': np.random.uniform(300000, 800000, NUM_UBICACIONES_ADV),\n",
    "            'coordenada_utm_y': np.random.uniform(4200000, 4800000, NUM_UBICACIONES_ADV),\n",
    "            'codigo_postal': [f\"{random.randint(1, 52):02d}{random.randint(1, 999):03d}\" \n",
    "                            for _ in range(NUM_UBICACIONES_ADV)],\n",
    "            'provincia': np.random.choice(['MADRID', 'BARCELONA', 'VALENCIA', 'SEVILLA', 'BILBAO', 'ZARAGOZA'], \n",
    "                                       NUM_UBICACIONES_ADV, p=[0.3, 0.25, 0.15, 0.1, 0.1, 0.1]),\n",
    "            'tipo_zona': np.random.choice(['URBANA', 'RURAL', 'SEMIURBANA'], \n",
    "                                        NUM_UBICACIONES_ADV, p=[0.6, 0.2, 0.2]),\n",
    "            'densidad_poblacion': np.random.lognormal(6, 1.5, NUM_UBICACIONES_ADV),\n",
    "            'nivel_socioeconomico': np.random.choice(['ALTO', 'MEDIO_ALTO', 'MEDIO', 'MEDIO_BAJO', 'BAJO'], \n",
    "                                                   NUM_UBICACIONES_ADV, p=[0.1, 0.2, 0.4, 0.2, 0.1]),\n",
    "            'indice_criminalidad': np.random.exponential(3, NUM_UBICACIONES_ADV),\n",
    "            'accesibilidad_tecnica': np.random.choice(['FACIL', 'NORMAL', 'DIFICIL'], \n",
    "                                                    NUM_UBICACIONES_ADV, p=[0.6, 0.3, 0.1]),\n",
    "            'cobertura_comunicaciones': np.random.beta(8, 2, NUM_UBICACIONES_ADV) * 100,\n",
    "            'historial_fraudes': np.random.poisson(0.5, NUM_UBICACIONES_ADV),\n",
    "            'NUM_CONTADORES_ADV_zona': np.random.poisson(10, NUM_UBICACIONES_ADV),\n",
    "            'distancia_centro_transformacion': np.random.exponential(500, NUM_UBICACIONES_ADV),\n",
    "            'calidad_suministro': np.random.beta(9, 1, NUM_UBICACIONES_ADV) * 100\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _generate_concentrador_nodes(self) -> pd.DataFrame:\n",
    "        \"\"\"Genera nodos de concentradores de comunicaciones\"\"\"\n",
    "        \n",
    "        data = {\n",
    "            'node_id': [f\"CON_{i:04d}\" for i in range(NUM_CONCENTRADORES_ADV)],\n",
    "            'fabricante': np.random.choice(['ZIV', 'SAGITTARIUS', 'PRIME', 'LANDIS', 'ITRON'], \n",
    "                                         NUM_CONCENTRADORES_ADV),\n",
    "            'modelo_concentrador': np.random.choice(['DCU_500', 'DCU_1000', 'PRIME_Master', 'G3_Head'], \n",
    "                                                  NUM_CONCENTRADORES_ADV),\n",
    "            'version_software': [f\"v{random.randint(2,6)}.{random.randint(0,9)}\" \n",
    "                               for _ in range(NUM_CONCENTRADORES_ADV)],\n",
    "            'capacidad_maxima': np.random.choice([500, 1000, 2000, 5000], NUM_CONCENTRADORES_ADV),\n",
    "            'contadores_conectados': np.random.randint(10, 500, NUM_CONCENTRADORES_ADV),\n",
    "            'tasa_lectura_exitosa': np.random.beta(9, 1, NUM_CONCENTRADORES_ADV) * 100,\n",
    "            'latencia_promedio': np.random.exponential(2, NUM_CONCENTRADORES_ADV),\n",
    "            'uptime_porcentaje': np.random.beta(95, 5, NUM_CONCENTRADORES_ADV) * 100,\n",
    "            'num_reinicios_mes': np.random.poisson(1, NUM_CONCENTRADORES_ADV),\n",
    "            'errores_comunicacion': np.random.poisson(5, NUM_CONCENTRADORES_ADV),\n",
    "            'temperatura_operacion': np.random.normal(35, 10, NUM_CONCENTRADORES_ADV),\n",
    "            'nivel_bateria': np.random.beta(8, 2, NUM_CONCENTRADORES_ADV) * 100,\n",
    "            'calidad_senal_red': np.random.beta(7, 3, NUM_CONCENTRADORES_ADV) * 100,\n",
    "            'protocolo_principal': np.random.choice(['PLC_PRIME', 'PLC_G3', 'RF_169MHz', 'GPRS'], \n",
    "                                                  NUM_CONCENTRADORES_ADV, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "            'backup_comunicacion': np.random.choice([True, False], NUM_CONCENTRADORES_ADV, p=[0.7, 0.3]),\n",
    "            'ultima_actualizacion': pd.to_datetime([\n",
    "                datetime.now() - timedelta(days=random.randint(1, 365)) \n",
    "                for _ in range(NUM_CONCENTRADORES_ADV)\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _generate_cliente_nodes(self) -> pd.DataFrame:\n",
    "        \"\"\"Genera nodos de clientes\"\"\"\n",
    "        \n",
    "        data = {\n",
    "            'node_id': [f\"CLI_{i:06d}\" for i in range(NUM_CLIENTES_ADV)],\n",
    "            'tipo_cliente': np.random.choice(['PERSONA_FISICA', 'PERSONA_JURIDICA', 'ADMINISTRACION'], \n",
    "                                           NUM_CLIENTES_ADV, p=[0.8, 0.15, 0.05]),\n",
    "            'antiguedad_cliente': np.random.exponential(5, NUM_CLIENTES_ADV),\n",
    "            'NUM_SUMINISTROS_ADV': np.random.choice([1, 2, 3, 4, 5], NUM_CLIENTES_ADV, p=[0.7, 0.2, 0.05, 0.03, 0.02]),\n",
    "            'score_crediticio': np.random.beta(6, 4, NUM_CLIENTES_ADV) * 1000,\n",
    "            'ingresos_declarados': np.random.lognormal(10, 0.8, NUM_CLIENTES_ADV),\n",
    "            'edad_estimada': np.random.normal(45, 15, NUM_CLIENTES_ADV),\n",
    "            'canal_preferido': np.random.choice(['ONLINE', 'TELEFONO', 'PRESENCIAL', 'EMAIL'], \n",
    "                                              NUM_CLIENTES_ADV, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "            'num_reclamaciones': np.random.poisson(0.5, NUM_CLIENTES_ADV),\n",
    "            'num_cambios_comercializadora': np.random.poisson(1, NUM_CLIENTES_ADV),\n",
    "            'satisfaccion_cliente': np.random.beta(7, 3, NUM_CLIENTES_ADV) * 10,\n",
    "            'uso_servicios_digitales': np.random.choice([True, False], NUM_CLIENTES_ADV, p=[0.6, 0.4]),\n",
    "            'conciencia_energetica': np.random.beta(5, 5, NUM_CLIENTES_ADV) * 10,\n",
    "            'historial_impagos': np.random.poisson(0.3, NUM_CLIENTES_ADV),\n",
    "            'metodo_contacto_preferido': np.random.choice(['EMAIL', 'SMS', 'CARTA', 'TELEFONO'], \n",
    "                                                        NUM_CLIENTES_ADV, p=[0.5, 0.3, 0.1, 0.1]),\n",
    "            'programa_fidelizacion': np.random.choice([True, False], NUM_CLIENTES_ADV, p=[0.3, 0.7])\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _generate_transformador_nodes(self) -> pd.DataFrame:\n",
    "        \"\"\"Genera nodos de transformadores de distribución\"\"\"\n",
    "        \n",
    "        data = {\n",
    "            'node_id': [f\"TRF_{i:04d}\" for i in range(NUM_TRANSFORMADORES_ADV)],\n",
    "            'potencia_nominal': np.random.choice([25, 50, 100, 160, 250, 400, 630], \n",
    "                                               NUM_TRANSFORMADORES_ADV, p=[0.1, 0.2, 0.25, 0.2, 0.15, 0.08, 0.02]),\n",
    "            'tension_primaria': np.random.choice([15000, 20000, 25000], NUM_TRANSFORMADORES_ADV, p=[0.4, 0.4, 0.2]),\n",
    "            'tension_secundaria': np.random.choice([400, 230], NUM_TRANSFORMADORES_ADV, p=[0.7, 0.3]),\n",
    "            'año_fabricacion': np.random.randint(1980, 2024, NUM_TRANSFORMADORES_ADV),\n",
    "            'fabricante_transformador': np.random.choice(['SCHNEIDER', 'ABB', 'SIEMENS', 'ORMAZABAL', 'COOPER'], \n",
    "                                                       NUM_TRANSFORMADORES_ADV),\n",
    "            'tipo_refrigeracion': np.random.choice(['ACEITE', 'SECO', 'SILICONA'], \n",
    "                                                 NUM_TRANSFORMADORES_ADV, p=[0.6, 0.3, 0.1]),\n",
    "            'carga_actual_porcentaje': np.random.beta(6, 4, NUM_TRANSFORMADORES_ADV) * 100,\n",
    "            'carga_maxima_historica': np.random.beta(8, 2, NUM_TRANSFORMADORES_ADV) * 100,\n",
    "            'temperatura_operacion': np.random.normal(45, 15, NUM_TRANSFORMADORES_ADV),\n",
    "            'NUM_CONTADORES_ADV_alimentados': np.random.poisson(30, NUM_TRANSFORMADORES_ADV),\n",
    "            'perdidas_tecnicas': np.random.beta(2, 8, NUM_TRANSFORMADORES_ADV) * 5,\n",
    "            'ultima_revision': pd.to_datetime([\n",
    "                datetime.now() - timedelta(days=random.randint(30, 1095)) \n",
    "                for _ in range(NUM_TRANSFORMADORES_ADV)\n",
    "            ]),\n",
    "            'estado_conservacion': np.random.choice(['EXCELENTE', 'BUENO', 'REGULAR', 'MALO'], \n",
    "                                                  NUM_TRANSFORMADORES_ADV, p=[0.2, 0.5, 0.25, 0.05]),\n",
    "            'num_averias': np.random.poisson(0.5, NUM_TRANSFORMADORES_ADV),\n",
    "            'tiempo_fuera_servicio': np.random.exponential(2, NUM_TRANSFORMADORES_ADV),\n",
    "            'monitorizacion_remota': np.random.choice([True, False], NUM_TRANSFORMADORES_ADV, p=[0.4, 0.6]),\n",
    "            'sistema_proteccion': np.random.choice(['FUSIBLES', 'INTERRUPTOR', 'AUTOMATICO'], \n",
    "                                                 NUM_TRANSFORMADORES_ADV, p=[0.3, 0.4, 0.3])\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _generate_zona_nodes(self) -> pd.DataFrame:\n",
    "        \"\"\"Genera nodos de zonas de distribución\"\"\"\n",
    "        \n",
    "        data = {\n",
    "            'node_id': [f\"ZON_{i:03d}\" for i in range(NUM_ZONAS_ADV)],\n",
    "            'nombre_zona': [f\"ZONA_{chr(65 + i//26)}{i%26 + 1}\" for i in range(NUM_ZONAS_ADV)],\n",
    "            'tipo_zona_electrica': np.random.choice(['URBANA_DENSA', 'URBANA', 'RURAL', 'INDUSTRIAL'], \n",
    "                                                  NUM_ZONAS_ADV, p=[0.2, 0.4, 0.3, 0.1]),\n",
    "            'NUM_TRANSFORMADORES_ADV': np.random.poisson(8, NUM_ZONAS_ADV),\n",
    "            'NUM_CONTADORES_ADV_zona': np.random.poisson(200, NUM_ZONAS_ADV),\n",
    "            'longitud_red_mt': np.random.exponential(10, NUM_ZONAS_ADV),  # km\n",
    "            'longitud_red_bt': np.random.exponential(25, NUM_ZONAS_ADV),  # km\n",
    "            'indice_calidad_suministro': np.random.beta(8, 2, NUM_ZONAS_ADV) * 10,\n",
    "            'tiempo_medio_interrupcion': np.random.exponential(2, NUM_ZONAS_ADV),  # horas/año\n",
    "            'perdidas_tecnicas_zona': np.random.beta(3, 7, NUM_ZONAS_ADV) * 10,  # %\n",
    "            'perdidas_no_tecnicas': np.random.exponential(2, NUM_ZONAS_ADV),  # %\n",
    "            'densidad_carga': np.random.lognormal(3, 1, NUM_ZONAS_ADV),  # MW/km²\n",
    "            'crecimiento_demanda': np.random.normal(2, 1, NUM_ZONAS_ADV),  # %/año\n",
    "            'num_incidencias_mes': np.random.poisson(3, NUM_ZONAS_ADV),\n",
    "            'cobertura_telemedida': np.random.beta(8, 2, NUM_ZONAS_ADV) * 100,  # %\n",
    "            'antiguedad_red_promedio': np.random.normal(25, 10, NUM_ZONAS_ADV),  # años\n",
    "            'inversion_mantenimiento': np.random.lognormal(8, 1, NUM_ZONAS_ADV),  # €/año\n",
    "            'personal_tecnico_asignado': np.random.poisson(2, NUM_ZONAS_ADV),\n",
    "            'vehiculos_disponibles': np.random.poisson(1, NUM_ZONAS_ADV),\n",
    "            'almacen_material': np.random.choice([True, False], NUM_ZONAS_ADV, p=[0.6, 0.4])\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _generate_heterogeneous_edges(self, node_data: Dict) -> Dict:\n",
    "        \"\"\"Genera todas las relaciones heterogéneas entre tipos de nodos\"\"\"\n",
    "        \n",
    "        print(\"🔗 Generando relaciones heterogéneas...\")\n",
    "        \n",
    "        edge_data = {}\n",
    "        \n",
    "        # 1. CONTADOR -> SUMINISTRO (1:1)\n",
    "        edge_data[('contador', 'mide', 'suministro')] = self._create_contador_suministro_edges(\n",
    "            node_data['contador'], node_data['suministro']\n",
    "        )\n",
    "        \n",
    "        # 2. SUMINISTRO -> COMERCIALIZADORA (N:1)\n",
    "        edge_data[('suministro', 'contratado_con', 'comercializadora')] = self._create_suministro_comercializadora_edges(\n",
    "            node_data['suministro'], node_data['comercializadora']\n",
    "        )\n",
    "        \n",
    "        # 3. CONTADOR -> UBICACION (N:1)\n",
    "        edge_data[('contador', 'ubicado_en', 'ubicacion')] = self._create_contador_ubicacion_edges(\n",
    "            node_data['contador'], node_data['ubicacion']\n",
    "        )\n",
    "        \n",
    "        # 4. CONTADOR -> CONCENTRADOR (N:1)\n",
    "        edge_data[('contador', 'comunica_via', 'concentrador')] = self._create_contador_concentrador_edges(\n",
    "            node_data['contador'], node_data['concentrador']\n",
    "        )\n",
    "        \n",
    "        # 5. SUMINISTRO -> CLIENTE (N:1)\n",
    "        edge_data[('suministro', 'pertenece_a', 'cliente')] = self._create_suministro_cliente_edges(\n",
    "            node_data['suministro'], node_data['cliente']\n",
    "        )\n",
    "        \n",
    "        # 6. CONTADOR -> TRANSFORMADOR (N:1)\n",
    "        edge_data[('contador', 'alimentado_por', 'transformador')] = self._create_contador_transformador_edges(\n",
    "            node_data['contador'], node_data['transformador']\n",
    "        )\n",
    "        \n",
    "        # 7. TRANSFORMADOR -> ZONA (N:1)\n",
    "        edge_data[('transformador', 'pertenece_zona', 'zona')] = self._create_transformador_zona_edges(\n",
    "            node_data['transformador'], node_data['zona']\n",
    "        )\n",
    "        \n",
    "        # 8. UBICACION -> ZONA (N:1)\n",
    "        edge_data[('ubicacion', 'dentro_de', 'zona')] = self._create_ubicacion_zona_edges(\n",
    "            node_data['ubicacion'], node_data['zona']\n",
    "        )\n",
    "        \n",
    "        # 9. Relaciones intra-tipo\n",
    "        \n",
    "        # CONTADOR -> CONTADOR (proximidad geográfica)\n",
    "        edge_data[('contador', 'cerca_de', 'contador')] = self._create_contador_contador_edges(\n",
    "            node_data['contador'], node_data['ubicacion']\n",
    "        )\n",
    "        \n",
    "        # CONTADOR -> CONTADOR (mismo modelo/marca)\n",
    "        edge_data[('contador', 'similar_a', 'contador')] = self._create_contador_similar_edges(\n",
    "            node_data['contador']\n",
    "        )\n",
    "        \n",
    "        # TRANSFORMADOR -> TRANSFORMADOR (misma zona)\n",
    "        edge_data[('transformador', 'conectado_a', 'transformador')] = self._create_transformador_transformador_edges(\n",
    "            node_data['transformador']\n",
    "        )\n",
    "        \n",
    "        # CLIENTE -> CLIENTE (múltiples suministros)\n",
    "        edge_data[('cliente', 'relacionado_con', 'cliente')] = self._create_cliente_cliente_edges(\n",
    "            node_data['cliente']\n",
    "        )\n",
    "        \n",
    "        return edge_data\n",
    "    \n",
    "    def _create_contador_suministro_edges(self, contadores: pd.DataFrame, suministros: pd.DataFrame) -> List:\n",
    "        \"\"\"Relación 1:1 entre contadores y suministros\"\"\"\n",
    "        edges = []\n",
    "        for i in range(min(len(contadores), len(suministros))):\n",
    "            edges.append([i, i])  # Correspondencia 1:1\n",
    "        return edges\n",
    "    \n",
    "    def _create_suministro_comercializadora_edges(self, suministros: pd.DataFrame, comercializadoras: pd.DataFrame) -> List:\n",
    "        \"\"\"Relación N:1 suministros -> comercializadoras\"\"\"\n",
    "        edges = []\n",
    "        for i in range(len(suministros)):\n",
    "            com_idx = np.random.randint(0, len(comercializadoras))\n",
    "            edges.append([i, com_idx])\n",
    "        return edges\n",
    "    \n",
    "    def _create_contador_ubicacion_edges(self, contadores: pd.DataFrame, ubicaciones: pd.DataFrame) -> List:\n",
    "        \"\"\"Relación N:1 contadores -> ubicaciones\"\"\"\n",
    "        edges = []\n",
    "        for i in range(len(contadores)):\n",
    "            ubi_idx = np.random.randint(0, len(ubicaciones))\n",
    "            edges.append([i, ubi_idx])\n",
    "        return edges\n",
    "    \n",
    "    def _create_contador_concentrador_edges(self, contadores: pd.DataFrame, concentradores: pd.DataFrame) -> List:\n",
    "        \"\"\"Relación N:1 contadores -> concentradores\"\"\"\n",
    "        edges = []\n",
    "        for i in range(len(contadores)):\n",
    "            # Simular distribución no uniforme (algunos concentradores más cargados)\n",
    "            prob_weights = np.random.exponential(1, len(concentradores))\n",
    "            prob_weights = prob_weights / prob_weights.sum()\n",
    "            conc_idx = np.random.choice(len(concentradores), p=prob_weights)\n",
    "            edges.append([i, conc_idx])\n",
    "        return edges\n",
    "    \n",
    "    def _create_suministro_cliente_edges(self, suministros: pd.DataFrame, clientes: pd.DataFrame) -> List:\n",
    "        \"\"\"Relación N:1 suministros -> clientes (algunos clientes tienen múltiples suministros)\"\"\"\n",
    "        edges = []\n",
    "        cliente_idx = 0\n",
    "        for i in range(len(suministros)):\n",
    "            edges.append([i, cliente_idx])\n",
    "            # Probabilidad de que el siguiente suministro sea del mismo cliente\n",
    "            if np.random.random() > 0.85:  # 15% probabilidad de múltiples suministros\n",
    "                cliente_idx = min(cliente_idx + 1, len(clientes) - 1)\n",
    "        return edges\n",
    "    \n",
    "    def _create_contador_transformador_edges(self, contadores: pd.DataFrame, transformadores: pd.DataFrame) -> List:\n",
    "        \"\"\"Relación N:1 contadores -> transformadores\"\"\"\n",
    "        edges = []\n",
    "        for i in range(len(contadores)):\n",
    "            # Distribución realista: cada transformador alimenta ~25 contadores\n",
    "            trf_idx = i // 25 % len(transformadores)\n",
    "            edges.append([i, trf_idx])\n",
    "        return edges\n",
    "    \n",
    "    def _create_transformador_zona_edges(self, transformadores: pd.DataFrame, zonas: pd.DataFrame) -> List:\n",
    "        \"\"\"Relación N:1 transformadores -> zonas\"\"\"\n",
    "        edges = []\n",
    "        for i in range(len(transformadores)):\n",
    "            zona_idx = i // 4 % len(zonas)  # ~4 transformadores por zona\n",
    "            edges.append([i, zona_idx])\n",
    "        return edges\n",
    "    \n",
    "    def _create_ubicacion_zona_edges(self, ubicaciones: pd.DataFrame, zonas: pd.DataFrame) -> List:\n",
    "        \"\"\"Relación N:1 ubicaciones -> zonas\"\"\"\n",
    "        edges = []\n",
    "        for i in range(len(ubicaciones)):\n",
    "            zona_idx = i // 20 % len(zonas)  # ~20 ubicaciones por zona\n",
    "            edges.append([i, zona_idx])\n",
    "        return edges\n",
    "    \n",
    "    def _create_contador_contador_edges(self, contadores: pd.DataFrame, ubicaciones: pd.DataFrame) -> List:\n",
    "        \"\"\"Relaciones entre contadores por proximidad geográfica\"\"\"\n",
    "        edges = []\n",
    "        \n",
    "        # Simular proximidad basada en ubicaciones\n",
    "        for i in range(len(contadores)):\n",
    "            # Cada contador se conecta con 2-5 vecinos cercanos\n",
    "            num_neighbors = np.random.randint(2, 6)\n",
    "            for _ in range(num_neighbors):\n",
    "                neighbor_idx = np.random.randint(0, len(contadores))\n",
    "                if neighbor_idx != i:\n",
    "                    edges.append([i, neighbor_idx])\n",
    "                    \n",
    "        return edges\n",
    "    \n",
    "    def _create_contador_similar_edges(self, contadores: pd.DataFrame) -> List:\n",
    "        \"\"\"Relaciones entre contadores de mismo modelo/marca\"\"\"\n",
    "        edges = []\n",
    "        \n",
    "        # Agrupar por marca y modelo\n",
    "        for marca in contadores['marca'].unique():\n",
    "            for modelo in contadores['modelo'].unique():\n",
    "                subset_indices = contadores[\n",
    "                    (contadores['marca'] == marca) & (contadores['modelo'] == modelo)\n",
    "                ].index.tolist()\n",
    "                \n",
    "                # Conectar contadores del mismo tipo\n",
    "                for i in range(len(subset_indices)):\n",
    "                    for j in range(i + 1, min(i + 5, len(subset_indices))):  # Limitar conexiones\n",
    "                        edges.append([subset_indices[i], subset_indices[j]])\n",
    "                        edges.append([subset_indices[j], subset_indices[i]])  # Bidireccional\n",
    "                        \n",
    "        return edges\n",
    "    \n",
    "    def _create_transformador_transformador_edges(self, transformadores: pd.DataFrame) -> List:\n",
    "        \"\"\"Relaciones entre transformadores en la misma red\"\"\"\n",
    "        edges = []\n",
    "        \n",
    "        # Conectar transformadores por grupos (simulando red eléctrica)\n",
    "        for i in range(0, len(transformadores), 10):  # Grupos de 10\n",
    "            group_end = min(i + 10, len(transformadores))\n",
    "            for j in range(i, group_end):\n",
    "                for k in range(j + 1, group_end):\n",
    "                    if np.random.random() < 0.3:  # 30% probabilidad de conexión\n",
    "                        edges.append([j, k])\n",
    "                        edges.append([k, j])  # Bidireccional\n",
    "                        \n",
    "        return edges\n",
    "    \n",
    "    def _create_cliente_cliente_edges(self, clientes: pd.DataFrame) -> List:\n",
    "        \"\"\"Relaciones entre clientes relacionados\"\"\"\n",
    "        edges = []\n",
    "        \n",
    "        # Simular relaciones familiares o empresariales\n",
    "        for i in range(len(clientes)):\n",
    "            if np.random.random() < 0.1:  # 10% probabilidad de tener relación\n",
    "                related_idx = np.random.randint(0, len(clientes))\n",
    "                if related_idx != i:\n",
    "                    edges.append([i, related_idx])\n",
    "                    \n",
    "        return edges\n",
    "    \n",
    "    def _assign_fraud_labels(self, contadores: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Asigna etiquetas de fraude a contadores con patrones realistas\"\"\"\n",
    "        \n",
    "        NUM_CONTADORES_ADV = len(contadores)\n",
    "        labels = ['NORMAL'] * NUM_CONTADORES_ADV\n",
    "        \n",
    "        # Calcular número de fraudes\n",
    "        num_fraudes = int(NUM_CONTADORES_ADV * FRAUD_PROBABILITY_ADV)\n",
    "        num_irregularidades = int(NUM_CONTADORES_ADV * IRREGULARITY_PROBABILITY_ADV)\n",
    "        \n",
    "        # Seleccionar índices para fraudes (con bias hacia ciertos patrones)\n",
    "        fraud_indices = []\n",
    "        irregularity_indices = []\n",
    "        \n",
    "        # Bias hacia contadores con ciertas características\n",
    "        fraud_candidates = contadores[\n",
    "            (contadores['consumo_promedio_diario'] < contadores['consumo_promedio_diario'].quantile(0.2)) |\n",
    "            (contadores['estado_comunicacion'] != 'ACTIVO') |\n",
    "            (contadores['anomalias_detectadas'] > 2) |\n",
    "            (contadores['resultado_ultima_inspeccion'] == 'ANOMALIA_MAYOR')\n",
    "        ].index.tolist()\n",
    "        \n",
    "        # Seleccionar fraudes de candidatos sospechosos (70%) y aleatorios (30%)\n",
    "        fraud_from_candidates = min(int(num_fraudes * 0.7), len(fraud_candidates))\n",
    "        fraud_indices.extend(np.random.choice(fraud_candidates, fraud_from_candidates, replace=False))\n",
    "        \n",
    "        remaining_fraud = num_fraudes - fraud_from_candidates\n",
    "        if remaining_fraud > 0:\n",
    "            available_indices = [i for i in range(NUM_CONTADORES_ADV) if i not in fraud_indices]\n",
    "            fraud_indices.extend(np.random.choice(available_indices, remaining_fraud, replace=False))\n",
    "        \n",
    "        # Seleccionar irregularidades (más aleatorias)\n",
    "        available_for_irregularity = [i for i in range(NUM_CONTADORES_ADV) if i not in fraud_indices]\n",
    "        irregularity_indices = np.random.choice(available_for_irregularity, num_irregularidades, replace=False)\n",
    "        \n",
    "        # Asignar etiquetas\n",
    "        for idx in fraud_indices:\n",
    "            labels[idx] = 'FRAUDE'\n",
    "            \n",
    "        for idx in irregularity_indices:\n",
    "            labels[idx] = 'IRREGULARIDAD'\n",
    "        \n",
    "        # Modificar características de contadores fraudulentos para crear patrones\n",
    "        contadores = contadores.copy()\n",
    "        \n",
    "        for idx in fraud_indices:\n",
    "            # Fraudes tienden a tener consumo bajo y patrones anómalos\n",
    "            contadores.loc[idx, 'consumo_promedio_diario'] *= np.random.uniform(0.1, 0.4)\n",
    "            contadores.loc[idx, 'variabilidad_consumo'] *= np.random.uniform(0.2, 0.6)\n",
    "            contadores.loc[idx, 'anomalias_detectadas'] += np.random.randint(1, 5)\n",
    "            contadores.loc[idx, 'eventos_alarma'] += np.random.randint(2, 8)\n",
    "            \n",
    "        for idx in irregularity_indices:\n",
    "            # Irregularidades tienen patrones menos extremos\n",
    "            contadores.loc[idx, 'consumo_promedio_diario'] *= np.random.uniform(0.5, 0.8)\n",
    "            contadores.loc[idx, 'score_fiabilidad'] *= np.random.uniform(0.6, 0.9)\n",
    "            contadores.loc[idx, 'anomalias_detectadas'] += np.random.randint(1, 3)\n",
    "        \n",
    "        contadores['label'] = labels\n",
    "        \n",
    "        print(f\"✅ Etiquetas asignadas: {labels.count('NORMAL')} Normal, {labels.count('FRAUDE')} Fraude, {labels.count('IRREGULARIDAD')} Irregularidad\")\n",
    "        \n",
    "        return contadores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b648b730-b13a-4565-a0df-46f6e64bacb4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.3. Data Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659d820-4e3e-4542-9f90-b6105e345ccb",
   "metadata": {},
   "source": [
    "We create a class in order to process the data. Take into account that our goal here is to have the data in PyTorch Geometric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b9ec41-7216-4060-b11f-6605314fc475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousGraphProcessor:\n",
    "    \"\"\"\n",
    "    Processor in order to convert the previous data into a PyTorch Geometric format\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.label_encoders = {}\n",
    "        \n",
    "    def create_heterogeneous_graph(self, data: Dict) -> HeteroData:\n",
    "        \"\"\"\n",
    "        Converts hetero data into a HeteroData object in PyTorch Geometric\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"🔄 Procesando grafo heterogéneo...\")\n",
    "        \n",
    "        hetero_data = HeteroData()\n",
    "        \n",
    "        # Process each type of node\n",
    "        for node_type, node_df in data['nodes'].items():\n",
    "            print(f\"   Procesando nodos tipo: {node_type}\")\n",
    "            \n",
    "            # Procesar características del nodo\n",
    "            x, feature_names = self._process_node_features(node_df, node_type)\n",
    "            hetero_data[node_type].x = torch.tensor(x, dtype=torch.float)\n",
    "            \n",
    "            # Guardar nombres de características para análisis posterior\n",
    "            hetero_data[node_type].feature_names = feature_names\n",
    "            \n",
    "            # Para contadores, agregar etiquetas de fraude\n",
    "            if node_type == 'contador' and 'label' in node_df.columns:\n",
    "                labels = [FRAUD_CLASSES[label] for label in node_df['label']]\n",
    "                hetero_data[node_type].y = torch.tensor(labels, dtype=torch.long)\n",
    "            \n",
    "            print(f\"     ✅ {len(node_df)} nodos, {x.shape[1]} características\")\n",
    "        \n",
    "        # Process heterogeneous relations:\n",
    "        for edge_type, edge_list in data['edges'].items():\n",
    "            if len(edge_list) > 0:\n",
    "                src_type, relation, dst_type = edge_type\n",
    "                edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "                hetero_data[src_type, relation, dst_type].edge_index = edge_index\n",
    "                print(f\"   ✅ {relation}: {len(edge_list)} relaciones ({src_type} -> {dst_type})\")\n",
    "        \n",
    "        # Transformar a formato homogéneo para algunos algoritmos\n",
    "        # hetero_data = T.ToHomogeneous()(hetero_data)\n",
    "        \n",
    "        print(f\"✅ Grafo heterogéneo creado con {len(data['nodes'])} tipos de nodos\")\n",
    "        \n",
    "        return hetero_data\n",
    "    \n",
    "    def _process_node_features(self, node_df: pd.DataFrame, node_type: str) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"Procesa características de un tipo específico de nodo\"\"\"\n",
    "        \n",
    "        # Excluir columnas no-feature\n",
    "        exclude_cols = ['node_id', 'label']\n",
    "        feature_cols = [col for col in node_df.columns if col not in exclude_cols]\n",
    "        \n",
    "        processed_features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            if node_df[col].dtype in ['object', 'bool']:\n",
    "                # Variables categóricas\n",
    "                if node_df[col].dtype == 'bool':\n",
    "                    # Variables booleanas\n",
    "                    processed_features.append(node_df[col].astype(int).values.reshape(-1, 1))\n",
    "                    feature_names.append(f\"{col}\")\n",
    "                else:\n",
    "                    # Variables categóricas -> One-hot encoding\n",
    "                    encoded = pd.get_dummies(node_df[col], prefix=col)\n",
    "                    processed_features.append(encoded.values)\n",
    "                    feature_names.extend(encoded.columns.tolist())\n",
    "                    \n",
    "            elif node_df[col].dtype in ['datetime64[ns]']:\n",
    "                # Variables temporales -> características numéricas\n",
    "                days_since = (datetime.now() - node_df[col]).dt.days.fillna(0).values.reshape(-1, 1)\n",
    "                processed_features.append(days_since)\n",
    "                feature_names.append(f\"{col}_days_since\")\n",
    "                \n",
    "            else:\n",
    "                # Variables numéricas -> normalización\n",
    "                scaler_key = f\"{node_type}_{col}\"\n",
    "                if scaler_key not in self.scalers:\n",
    "                    self.scalers[scaler_key] = StandardScaler()\n",
    "                \n",
    "                values = node_df[col].fillna(node_df[col].median()).values.reshape(-1, 1)\n",
    "                normalized = self.scalers[scaler_key].fit_transform(values)\n",
    "                processed_features.append(normalized)\n",
    "                feature_names.append(f\"{col}_normalized\")\n",
    "        \n",
    "        # Concatenar todas las características\n",
    "        if processed_features:\n",
    "            final_features = np.concatenate(processed_features, axis=1)\n",
    "        else:\n",
    "            final_features = np.zeros((len(node_df), 1))\n",
    "            feature_names = ['dummy_feature']\n",
    "        \n",
    "        return final_features, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79e9265-f9f8-4011-b90a-a88cbb915b16",
   "metadata": {},
   "source": [
    "Now, the architecture, trainer and evaluator are going to be imported from the utils folder where we have those .pys stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f003b3-5366-4205-b4cb-4dda184aae71",
   "metadata": {},
   "source": [
    "## 5.4. Advanced GNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6ac8252-6e19-467c-ae65-2e7cfe8baf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.arquitecturas_GNN import HeterogeneousFraudGNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c3e81-01eb-4619-a936-724c6ac4950c",
   "metadata": {},
   "source": [
    "## 5.5. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50a0c23b-1938-4da3-9187-060a91ae2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.trainer import HeterogeneousFraudTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeab319-61ec-45b2-8458-a61d6db7c49d",
   "metadata": {},
   "source": [
    "## 5.6. Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80f3fa23-acb9-4a75-ac12-e122d95b365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluator import HeterogeneousEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41995fd1-3bcc-482f-82ad-eea09956ec29",
   "metadata": {},
   "source": [
    "## 5.7. Pipeline with simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e5700d7-6bb4-4cdc-9604-405228b5dc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_heterogeneous():\n",
    "    \"\"\"\n",
    "    Principal pipeline that launches every step in the training phase of the heterogeneous model. \n",
    "    This pipeline is used with the simulated data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configurae the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Usando dispositivo: {device}\")\n",
    "    \n",
    "    # 1. Generate heterogeneous data\n",
    "    logger.info(\"🎲 Generando datos heterogéneos complejos...\")\n",
    "    simulator = HeterogeneousDataSimulator()\n",
    "    raw_data = simulator.generate_heterogeneous_data()\n",
    "        \n",
    "    # 2. Procesar a formato PyTorch Geometric\n",
    "    logger.info(\"🔄 Procesando a formato heterogéneo...\")\n",
    "    processor = HeterogeneousGraphProcessor()\n",
    "    hetero_data = processor.create_heterogeneous_graph(raw_data)\n",
    "        \n",
    "    # 3. Split the data\n",
    "    logger.info(\"📊 Dividiendo datos...\")\n",
    "    NUM_CONTADORES_ADV = hetero_data['contador'].x.size(0)\n",
    "    indices = torch.randperm(NUM_CONTADORES_ADV)\n",
    "        \n",
    "    train_size = int(0.6 * NUM_CONTADORES_ADV)\n",
    "    val_size = int(0.2 * NUM_CONTADORES_ADV)\n",
    "        \n",
    "    train_mask = torch.zeros(NUM_CONTADORES_ADV, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(NUM_CONTADORES_ADV, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(NUM_CONTADORES_ADV, dtype=torch.bool)\n",
    "        \n",
    "    train_mask[indices[:train_size]] = True\n",
    "    val_mask[indices[train_size:train_size + val_size]] = True\n",
    "    test_mask[indices[train_size + val_size:]] = True\n",
    "\n",
    "    # 4. Create the heterogeneous model\n",
    "    logger.info(\"🧠 Creando modelo GNN heterogéneo...\")\n",
    "        \n",
    "    # Adjust input dims in terms of real data:\n",
    "    for node_type in hetero_data.node_types:\n",
    "        actual_dim = hetero_data[node_type].x.size(1)\n",
    "        NODE_DIMS_ADV[node_type] = actual_dim\n",
    "        \n",
    "    model = HeterogeneousFraudGNN(\n",
    "        metadata=hetero_data.metadata(),\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_classes=len(FRAUD_CLASSES),\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "        \n",
    "    # Contar parámetros\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    logger.info(f\"Parámetros del modelo: {total_params:,}\")\n",
    "        \n",
    "    # 5. Training\n",
    "    logger.info(\"🎓 Entrenando modelo heterogéneo...\")\n",
    "    trainer = HeterogeneousFraudTrainer(model, device)\n",
    "    hetero_data = hetero_data.to(device)\n",
    "    train_mask = train_mask.to(device)\n",
    "    val_mask = val_mask.to(device)\n",
    "    test_mask = test_mask.to(device)\n",
    "        \n",
    "    history = trainer.train(hetero_data, train_mask, val_mask)\n",
    "\n",
    "    # 6. Evaluate over the test set\n",
    "    logger.info(\"📊 Evaluando modelo...\")\n",
    "    model.load_state_dict(torch.load('best_hetero_fraud_model.pth'))\n",
    "    evaluator = HeterogeneousEvaluator(model, device)\n",
    "    \n",
    "    test_results = evaluator.evaluate_detailed(hetero_data, test_mask)\n",
    "        \n",
    "    # 7. Analize attention\n",
    "    attention_analysis = evaluator.analyze_attention_weights(hetero_data)\n",
    "        \n",
    "    # 8. Show results\n",
    "    logger.info(\"📈 RESULTADOS FINALES:\")\n",
    "    logger.info(f\"Mejor precisión de validación: {history['best_val_acc']:.4f}\")\n",
    "    logger.info(f\"ROC AUC: {test_results['roc_auc']:.4f}\")\n",
    "    logger.info(\"\\nReporte de clasificación:\")\n",
    "    print(test_results['classification_report'])\n",
    "        \n",
    "    logger.info(\"\\nPesos de atención por tipo de información:\")\n",
    "    for info_type, weight in attention_analysis.items():\n",
    "        logger.info(f\"  {info_type}: {weight:.3f}\")\n",
    "        \n",
    "    # 9. Visualize the results:\n",
    "    plot_heterogeneous_results(history, test_results, attention_analysis)\n",
    "        \n",
    "    # 10. Detect potential frauds\n",
    "    logger.info(\"🔍 Detectando fraudes potenciales...\")\n",
    "    all_predictions = evaluator.predict(hetero_data)\n",
    "    all_probabilities = evaluator.predict_proba(hetero_data)\n",
    "    \n",
    "    # Casos con alta probabilidad de fraude pero sin etiqueta\n",
    "    potential_frauds = []\n",
    "    for i in range(len(all_predictions)):\n",
    "        if hetero_data['contador'].y[i] == 0:  # Etiquetado como normal\n",
    "            fraud_prob = all_probabilities[i][1] + all_probabilities[i][2]\n",
    "            if fraud_prob > 0.8:  # Alta probabilidad de fraude\n",
    "                potential_frauds.append({\n",
    "                    'contador_idx': i,\n",
    "                    'FRAUD_PROBABILITY_ADV': fraud_prob,\n",
    "                    'predicted_class': all_predictions[i],\n",
    "                    'node_id': raw_data['nodes']['contador'].iloc[i]['node_id']\n",
    "                })\n",
    "    \n",
    "    logger.info(f\"Fraudes potenciales detectados: {len(potential_frauds)}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_results': test_results,\n",
    "        'attention_analysis': attention_analysis,\n",
    "        'potential_frauds': potential_frauds,\n",
    "        'hetero_data': hetero_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450e394-5b0c-4a6d-ab1d-0517b38bd1e6",
   "metadata": {},
   "source": [
    "## 5.8. Pipeline creation (real data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fc40c78-871c-4e65-8b44-2b98e571a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousGraphProcessor:\n",
    "    \"\"\"\n",
    "    Procesador para convertir datos de Neo4j en formato PyTorch Geometric\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.label_encoders = {}\n",
    "        \n",
    "    def create_heterogeneous_graph(self, data: Dict) -> HeteroData:\n",
    "        \"\"\"\n",
    "        Convierte datos heterogéneos en un objeto HeteroData de PyTorch Geometric\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"🔄 Procesando grafo heterogéneo desde Neo4j...\")\n",
    "        \n",
    "        hetero_data = HeteroData()\n",
    "        \n",
    "        # Procesar cada tipo de nodo\n",
    "        for node_type, node_df in data['nodes'].items():\n",
    "            if len(node_df) == 0:\n",
    "                print(f\"   ⚠️ Saltando nodos tipo: {node_type} (vacío)\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"   Procesando nodos tipo: {node_type}\")\n",
    "            \n",
    "            # Procesar características del nodo\n",
    "            x, feature_names = self._process_node_features(node_df, node_type)\n",
    "            hetero_data[node_type].x = torch.tensor(x, dtype=torch.float)\n",
    "            \n",
    "            # Save names of the features for the analysis we will do after\n",
    "            hetero_data[node_type].feature_names = feature_names\n",
    "            \n",
    "            # For CONTADORES, aggregate fraud labels\n",
    "            if node_type == 'contador' and 'label' in node_df.columns:\n",
    "                labels = [FRAUD_CLASSES[label] for label in node_df['label']]\n",
    "                hetero_data[node_type].y = torch.tensor(labels, dtype=torch.long)\n",
    "            \n",
    "            print(f\"     ✅ {len(node_df)} nodes, {x.shape[1]} features\")\n",
    "        \n",
    "        # Process heterogeneous relations\n",
    "        for edge_type, edge_list in data['edges'].items():\n",
    "            if len(edge_list) > 0:\n",
    "                src_type, relation, dst_type = edge_type\n",
    "                \n",
    "                # Verify that both type of node exits\n",
    "                if src_type in hetero_data.node_types and dst_type in hetero_data.node_types:\n",
    "                    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "                    hetero_data[src_type, relation, dst_type].edge_index = edge_index\n",
    "                    print(f\"   ✅ {relation}: {len(edge_list)} relaciones ({src_type} -> {dst_type})\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️ Saltando relación {relation}: tipos de nodo faltantes\")\n",
    "        \n",
    "        print(f\"✅ Heterogeneous graph created with {len(data['nodes'])} type of nodes\")\n",
    "        \n",
    "        return hetero_data\n",
    "    \n",
    "    def _process_node_features(self, node_df: pd.DataFrame, node_type: str) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"\n",
    "        Process one type of node's features.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Excluir columnas no-feature\n",
    "        exclude_cols = ['node_id', 'label', 'nis_rad', 'nis_expediente', 'codigo_comercializadora', 'concentrador_id']\n",
    "        feature_cols = [col for col in node_df.columns if col not in exclude_cols]\n",
    "        \n",
    "        processed_features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            try:\n",
    "                if node_df[col].dtype in ['object', 'bool']:\n",
    "                    # Variables categóricas\n",
    "                    if node_df[col].dtype == 'bool':\n",
    "                        # Variables booleanas\n",
    "                        processed_features.append(node_df[col].astype(int).values.reshape(-1, 1))\n",
    "                        feature_names.append(f\"{col}\")\n",
    "                    else:\n",
    "                        # Variables categóricas -> One-hot encoding\n",
    "                        # Limitar número de categorías para evitar explosión dimensional\n",
    "                        unique_values = node_df[col].nunique()\n",
    "                        if unique_values > 10:\n",
    "                            # Si hay muchas categorías, usar solo las top 10\n",
    "                            top_categories = node_df[col].value_counts().head(10).index\n",
    "                            node_df_temp = node_df[col].copy()\n",
    "                            node_df_temp[~node_df_temp.isin(top_categories)] = 'OTHER'\n",
    "                            encoded = pd.get_dummies(node_df_temp, prefix=col)\n",
    "                        else:\n",
    "                            encoded = pd.get_dummies(node_df[col], prefix=col)\n",
    "                        \n",
    "                        processed_features.append(encoded.values)\n",
    "                        feature_names.extend(encoded.columns.tolist())\n",
    "                        \n",
    "                elif node_df[col].dtype in ['datetime64[ns]', '<M8[ns]']:\n",
    "                    # Variables temporales -> características numéricas\n",
    "                    try:\n",
    "                        days_since = (datetime.now() - pd.to_datetime(node_df[col])).dt.days.fillna(0).values.reshape(-1, 1)\n",
    "                        processed_features.append(days_since)\n",
    "                        feature_names.append(f\"{col}_days_since\")\n",
    "                    except:\n",
    "                        # Si falla la conversión temporal, usar valor por defecto\n",
    "                        default_values = np.zeros((len(node_df), 1))\n",
    "                        processed_features.append(default_values)\n",
    "                        feature_names.append(f\"{col}_days_since\")\n",
    "                        \n",
    "                else:\n",
    "                    # Variables numéricas -> normalización\n",
    "                    scaler_key = f\"{node_type}_{col}\"\n",
    "                    if scaler_key not in self.scalers:\n",
    "                        self.scalers[scaler_key] = StandardScaler()\n",
    "                    \n",
    "                    # Convertir a numérico y manejar NaNs\n",
    "                    values = pd.to_numeric(node_df[col], errors='coerce').fillna(0).values.reshape(-1, 1)\n",
    "                    \n",
    "                    # Verificar si hay varianza en los datos\n",
    "                    if np.std(values) > 0:\n",
    "                        normalized = self.scalers[scaler_key].fit_transform(values)\n",
    "                    else:\n",
    "                        normalized = values  # Si no hay varianza, mantener los valores originales\n",
    "                    \n",
    "                    processed_features.append(normalized)\n",
    "                    feature_names.append(f\"{col}_normalized\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Error procesando columna {col}: {e}\")\n",
    "                # Create features by default in case of error\n",
    "                default_values = np.zeros((len(node_df), 1))\n",
    "                processed_features.append(default_values)\n",
    "                feature_names.append(f\"{col}_default\")\n",
    "        \n",
    "        # Concatenate all the features\n",
    "        if processed_features:\n",
    "            final_features = np.concatenate(processed_features, axis=1)\n",
    "        else:\n",
    "            final_features = np.zeros((len(node_df), 1))\n",
    "            feature_names = ['dummy_feature']\n",
    "        \n",
    "        # Verificar dimensiones\n",
    "        print(f\"     Características finales: {final_features.shape}\")\n",
    "        \n",
    "        return final_features, feature_names\n",
    "\n",
    "def main_heterogeneous_neo4j():\n",
    "    \"\"\"\n",
    "    Pipeline principal que usa datos reales de Neo4j\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # 1. Load data from Neo4j\n",
    "    logger.info(\"🔌 Connecting to Neo4j and loading data...\")\n",
    "    \n",
    "    try:\n",
    "        neo4j_loader = Neo4jDataLoader(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "        raw_data = neo4j_loader.load_heterogeneous_data()\n",
    "        neo4j_loader.close()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error connecting to Neo4j: {e}\")\n",
    "        return None\n",
    "        \n",
    "    # 2. Procesar a formato PyTorch Geometric\n",
    "    logger.info(\"🔄 Processing into PyTorch Geometric format...\")\n",
    "    processor = HeterogeneousGraphProcessor()\n",
    "    hetero_data = processor.create_heterogeneous_graph(raw_data)\n",
    "    \n",
    "    # Verificar que tenemos datos de contadores\n",
    "    if 'contador' not in hetero_data.node_types:\n",
    "        logger.error(\"We couldn' find CONTADOR type of nodes. Abortting...\")\n",
    "        return None\n",
    "        \n",
    "    # 3. Split the data\n",
    "    logger.info(\"📊 Splitting the data...\")\n",
    "    num_contadores = hetero_data['contador'].x.size(0)\n",
    "    indices = torch.randperm(num_contadores)\n",
    "        \n",
    "    train_size = int(0.6 * num_contadores)\n",
    "    val_size = int(0.2 * num_contadores)\n",
    "        \n",
    "    train_mask = torch.zeros(num_contadores, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_contadores, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_contadores, dtype=torch.bool)\n",
    "        \n",
    "    train_mask[indices[:train_size]] = True\n",
    "    val_mask[indices[train_size:train_size + val_size]] = True\n",
    "    test_mask[indices[train_size + val_size:]] = True\n",
    "\n",
    "    # 4. Create heterogeneous model\n",
    "    logger.info(\"🧠 Creating heterogeneous GNN model...\")\n",
    "        \n",
    "    # Adjust input dims in terms of real data:\n",
    "    for node_type in hetero_data.node_types:\n",
    "        actual_dim = hetero_data[node_type].x.size(1)\n",
    "        NODE_DIMS_ADV[node_type] = actual_dim\n",
    "        logger.info(f\"   {node_type}: {actual_dim} características\")\n",
    "\n",
    "    model = HeterogeneousFraudGNN(\n",
    "        metadata=hetero_data.metadata(),\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_classes=len(FRAUD_CLASSES),\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "        \n",
    "    # Total number of params\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    logger.info(f\"Params of the model: {total_params:,}\")\n",
    "        \n",
    "    # 5. Training\n",
    "    logger.info(\"🎓 Training heterogeneous model...\")\n",
    "    trainer = HeterogeneousFraudTrainer(model, device)\n",
    "    hetero_data = hetero_data.to(device)\n",
    "    train_mask = train_mask.to(device)\n",
    "    val_mask = val_mask.to(device)\n",
    "    test_mask = test_mask.to(device)\n",
    "        \n",
    "    history = trainer.train(hetero_data, train_mask, val_mask)\n",
    "\n",
    "    # 6. Evaluation\n",
    "    logger.info(\"📊 Evaluating the model...\")\n",
    "    \n",
    "    # Load the best model in case it exists\n",
    "    best_model_path = 'models/best_hetero_fraud_model.pth'\n",
    "    if os.path.exists(best_model_path):\n",
    "        model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    \n",
    "    evaluator = HeterogeneousEvaluator(model, device)\n",
    "    test_results = evaluator.evaluate_detailed(hetero_data, test_mask)\n",
    "        \n",
    "    # 7. Attention analysis (if it is available)\n",
    "    attention_analysis = None\n",
    "    try:\n",
    "        attention_analysis = evaluator.analyze_attention_weights(hetero_data)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"We couldn't analyze the attention: {e}\")\n",
    "        \n",
    "    # 8. Show the results\n",
    "    logger.info(\"📈 FINAL RESULTS:\")\n",
    "    logger.info(f\"Best precision ih validation: {history['best_val_acc']:.4f}\")\n",
    "    logger.info(f\"ROC AUC: {test_results['roc_auc']:.4f}\")\n",
    "    logger.info(\"\\nClassification report:\")\n",
    "    print(test_results['classification_report'])\n",
    "        \n",
    "    if attention_analysis:\n",
    "        logger.info(\"\\nPesos de atención por tipo de información:\")\n",
    "        for info_type, weight in attention_analysis.items():\n",
    "            logger.info(f\"  {info_type}: {weight:.3f}\")\n",
    "        \n",
    "    # 9. Visualizar resultados\n",
    "    plot_heterogeneous_results(history, test_results, attention_analysis)\n",
    "        \n",
    "    # 10. Detectar fraudes potenciales\n",
    "    logger.info(\"🔍 Detectando fraudes potenciales...\")\n",
    "    all_predictions = evaluator.predict(hetero_data)\n",
    "    all_probabilities = evaluator.predict_proba(hetero_data)\n",
    "    \n",
    "    # Casos con alta probabilidad de fraude\n",
    "    potential_frauds = []\n",
    "    for i in range(len(all_predictions)):\n",
    "        if hetero_data['contador'].y[i] == 0:  # Etiquetado como normal\n",
    "            fraud_prob = all_probabilities[i][1] + all_probabilities[i][2]\n",
    "            if fraud_prob > 0.7:  # Alta probabilidad de fraude\n",
    "                # Buscar el NIS correspondiente\n",
    "                contador_nis = raw_data['nodes']['contador'].iloc[i]['nis_rad']\n",
    "                potential_frauds.append({\n",
    "                    'contador_idx': i,\n",
    "                    'nis_rad': contador_nis,\n",
    "                    'fraud_probability': fraud_prob,\n",
    "                    'predicted_class': all_predictions[i]\n",
    "                })\n",
    "    \n",
    "    logger.info(f\"Fraudes potenciales detectados: {len(potential_frauds)}\")\n",
    "    \n",
    "    # Mostrar top 5 casos más sospechosos\n",
    "    if potential_frauds:\n",
    "        potential_frauds.sort(key=lambda x: x['fraud_probability'], reverse=True)\n",
    "        logger.info(\"Top 5 casos más sospechosos:\")\n",
    "        for i, case in enumerate(potential_frauds[:5], 1):\n",
    "            logger.info(f\"  {i}. NIS: {case['nis_rad']}, Prob: {case['fraud_probability']:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_results': test_results,\n",
    "        'attention_analysis': attention_analysis,\n",
    "        'potential_frauds': potential_frauds,\n",
    "        'hetero_data': hetero_data,\n",
    "        'raw_data': raw_data\n",
    "    }\n",
    "\n",
    "class HeterogeneousModelAnalyzer:\n",
    "    \"\"\"Analizador avanzado para modelos heterogéneos con datos de Neo4j\"\"\"\n",
    "    \n",
    "    def __init__(self, model, data, raw_data, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.data = data.to(device)\n",
    "        self.raw_data = raw_data\n",
    "        self.device = device\n",
    "        \n",
    "    def analyze_node_importance(self, node_type='contador', top_k=10):\n",
    "        \"\"\"Analiza la importancia de nodos de un tipo específico\"\"\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self.model(self.data.x_dict, self.data.edge_index_dict)\n",
    "            \n",
    "            if node_type == 'contador':\n",
    "                node_embeddings = out\n",
    "                node_labels = self.data['contador'].y if hasattr(self.data['contador'], 'y') else None\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "            # Calculate importance based on the norm of the embedding. This can be done with torch.norm\n",
    "            importance_scores = torch.norm(node_embeddings, dim=1)\n",
    "            \n",
    "            # Get the top-k most important nodes:\n",
    "            top_indices = torch.topk(importance_scores, top_k).indices\n",
    "            \n",
    "            results = []\n",
    "            for idx in top_indices:\n",
    "                # Looks for info of the CONTADOR in the original data\n",
    "                contador_info = self.raw_data['nodes']['contador'].iloc[idx.item()]\n",
    "                \n",
    "                results.append({\n",
    "                    'node_index': idx.item(),\n",
    "                    'nis_rad': contador_info['nis_rad'],\n",
    "                    'marca': contador_info.get('marca', 'N/A'),\n",
    "                    'modelo': contador_info.get('modelo', 'N/A'),\n",
    "                    'importance_score': importance_scores[idx].item(),\n",
    "                    'predicted_class': node_embeddings[idx].argmax().item(),\n",
    "                    'true_class': node_labels[idx].item() if node_labels is not None else None\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_fraud_risk_report(self, threshold=0.7):\n",
    "        \"\"\"\n",
    "        Generates a risk fraud report with detailed information.\n",
    "        \"\"\"\n",
    "        \n",
    "        evaluator = HeterogeneousEvaluator(self.model, self.device)\n",
    "        probabilities = evaluator.predict_proba(self.data)\n",
    "        \n",
    "        # Calculate risk scores\n",
    "        fraud_scores = probabilities[:, 1] + probabilities[:, 2]  # Fraude + Irregularidad\n",
    "        \n",
    "        high_risk_indices = np.where(fraud_scores > threshold)[0]\n",
    "        \n",
    "        report = {\n",
    "            'total_contadores': len(probabilities),\n",
    "            'high_risk_count': len(high_risk_indices),\n",
    "            'high_risk_percentage': len(high_risk_indices) / len(probabilities) * 100,\n",
    "            'average_fraud_score': np.mean(fraud_scores),\n",
    "            'high_risk_nodes': []\n",
    "        }\n",
    "        \n",
    "        for idx in high_risk_indices:\n",
    "            # Obtain detailed info about the CONTADOR\n",
    "            contador_info = self.raw_data['nodes']['contador'].iloc[idx]\n",
    "            \n",
    "            report['high_risk_nodes'].append({\n",
    "                'node_index': int(idx),\n",
    "                'nis_rad': contador_info['nis_rad'],\n",
    "                'marca': contador_info.get('marca', 'N/A'),\n",
    "                'modelo': contador_info.get('modelo', 'N/A'),\n",
    "                'ubicacion': contador_info.get('ubicacion', 'N/A'),\n",
    "                'fraud_probability': float(fraud_scores[idx]),\n",
    "                'predicted_class': int(probabilities[idx].argmax()),\n",
    "                'class_probabilities': {\n",
    "                    'normal': float(probabilities[idx][0]),\n",
    "                    'fraude': float(probabilities[idx][1]),\n",
    "                    'irregularidad': float(probabilities[idx][2])\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Sort fraud probabilities\n",
    "        report['high_risk_nodes'].sort(key=lambda x: x['fraud_probability'], reverse=True)\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f45250-4df3-44d1-b14d-224a310f16ce",
   "metadata": {},
   "source": [
    "## 5.9. Advanced analysis of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08b43d0b-43c4-4c07-964f-3b614c5180ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousModelAnalyzer:\n",
    "    \"\"\"Analizador avanzado para modelos heterogéneos\"\"\"\n",
    "    \n",
    "    def __init__(self, model, data, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.data = data.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def analyze_node_importance(self, node_type='contador', top_k=10):\n",
    "        \"\"\"Analiza la importancia de nodos de un tipo específico\"\"\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Obtener embeddings finales\n",
    "            out = self.model(self.data.x_dict, self.data.edge_index_dict)\n",
    "            \n",
    "            if node_type == 'contador':\n",
    "                node_embeddings = out\n",
    "                node_labels = self.data['contador'].y\n",
    "            else:\n",
    "                # Para otros tipos de nodos, necesitaríamos extraer sus embeddings\n",
    "                return None\n",
    "            \n",
    "            # Calcular importancia basada en la norma del embedding\n",
    "            importance_scores = torch.norm(node_embeddings, dim=1)\n",
    "            \n",
    "            # Obtener top-k nodos más importantes\n",
    "            top_indices = torch.topk(importance_scores, top_k).indices\n",
    "            \n",
    "            results = []\n",
    "            for idx in top_indices:\n",
    "                results.append({\n",
    "                    'node_index': idx.item(),\n",
    "                    'importance_score': importance_scores[idx].item(),\n",
    "                    'predicted_class': node_embeddings[idx].argmax().item(),\n",
    "                    'true_class': node_labels[idx].item() if node_labels is not None else None\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_edge_importance(self, edge_type):\n",
    "        \"\"\"Analiza la importancia de diferentes tipos de edges\"\"\"\n",
    "        \n",
    "        # Implementación simplificada\n",
    "        if edge_type in self.data.edge_index_dict:\n",
    "            edge_index = self.data.edge_index_dict[edge_type]\n",
    "            num_edges = edge_index.size(1)\n",
    "            \n",
    "            return {\n",
    "                'edge_type': edge_type,\n",
    "                'num_edges': num_edges,\n",
    "                'source_nodes': edge_index[0].unique().size(0),\n",
    "                'target_nodes': edge_index[1].unique().size(0),\n",
    "                'avg_degree': num_edges / edge_index[0].unique().size(0) if edge_index[0].unique().size(0) > 0 else 0\n",
    "            }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def generate_fraud_risk_report(self, threshold=0.7):\n",
    "        \"\"\"Genera reporte de riesgo de fraude\"\"\"\n",
    "        \n",
    "        evaluator = HeterogeneousEvaluator(self.model, self.device)\n",
    "        probabilities = evaluator.predict_proba(self.data)\n",
    "        \n",
    "        # Calcular scores de riesgo\n",
    "        fraud_scores = probabilities[:, 1] + probabilities[:, 2]  # Fraude + Irregularidad\n",
    "        \n",
    "        high_risk_indices = np.where(fraud_scores > threshold)[0]\n",
    "        \n",
    "        report = {\n",
    "            'total_contadores': len(probabilities),\n",
    "            'high_risk_count': len(high_risk_indices),\n",
    "            'high_risk_percentage': len(high_risk_indices) / len(probabilities) * 100,\n",
    "            'average_fraud_score': np.mean(fraud_scores),\n",
    "            'high_risk_nodes': []\n",
    "        }\n",
    "        \n",
    "        for idx in high_risk_indices:\n",
    "            report['high_risk_nodes'].append({\n",
    "                'node_index': int(idx),\n",
    "                'FRAUD_PROBABILITY_ADV': float(fraud_scores[idx]),\n",
    "                'predicted_class': int(probabilities[idx].argmax()),\n",
    "                'class_probabilities': probabilities[idx].tolist()\n",
    "            })\n",
    "        \n",
    "        # Ordenar por probabilidad de fraude\n",
    "        report['high_risk_nodes'].sort(key=lambda x: x['FRAUD_PROBABILITY_ADV'], reverse=True)\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d590fd00-de29-47bd-80b1-d8c5101895e3",
   "metadata": {},
   "source": [
    "## 5.10. Pipeline execution (simulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb17dce-3224-4be0-8e37-ce1f7addd1ff",
   "metadata": {},
   "source": [
    "In this step we will be launching the previously defined pipeline:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "daaa3050-a5cf-4666-b986-f3a5fc5ac1b8",
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Ejecutar pipeline heterogéneo\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    print(\"🚀 STARTING GNN HETEROGENEOUS MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = main_heterogeneous()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🎯 TRAINING HAS BEEN COMPLETED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"📊 ROC AUC: {results['test_results']['roc_auc']:.4f}\")\n",
    "    print(f\"🎯 Best Val Accuracy: {results['history']['best_val_acc']:.4f}\")\n",
    "    print(f\"🔍 Potential frauds: {len(results['potential_frauds'])}\")\n",
    "    print(f\"💾 Model saved in: models/best_hetero_fraud_model.pth\")\n",
    "    \n",
    "    # Aditional analysis\n",
    "    analyzer = HeterogeneousModelAnalyzer(results['model'], results['hetero_data'])\n",
    "    \n",
    "    # Analyze the nodes importance\n",
    "    important_nodes = analyzer.analyze_node_importance(top_k=5)\n",
    "    print(f\"\\n🔝 Top 5 nodos más importantes:\")\n",
    "    for i, node in enumerate(important_nodes, 1):\n",
    "        print(f\"  {i}. Nodo {node['node_index']}: Score {node['importance_score']:.3f}\")\n",
    "    \n",
    "    # Generar reporte de riesgo\n",
    "    risk_report = analyzer.generate_fraud_risk_report(threshold=0.7)\n",
    "    print(f\"\\n⚠️  Reporte de Riesgo:\")\n",
    "    print(f\"  Total contadores: {risk_report['total_contadores']:,}\")\n",
    "    print(f\"  Alto riesgo: {risk_report['high_risk_count']} ({risk_report['high_risk_percentage']:.1f}%)\")\n",
    "    print(f\"  Score promedio: {risk_report['average_fraud_score']:.3f}\")\n",
    "    \n",
    "    print(\"\\n✅ ANÁLISIS COMPLETADO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd98a9-12ef-4cfc-b6a8-84d875c1159a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "117bafcb-c9ea-4575-8c6b-c00ab6952506",
   "metadata": {},
   "source": [
    "## 5.11. Pipeline execution (Real data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f19b220-0c41-4cae-964f-e12d5f26856d",
   "metadata": {},
   "source": [
    "In this step we will be launching the previously defined pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e4bcc8-ade9-423a-bad9-7ced291c0de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cpu\n",
      "INFO:__main__:🔌 Connecting to Neo4j and loading data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 STARTING GNN HETEROGENEOUS MODEL WITH NEO4J DATA\n",
      "============================================================\n",
      "🔌 Connecting to Neo4j and loading data ...\n",
      "🔗 Loading relations from Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:🔄 Processing into PyTorch Geometric format...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Etiquetas desde BD: 4933 Normal, 38 Fraude, 34 Irregularidad\n",
      "✅ Cargados 10841 nodos y 64912 relaciones desde Neo4j\n",
      "   - contador: 5005 nodos\n",
      "   - suministro: 5005 nodos\n",
      "   - comercializadora: 21 nodos\n",
      "   - ubicacion: 504 nodos\n",
      "   - concentrador: 102 nodos\n",
      "   - expediente_fraude: 204 nodos\n",
      "🔄 Procesando grafo heterogéneo desde Neo4j...\n",
      "   Procesando nodos tipo: contador\n",
      "     Características finales: (5005, 64)\n",
      "     ✅ 5005 nodes, 64 features\n",
      "   Procesando nodos tipo: suministro\n",
      "     Características finales: (5005, 47)\n",
      "     ✅ 5005 nodes, 47 features\n",
      "   Procesando nodos tipo: comercializadora\n",
      "     Características finales: (21, 11)\n",
      "     ✅ 21 nodes, 11 features\n",
      "   Procesando nodos tipo: ubicacion\n",
      "     Características finales: (504, 24)\n",
      "     ✅ 504 nodes, 24 features\n",
      "   Procesando nodos tipo: concentrador\n",
      "     Características finales: (102, 15)\n",
      "     ✅ 102 nodes, 15 features\n",
      "   Procesando nodos tipo: expediente_fraude\n",
      "     Características finales: (204, 55)\n",
      "     ✅ 204 nodes, 55 features\n",
      "   ✅ mide: 5010 relaciones (contador -> suministro)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:📊 Splitting the data...\n",
      "INFO:__main__:🧠 Creating heterogeneous GNN model...\n",
      "INFO:__main__:   contador: 64 características\n",
      "INFO:__main__:   suministro: 47 características\n",
      "INFO:__main__:   comercializadora: 11 características\n",
      "INFO:__main__:   ubicacion: 24 características\n",
      "INFO:__main__:   concentrador: 15 características\n",
      "INFO:__main__:   expediente_fraude: 55 características\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ comunica_via: 5006 relaciones (contador -> concentrador)\n",
      "   ✅ ubicado_en: 5005 relaciones (contador -> ubicacion)\n",
      "   ✅ involucrado_en: 154 relaciones (contador -> expediente_fraude)\n",
      "   ✅ contratado_con: 5005 relaciones (suministro -> comercializadora)\n",
      "   ✅ cerca_de: 15030 relaciones (contador -> contador)\n",
      "   ✅ similar_a: 29702 relaciones (contador -> contador)\n",
      "✅ Heterogeneous graph created with 6 type of nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Params of the model: 218,248\n",
      "INFO:__main__:🎓 Training heterogeneous model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎓 Iniciando entrenamiento por 300 épocas...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configure seeds\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    \n",
    "    print(\"🚀 STARTING GNN HETEROGENEOUS MODEL WITH NEO4J DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create the directory \"models\" in case it douesn't exist\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # Execute the principal pipeline\n",
    "    results = main_heterogeneous_neo4j()\n",
    "    \n",
    "    if results is not None:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"🎯 HETEROGENEOUS TRAINING COMPLETED\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"📊 ROC AUC: {results['test_results']['roc_auc']:.4f}\")\n",
    "        print(f\"🎯 Best Val Accuracy: {results['history']['best_val_acc']:.4f}\")\n",
    "        print(f\"🔍 Potential fraud: {len(results['potential_frauds'])}\")\n",
    "        print(f\"💾 Model saved in: models/best_hetero_fraud_model.pth\")\n",
    "        \n",
    "        # Additional analysis\n",
    "        analyzer = HeterogeneousModelAnalyzer(\n",
    "            results['model'], \n",
    "            results['hetero_data'], \n",
    "            results['raw_data']\n",
    "        )\n",
    "        \n",
    "        # Analyze nodes importance\n",
    "        important_nodes = analyzer.analyze_node_importance(top_k=5)\n",
    "        print(f\"\\n🔝 Top 5 nodos más importantes:\")\n",
    "        for i, node in enumerate(important_nodes, 1):\n",
    "            print(f\"  {i}. NIS: {node['nis_rad']} | {node['marca']} {node['modelo']} | Score: {node['importance_score']:.3f}\")\n",
    "        \n",
    "        # Generate risk report\n",
    "        risk_report = analyzer.generate_fraud_risk_report(threshold=0.7)\n",
    "        print(f\"\\n⚠️  Risk report:\")\n",
    "        print(f\"  Total CONTADORES: {risk_report['total_contadores']:,}\")\n",
    "        print(f\"  High risk: {risk_report['high_risk_count']} ({risk_report['high_risk_percentage']:.1f}%)\")\n",
    "        print(f\"  Mean score value: {risk_report['average_fraud_score']:.3f}\")\n",
    "        \n",
    "        if risk_report['high_risk_nodes']:\n",
    "            print(f\"\\n🚨 Top 3 contadores de mayor riesgo:\")\n",
    "            for i, node in enumerate(risk_report['high_risk_nodes'][:3], 1):\n",
    "                print(f\"  {i}. NIS: {node['nis_rad']} | {node['marca']} {node['modelo']} | Riesgo: {node['fraud_probability']:.3f}\")\n",
    "        \n",
    "        print(\"\\n✅ ANALYSIS COMPLETED\")\n",
    "    else:\n",
    "        print(\"\\n❌ ERROR: No se pudo completar el análisis\")\n",
    "        print(\"Verifica la conexión a Neo4j y que existan datos en la base de datos.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
