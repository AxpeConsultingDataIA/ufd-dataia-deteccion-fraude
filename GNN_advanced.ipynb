{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abbaa0dc-5ede-415d-a210-4d421d21bbd9",
   "metadata": {},
   "source": [
    "# 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5084a81a-243f-47ad-b7a3-0c2f42e26c8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (5.28.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from neo4j) (2024.1)\n",
      "Requirement already satisfied: torch in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: torch_geometric in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch_geometric) (3.11.10)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch_geometric) (2025.3.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch_geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch_geometric) (3.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch_geometric) (2.32.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp->torch_geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch_geometric) (3.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from requests->torch_geometric) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from requests->torch_geometric) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from requests->torch_geometric) (2025.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->torch_geometric) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install neo4j\n",
    "!pip install torch\n",
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a9af8b-d261-456d-959c-d5f1e7b0ab9b",
   "metadata": {},
   "source": [
    "# 2. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbdcc8bb-cfac-432d-8c07-8571b15d198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "import logging\n",
    "from datetime import datetime, timedelta, date\n",
    "import random\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "import os\n",
    "from neo4j.time import Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7ddd7de-f4e9-41d9-aed8-501e08f26710",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0138ae9e-4040-4ff5-99a4-15748514a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c718520-2ce9-4d3b-b98f-a240183d0bb2",
   "metadata": {},
   "source": [
    "# 3. Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b4b2c4-a3c8-41bc-8b64-ff344a404ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = \"neo4j://localhost:7687\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"password\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2fe2c15-5acf-467c-89de-92adf8ad6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff0660c-cbeb-445d-9f88-f82de5460bf0",
   "metadata": {},
   "source": [
    "# 4. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b57e12b0-d347-4adb-b668-2461419062c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_neo4j_date(x):\n",
    "    if isinstance(x, Date):\n",
    "        return date(x.year, x.month, x.day)\n",
    "    elif isinstance(x, datetime):\n",
    "        return x.date()\n",
    "    elif isinstance(x, date):\n",
    "        return x\n",
    "    else:\n",
    "        return pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "471b719a-a9c6-4e0d-87cd-22c73dd25d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heterogeneous_results(history, test_results, attention_analysis):\n",
    "    \"\"\"\n",
    "    Visualiza los resultados del modelo heterog√©neo\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Historial de entrenamiento\n",
    "    axes[0, 0].plot(history['train_losses'], label='Train Loss', alpha=0.7)\n",
    "    axes[0, 0].plot(history['val_losses'], label='Val Loss', alpha=0.7)\n",
    "    axes[0, 0].set_title('P√©rdida durante el entrenamiento')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracies\n",
    "    axes[0, 1].plot(history['train_accuracies'], label='Train Accuracy', alpha=0.7)\n",
    "    axes[0, 1].plot(history['val_accuracies'], label='Val Accuracy', alpha=0.7)\n",
    "    axes[0, 1].set_title('Precisi√≥n durante el entrenamiento')\n",
    "    axes[0, 1].set_xlabel('√âpoca')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Matriz de confusi√≥n\n",
    "    sns.heatmap(test_results['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Fraude', 'Irregularidad'],\n",
    "                yticklabels=['Normal', 'Fraude', 'Irregularidad'], ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Matriz de Confusi√≥n')\n",
    "    axes[1, 0].set_xlabel('Predicci√≥n')\n",
    "    axes[1, 0].set_ylabel('Verdadero')\n",
    "    \n",
    "    # 3. Pesos de atenci√≥n\n",
    "    if attention_analysis:\n",
    "        info_types = list(attention_analysis.keys())\n",
    "        weights = list(attention_analysis.values())\n",
    "        \n",
    "        bars = axes[1, 1].bar(range(len(info_types)), weights, alpha=0.7, color='skyblue')\n",
    "        axes[1, 1].set_title('Pesos de atenci√≥n por tipo de informaci√≥n')\n",
    "        axes[1, 1].set_xlabel('Tipo de informaci√≥n')\n",
    "        axes[1, 1].set_ylabel('Peso de atenci√≥n')\n",
    "        axes[1, 1].set_xticks(range(len(info_types)))\n",
    "        axes[1, 1].set_xticklabels(info_types, rotation=45, ha='right')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Agregar valores a las barras\n",
    "        for bar, weight in zip(bars, weights):\n",
    "            height = bar.get_height()\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                            f'{weight:.3f}', ha='center', va='bottom')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No hay an√°lisis de atenci√≥n disponible', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('An√°lisis de Atenci√≥n')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a9eb43-7da9-40c8-b182-7530577a9fb3",
   "metadata": {},
   "source": [
    "# 5. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787ea602-b4b3-4a20-826c-44cdd2c7334c",
   "metadata": {},
   "source": [
    "## 5.1. Load data from Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f218b0-b73a-46bb-a87e-0406392fe152",
   "metadata": {},
   "source": [
    "We will be writing the code in order to load the data from neo4j when the time comes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "852aeab5-b980-409b-8901-8dbb84e0ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jDataLoader:\n",
    "    \"\"\"\n",
    "    Loads heterogeneous data from Neo4j.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, uri: str, username: str, password: str):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Used to close the connection\n",
    "        \"\"\"\n",
    "        self.driver.close()\n",
    "    \n",
    "    def load_heterogeneous_data(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Loads all the heterogeneous data from Neo4j. The idea is to use all the defined functions for each node.\n",
    "        \"\"\"\n",
    "        print(\"üîå Connecting to Neo4j and loading data ...\")\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            # Load each type of node\n",
    "            node_data = {\n",
    "                'contador': self._load_contador_nodes(session),\n",
    "                'suministro': self._load_suministro_nodes(session),\n",
    "                'comercializadora': self._load_comercializadora_nodes(session),\n",
    "                'ubicacion': self._load_ubicacion_nodes(session),\n",
    "                'concentrador': self._load_concentrador_nodes(session),\n",
    "                'expediente_fraude': self._load_expediente_fraude_nodes(session)\n",
    "            }\n",
    "            \n",
    "            # Load relations\n",
    "            edge_data = self._load_heterogeneous_edges(session, node_data)\n",
    "            \n",
    "            # Assign fraud labels based on expedientes.\n",
    "            node_data['contador'] = self._assign_fraud_labels_from_expedientes(\n",
    "                node_data['contador'], node_data['expediente_fraude'], session\n",
    "            )\n",
    "        \n",
    "        # Let's show now some statistics of the graph\n",
    "        total_nodes = sum(len(nodes) for nodes in node_data.values())\n",
    "        total_edges = sum(len(edges) for edges in edge_data.values())\n",
    "        print(f\"‚úÖ Cargados {total_nodes} nodos y {total_edges} relaciones desde Neo4j\")\n",
    "        \n",
    "        for node_type, df in node_data.items():\n",
    "            print(f\"   - {node_type}: {len(df)} nodos\")\n",
    "        \n",
    "        return {\n",
    "            'nodes': node_data,\n",
    "            'edges': edge_data\n",
    "        }\n",
    "    \n",
    "    def _load_contador_nodes(self, session) -> pd.DataFrame:\n",
    "        \"\"\"Loads CONTADOR nodes from Neo4j\"\"\"\n",
    "        \n",
    "        query = \"\"\"\n",
    "        MATCH (c:CONTADOR)\n",
    "        OPTIONAL MATCH (c)-[:GENERA_MEDICION]->(m:MEDICION)\n",
    "        WITH c, \n",
    "             COUNT(m) as num_mediciones,\n",
    "             AVG(m.energia_activa) as consumo_promedio,\n",
    "             MAX(m.energia_activa) as consumo_maximo,\n",
    "             MIN(m.energia_activa) as consumo_minimo,\n",
    "             STDEV(m.energia_activa) as variabilidad_consumo\n",
    "        RETURN c.nis_rad as nis_rad,\n",
    "               c.numero_contador as numero_contador,\n",
    "               c.marca_contador as marca,\n",
    "               c.modelo_contador as modelo,\n",
    "               c.tipo_aparato as tipo_aparato,\n",
    "               c.telegest_activo as telegest_activo,\n",
    "               c.estado_tg as estado_comunicacion,\n",
    "               c.tension as tension_nominal,\n",
    "               c.fases_contador as fases,\n",
    "               c.potencia_maxima as potencia_maxima,\n",
    "               c.fecha_instalacion as fecha_instalacion,\n",
    "               c.version_firmware as version_firmware,\n",
    "               c.estado_contrato as estado_contrato,\n",
    "               COALESCE(num_mediciones, 0) as num_mediciones,\n",
    "               COALESCE(consumo_promedio, 0) as consumo_promedio_diario,\n",
    "               COALESCE(consumo_maximo, 0) as consumo_maximo_registrado,\n",
    "               COALESCE(consumo_minimo, 0) as consumo_minimo_registrado,\n",
    "               COALESCE(variabilidad_consumo, 0) as variabilidad_consumo\n",
    "        ORDER BY c.nis_rad\n",
    "        \"\"\"\n",
    "        # Run the query just defined:\n",
    "        result = session.run(query)\n",
    "        records = [dict(record) for record in result]\n",
    "        \n",
    "        if not records:\n",
    "            print(\"‚ö†Ô∏è We couldn't find CONTADORES.\")\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        \n",
    "        # Add calculated features\n",
    "        df[\"fecha_instalacion\"] = df[\"fecha_instalacion\"].map(convert_neo4j_date)\n",
    "        df[\"fecha_instalacion\"] = pd.to_datetime(df[\"fecha_instalacion\"])\n",
    "\n",
    "        # Calcula los d√≠as como enteros\n",
    "        df[\"dias_desde_instalacion\"] = (\n",
    "            pd.Timestamp.now().normalize() - df[\"fecha_instalacion\"]\n",
    "        ).dt.days.fillna(0)\n",
    "        \n",
    "        # Let's create a unique ID for the mapping.\n",
    "        df['node_id'] = df['nis_rad']\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _load_suministro_nodes(self, session) -> pd.DataFrame:\n",
    "        \"\"\"Loads SUMINISTRO nodes from Neo4j\"\"\"\n",
    "        \n",
    "        query = \"\"\"\n",
    "        MATCH (s:SUMINISTRO)\n",
    "        RETURN s.nis_rad as nis_rad,\n",
    "               s.fecha_alta_suministro as fecha_alta,\n",
    "               s.estado_contrato as estado_contrato,\n",
    "               s.tipo_punto as tipo_suministro,\n",
    "               s.potencia_contratada as potencia_contratada,\n",
    "               s.potencia_maxima as potencia_maxima_demandada,\n",
    "               s.tarifa_activa as tarifa_activa,\n",
    "               s.tension_suministro as tension_suministro,\n",
    "               s.fases_suministro as fases_suministro,\n",
    "               s.cnae as cnae,\n",
    "               s.comercializadora_codigo as comercializadora_codigo\n",
    "        ORDER BY s.nis_rad\n",
    "        \"\"\"\n",
    "        \n",
    "        result = session.run(query)\n",
    "        records = [dict(record) for record in result]\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        df['node_id'] = df['nis_rad']\n",
    "        \n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def _load_comercializadora_nodes(self, session) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads COMERCIALIZADORA nodes from Neo4j\n",
    "        \"\"\"\n",
    "        \n",
    "        query = \"\"\"\n",
    "        MATCH (c:COMERCIALIZADORA)\n",
    "        RETURN c.codigo_comercializadora as codigo_comercializadora,\n",
    "               c.nombre_comercializadora as nombre_comercializadora\n",
    "        ORDER BY c.codigo_comercializadora\n",
    "        \"\"\"\n",
    "        \n",
    "        result = session.run(query)\n",
    "        records = [dict(record) for record in result]\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        df['node_id'] = df['codigo_comercializadora']\n",
    "        \n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def _load_ubicacion_nodes(self, session) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads UBICACION nodes from Neo4j\n",
    "        \"\"\"\n",
    "        \n",
    "        query = \"\"\"\n",
    "        MATCH (u:UBICACION)\n",
    "        RETURN u.coordenada_x as coordenada_x,\n",
    "               u.coordenada_y as coordenada_y,\n",
    "               u.codigo_postal as codigo_postal,\n",
    "               u.area_ejecucion as area_ejecucion,\n",
    "               toString(u.coordenada_x) + '_' + toString(u.coordenada_y) as node_id\n",
    "        ORDER BY u.coordenada_x, u.coordenada_y\n",
    "        \"\"\"\n",
    "        \n",
    "        result = session.run(query)\n",
    "        records = [dict(record) for record in result]\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        \n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def _load_concentrador_nodes(self, session) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads CONCENTRADOR nodes from Neo4j\n",
    "        \"\"\"\n",
    "        \n",
    "        query = \"\"\"\n",
    "        MATCH (c:CONCENTRADOR)\n",
    "        RETURN c.concentrador_id as concentrador_id,\n",
    "               c.version_concentrador as version_concentrador,\n",
    "               c.estado_comunicacion as estado_comunicacion,\n",
    "               c.tipo_reporte as tipo_reporte\n",
    "        ORDER BY c.concentrador_id\n",
    "        \"\"\"\n",
    "        \n",
    "        result = session.run(query)\n",
    "        records = [dict(record) for record in result]\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        df['node_id'] = df['concentrador_id']\n",
    "        \n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def _load_expediente_fraude_nodes(self, session) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads EXPEDIENTE_FRAUDE nodes from Neo4j\n",
    "        \"\"\"\n",
    "        \n",
    "        query = \"\"\"\n",
    "        MATCH (e:EXPEDIENTE_FRAUDE)\n",
    "        RETURN e.nis_expediente as nis_expediente,\n",
    "               e.clasificacion_fraude as clasificacion_fraude,\n",
    "               e.tipo_anomalia as tipo_anomalia,\n",
    "               e.estado_expediente as estado_expediente,\n",
    "               e.fecha_acta as fecha_acta,\n",
    "               e.fecha_inicio_anomalia as fecha_inicio_anomalia,\n",
    "               e.fecha_fin_anomalia as fecha_fin_anomalia,\n",
    "               e.energia_liquidable as energia_liquidable,\n",
    "               e.valoracion_total as valoracion_total,\n",
    "               e.dias_liquidables as dias_liquidables,\n",
    "               e.porcentaje_liquidable as porcentaje_liquidable\n",
    "        ORDER BY e.nis_expediente\n",
    "        \"\"\"\n",
    "        \n",
    "        result = session.run(query)\n",
    "        records = [dict(record) for record in result]\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        if not df.empty:\n",
    "            df['node_id'] = df['nis_expediente']\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _load_heterogeneous_edges(self, session, node_data: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Loads all the heterogeneous relations from Neo4j\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"üîó Loading relations from Neo4j...\")\n",
    "        \n",
    "        edge_data = {}\n",
    "        \n",
    "        # 1. CONTADOR -> SUMINISTRO (relation MIDE_CONSUMO_DE)\n",
    "        edge_data[('contador', 'mide', 'suministro')] = self._load_edges_from_query(\n",
    "            session,\n",
    "            \"\"\"\n",
    "            MATCH (c:CONTADOR)-[:MIDE_CONSUMO_DE]->(s:SUMINISTRO)\n",
    "            RETURN c.nis_rad as source, s.nis_rad as target\n",
    "            \"\"\",\n",
    "            node_data['contador'], node_data['suministro']\n",
    "        )\n",
    "        \n",
    "        # 2. CONTADOR -> CONCENTRADOR (relation CONECTADO_A)\n",
    "        edge_data[('contador', 'comunica_via', 'concentrador')] = self._load_edges_from_query(\n",
    "            session,\n",
    "            \"\"\"\n",
    "            MATCH (c:CONTADOR)-[:CONECTADO_A]->(con:CONCENTRADOR)\n",
    "            RETURN c.nis_rad as source, con.concentrador_id as target\n",
    "            \"\"\",\n",
    "            node_data['contador'], node_data['concentrador']\n",
    "        )\n",
    "        \n",
    "        # 3. CONTADOR -> UBICACION (relation INSTALADO_EN)\n",
    "        edge_data[('contador', 'ubicado_en', 'ubicacion')] = self._load_edges_from_query(\n",
    "            session,\n",
    "            \"\"\"\n",
    "            MATCH (c:CONTADOR)-[:INSTALADO_EN]->(u:UBICACION)\n",
    "            RETURN c.nis_rad as source, \n",
    "                   toString(u.coordenada_x) + '_' + toString(u.coordenada_y) as target\n",
    "            \"\"\",\n",
    "            node_data['contador'], node_data['ubicacion']\n",
    "        )\n",
    "        \n",
    "        # 4. CONTADOR -> EXPEDIENTE_FRAUDE (relation INVOLUCRADO_EN_FRAUDE)\n",
    "        if not node_data['expediente_fraude'].empty:\n",
    "            edge_data[('contador', 'involucrado_en', 'expediente_fraude')] = self._load_edges_from_query(\n",
    "                session,\n",
    "                \"\"\"\n",
    "                MATCH (c:CONTADOR)-[:INVOLUCRADO_EN_FRAUDE]->(e:EXPEDIENTE_FRAUDE)\n",
    "                RETURN c.nis_rad as source, e.nis_expediente as target\n",
    "                \"\"\",\n",
    "                node_data['contador'], node_data['expediente_fraude']\n",
    "            )\n",
    "        \n",
    "        # 5. SUMINISTRO -> COMERCIALIZADORA (basado en c√≥digo comercializadora)\n",
    "        edge_data[('suministro', 'contratado_con', 'comercializadora')] = self._create_suministro_comercializadora_edges(\n",
    "            node_data['suministro'], node_data['comercializadora']\n",
    "        )\n",
    "        \n",
    "        # 6. Proximity relations between CONTADORES (These ones are simulated)\n",
    "        edge_data[('contador', 'cerca_de', 'contador')] = self._create_contador_proximidad_edges(\n",
    "            node_data['contador'], node_data['ubicacion']\n",
    "        )\n",
    "        \n",
    "        # 7. Similar CONTADORES in terms of MARCA/MODELO\n",
    "        edge_data[('contador', 'similar_a', 'contador')] = self._create_contador_similar_edges(\n",
    "            node_data['contador']\n",
    "        )\n",
    "        \n",
    "        return edge_data\n",
    "    \n",
    "    def _load_edges_from_query(self, session, query: str, source_df: pd.DataFrame, \n",
    "                             target_df: pd.DataFrame) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Carga relaciones desde Neo4j usando una query espec√≠fica\n",
    "        \"\"\"\n",
    "        \n",
    "        result = session.run(query)\n",
    "        edges = []\n",
    "        \n",
    "        # Crear mapeos de node_id a √≠ndice\n",
    "        source_map = {node_id: idx for idx, node_id in enumerate(source_df['node_id'])}\n",
    "        target_map = {node_id: idx for idx, node_id in enumerate(target_df['node_id'])}\n",
    "        \n",
    "        for record in result:\n",
    "            source_id = record['source']\n",
    "            target_id = record['target']\n",
    "            \n",
    "            if source_id in source_map and target_id in target_map:\n",
    "                source_idx = source_map[source_id]\n",
    "                target_idx = target_map[target_id]\n",
    "                edges.append([source_idx, target_idx])\n",
    "        \n",
    "        # If there are no relations in the db, we create some basic ones for testing\n",
    "        if not edges and len(source_df) > 0 and len(target_df) > 0:\n",
    "            print(f\"   ‚ö†Ô∏è We couldn't find real relations, creating test relations...\")\n",
    "            for i in range(min(len(source_df), len(target_df))):\n",
    "                target_idx = i % len(target_df)\n",
    "                edges.append([i, target_idx])\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def _create_suministro_comercializadora_edges(self, suministros: pd.DataFrame, \n",
    "                                                comercializadoras: pd.DataFrame) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Creates relations SUMINISTRO -> COMERCIALIZADORA based on the code\n",
    "        \"\"\"\n",
    "        \n",
    "        edges = []\n",
    "        com_map = {row['codigo_comercializadora']: idx \n",
    "                  for idx, row in comercializadoras.iterrows()}\n",
    "        \n",
    "        for idx, row in suministros.iterrows():\n",
    "            com_codigo = row.get('comercializadora_codigo', 'COM_001')\n",
    "            if com_codigo in com_map:\n",
    "                edges.append([idx, com_map[com_codigo]])\n",
    "            else:\n",
    "                # Assign randomly if the COMERCIALIZADORA does not exist\n",
    "                com_idx = np.random.randint(0, len(comercializadoras))\n",
    "                edges.append([idx, com_idx])\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def _create_contador_proximidad_edges(self, contadores: pd.DataFrame, \n",
    "                                        ubicaciones: pd.DataFrame) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Crea relaciones de proximidad entre contadores\n",
    "        \"\"\"\n",
    "        \n",
    "        edges = []\n",
    "        # Simulamos proximidad: cada contador se conecta con 2-4 vecinos\n",
    "        for i in range(len(contadores)):\n",
    "            num_neighbors = np.random.randint(2, 5)\n",
    "            for _ in range(num_neighbors):\n",
    "                neighbor_idx = np.random.randint(0, len(contadores))\n",
    "                if neighbor_idx != i:\n",
    "                    edges.append([i, neighbor_idx])\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def _create_contador_similar_edges(self, contadores: pd.DataFrame) -> List[List[int]]:\n",
    "        \"\"\"Crea relaciones entre contadores similares (misma marca/modelo)\"\"\"\n",
    "        \n",
    "        edges = []\n",
    "        \n",
    "        # Agrupar por marca y modelo\n",
    "        for marca in contadores['marca'].unique():\n",
    "            for modelo in contadores['modelo'].unique():\n",
    "                subset_indices = contadores[\n",
    "                    (contadores['marca'] == marca) & (contadores['modelo'] == modelo)\n",
    "                ].index.tolist()\n",
    "                \n",
    "                # Conectar contadores del mismo tipo\n",
    "                for i in range(len(subset_indices)):\n",
    "                    for j in range(i + 1, min(i + 4, len(subset_indices))):\n",
    "                        edges.append([subset_indices[i], subset_indices[j]])\n",
    "                        edges.append([subset_indices[j], subset_indices[i]])\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def _assign_fraud_labels_from_expedientes(self, contadores: pd.DataFrame, \n",
    "                                           expedientes: pd.DataFrame, session) -> pd.DataFrame:\n",
    "        \"\"\"Asigna etiquetas de fraude basadas en expedientes reales\"\"\"\n",
    "        \n",
    "        contadores = contadores.copy()\n",
    "        \n",
    "        # Inicializar todas las etiquetas como NORMAL\n",
    "        contadores['label'] = 'NORMAL'\n",
    "        \n",
    "        if not expedientes.empty:\n",
    "            # Obtener relaciones contador -> expediente\n",
    "            query = \"\"\"\n",
    "            MATCH (c:CONTADOR)-[:INVOLUCRADO_EN_FRAUDE]->(e:EXPEDIENTE_FRAUDE)\n",
    "            RETURN c.nis_rad as contador_nis, e.clasificacion_fraude as clasificacion\n",
    "            \"\"\"\n",
    "            \n",
    "            result = session.run(query)\n",
    "            fraud_relations = [dict(record) for record in result]\n",
    "            \n",
    "            # Crear mapeo de NIS a √≠ndice\n",
    "            nis_to_idx = {row['nis_rad']: idx for idx, row in contadores.iterrows()}\n",
    "            \n",
    "            # Asignar etiquetas basadas en clasificaci√≥n de fraude\n",
    "            fraud_count = 0\n",
    "            irregularity_count = 0\n",
    "            \n",
    "            for relation in fraud_relations:\n",
    "                contador_nis = relation['contador_nis']\n",
    "                clasificacion = relation['clasificacion']\n",
    "                \n",
    "                if contador_nis in nis_to_idx:\n",
    "                    idx = nis_to_idx[contador_nis]\n",
    "                    \n",
    "                    if clasificacion and 'FRAUDE' in clasificacion.upper():\n",
    "                        contadores.loc[idx, 'label'] = 'FRAUDE'\n",
    "                        fraud_count += 1\n",
    "                    elif clasificacion and any(term in clasificacion.upper() \n",
    "                                             for term in ['IRREGULARIDAD', 'ANOMALIA']):\n",
    "                        contadores.loc[idx, 'label'] = 'IRREGULARIDAD'\n",
    "                        irregularity_count += 1\n",
    "            \n",
    "            normal_count = len(contadores) - fraud_count - irregularity_count\n",
    "            print(f\"‚úÖ Etiquetas desde BD: {normal_count} Normal, {fraud_count} Fraude, {irregularity_count} Irregularidad\")\n",
    "        \n",
    "        else:\n",
    "            # Si no hay expedientes, simular algunas etiquetas para prueba\n",
    "            print(\"‚ö†Ô∏è No se encontraron expedientes de fraude, simulando etiquetas...\")\n",
    "            num_fraudes = max(1, int(len(contadores) * 0.05))  # 5% fraudes\n",
    "            num_irregularidades = max(1, int(len(contadores) * 0.03))  # 3% irregularidades\n",
    "            \n",
    "            fraud_indices = np.random.choice(len(contadores), num_fraudes, replace=False)\n",
    "            remaining_indices = [i for i in range(len(contadores)) if i not in fraud_indices]\n",
    "            irregularity_indices = np.random.choice(remaining_indices, num_irregularidades, replace=False)\n",
    "            \n",
    "            for idx in fraud_indices:\n",
    "                contadores.loc[idx, 'label'] = 'FRAUDE'\n",
    "            \n",
    "            for idx in irregularity_indices:\n",
    "                contadores.loc[idx, 'label'] = 'IRREGULARIDAD'\n",
    "            \n",
    "            normal_count = len(contadores) - num_fraudes - num_irregularidades\n",
    "            print(f\"‚úÖ Etiquetas simuladas: {normal_count} Normal, {num_fraudes} Fraude, {num_irregularidades} Irregularidad\")\n",
    "        \n",
    "        return contadores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3afc1fa-8c41-41ce-83b4-df3dc53ff4ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.2. Simualte the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a5635a-a75d-44dd-9afc-f7da9a6845bb",
   "metadata": {},
   "source": [
    "We will use a simulator of data as done in the GNN_dummy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "884f0f9f-52eb-4a48-85c9-549b0cbb24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousDataSimulator:\n",
    "    \"\"\"\n",
    "    Simulate the data for an heterogeneous complete graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        random.seed(RANDOM_SEED)\n",
    "        \n",
    "    def generate_heterogeneous_data(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Generates a complete heterogeneous dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"üé≤ Generando datos heterog√©neos complejos...\")\n",
    "        \n",
    "        # We generate nodes for each type:\n",
    "        node_data = {\n",
    "            'contador': self._generate_contador_nodes(),\n",
    "            'suministro': self._generate_suministro_nodes(),\n",
    "            'comercializadora': self._generate_comercializadora_nodes(),\n",
    "            'ubicacion': self._generate_ubicacion_nodes(),\n",
    "            'concentrador': self._generate_concentrador_nodes(),\n",
    "            'cliente': self._generate_cliente_nodes(),\n",
    "            'transformador': self._generate_transformador_nodes(),\n",
    "            'zona': self._generate_zona_nodes()\n",
    "        }\n",
    "        \n",
    "        # We generate heterogeneous relations also making use of the _generate_heterogeneous_edges function:\n",
    "        edge_data = self._generate_heterogeneous_edges(node_data)\n",
    "        \n",
    "        # We add labels of fraud to each contador.\n",
    "        node_data['contador'] = self._assign_fraud_labels(node_data['contador'])\n",
    "\n",
    "        # Show how many nodes and relations we have:\n",
    "        print(f\"‚úÖ Generados {sum(len(nodes) for nodes in node_data.values())} nodos\")\n",
    "        print(f\"‚úÖ Generadas {sum(len(edges) for edges in edge_data.values())} relaciones\")\n",
    "        \n",
    "        return {\n",
    "            'nodes': node_data,\n",
    "            'edges': edge_data\n",
    "        }\n",
    "    \n",
    "    def _generate_contador_nodes(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generates contador nodes with detailed features.\n",
    "        \"\"\"\n",
    "        \n",
    "        data = {\n",
    "            'node_id': [f\"CNT_{i:06d}\" for i in range(NUM_CONTADORES_ADV)],\n",
    "            \n",
    "            # Caracter√≠sticas t√©cnicas\n",
    "            'marca': np.random.choice(['LANDIS', 'ITRON', 'CIRCUTOR', 'SCHNEIDER', 'ABB'], NUM_CONTADORES_ADV),\n",
    "            'modelo': np.random.choice(['E350', 'E450', 'A1140', 'A1800', 'B23'], NUM_CONTADORES_ADV),\n",
    "            'tipo_aparato': np.random.choice(['ELECTRONICO', 'ELECTROMECANICO', 'HIBRIDO'], \n",
    "                                           NUM_CONTADORES_ADV, p=[0.7, 0.2, 0.1]),\n",
    "            'numero_serie': [f\"SN{random.randint(100000, 999999)}\" for _ in range(NUM_CONTADORES_ADV)],\n",
    "            'fecha_fabricacion': pd.to_datetime([\n",
    "                datetime.now() - timedelta(days=random.randint(30, 3650)) \n",
    "                for _ in range(NUM_CONTADORES_ADV)\n",
    "            ]),\n",
    "            'fecha_instalacion': pd.to_datetime([\n",
    "                datetime.now() - timedelta(days=random.randint(1, 1825)) \n",
    "                for _ in range(NUM_CONTADORES_ADV)\n",
    "            ]),\n",
    "            \n",
    "            # Caracter√≠sticas el√©ctricas\n",
    "            'potencia_maxima': np.random.lognormal(8.5, 0.8, NUM_CONTADORES_ADV),\n",
    "            'tension_nominal': np.random.choice([230, 400], NUM_CONTADORES_ADV, p=[0.6, 0.4]),\n",
    "            'fases': np.random.choice([1, 3], NUM_CONTADORES_ADV, p=[0.7, 0.3]),\n",
    "            'clase_precision': np.random.choice([1, 2, 0.5], NUM_CONTADORES_ADV, p=[0.6, 0.3, 0.1]),\n",
    "            'intensidad_maxima': np.random.lognormal(3.0, 0.5, NUM_CONTADORES_ADV),\n",
    "            \n",
    "            # Telegesti√≥n y comunicaciones\n",
    "            'telegest_activo': np.random.choice([True, False], NUM_CONTADORES_ADV, p=[0.85, 0.15]),\n",
    "            'protocolo_comunicacion': np.random.choice(['PLC', 'RF', 'GPRS', 'ETHERNET'], \n",
    "                                                     NUM_CONTADORES_ADV, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "            'version_firmware': [f\"v{random.randint(1,5)}.{random.randint(0,9)}.{random.randint(0,9)}\" \n",
    "                               for _ in range(NUM_CONTADORES_ADV)],\n",
    "            'estado_comunicacion': np.random.choice(['ACTIVO', 'INTERMITENTE', 'PERDIDO'], \n",
    "                                                  NUM_CONTADORES_ADV, p=[0.8, 0.15, 0.05]),\n",
    "            \n",
    "            # M√©tricas operacionales\n",
    "            'dias_desde_instalacion': [(datetime.now() - pd.to_datetime([\n",
    "                datetime.now() - timedelta(days=random.randint(1, 1825)) \n",
    "                for _ in range(NUM_CONTADORES_ADV)\n",
    "            ])[i]).days for i in range(NUM_CONTADORES_ADV)],\n",
    "            'num_reinicios': np.random.poisson(2, NUM_CONTADORES_ADV),\n",
    "            'eventos_alarma': np.random.poisson(1, NUM_CONTADORES_ADV),\n",
    "            'calidad_senal': np.random.beta(8, 2, NUM_CONTADORES_ADV) * 100,\n",
    "            \n",
    "            # Consumo y patrones\n",
    "            'consumo_promedio_diario': np.random.lognormal(3.5, 1.2, NUM_CONTADORES_ADV),\n",
    "            'consumo_maximo_registrado': np.random.lognormal(4.0, 1.0, NUM_CONTADORES_ADV),\n",
    "            'consumo_minimo_registrado': np.random.exponential(5, NUM_CONTADORES_ADV),\n",
    "            'variabilidad_consumo': np.random.exponential(20, NUM_CONTADORES_ADV),\n",
    "            'patron_horario': np.random.choice(['RESIDENCIAL', 'COMERCIAL', 'INDUSTRIAL'], \n",
    "                                             NUM_CONTADORES_ADV, p=[0.6, 0.3, 0.1]),\n",
    "            'dias_sin_consumo': np.random.poisson(3, NUM_CONTADORES_ADV),\n",
    "            'picos_consumo_anormales': np.random.poisson(0.5, NUM_CONTADORES_ADV),\n",
    "            \n",
    "            # Mantenimiento y incidencias\n",
    "            'ultima_inspeccion': pd.to_datetime([\n",
    "                datetime.now() - timedelta(days=random.randint(0, 730)) \n",
    "                for _ in range(NUM_CONTADORES_ADV)\n",
    "            ]),\n",
    "            'resultado_ultima_inspeccion': np.random.choice(['CORRECTO', 'ANOMALIA_MENOR', 'ANOMALIA_MAYOR'], \n",
    "                                                          NUM_CONTADORES_ADV, p=[0.85, 0.12, 0.03]),\n",
    "            'num_averias': np.random.poisson(0.3, NUM_CONTADORES_ADV),\n",
    "            'dias_fuera_servicio': np.random.poisson(1, NUM_CONTADORES_ADV),\n",
    "            \n",
    "            # Indicadores de riesgo\n",
    "            'score_fiabilidad': np.random.beta(7, 3, NUM_CONTADORES_ADV) * 100,\n",
    "            'anomalias_detectadas': np.random.poisson(0.8, NUM_CONTADORES_ADV),\n",
    "            'tendencia_consumo': np.random.choice(['ESTABLE', 'CRECIENTE', 'DECRECIENTE'], \n",
    "                                                NUM_CONTADORES_ADV, p=[0.7, 0.2, 0.1])\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _generate_suministro_nodes(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generates nodes of suministros/contratos\n",
    "        \"\"\"\n",
    "        \n",
    "        data = {\n",
    "            'node_id': [f\"SUM_{i:06d}\" for i in range(NUM_SUMINISTROS_ADV)],\n",
    "            \n",
    "            # Datos contractuales\n",
    "            'fecha_alta': pd.to_datetime([\n",
    "                datetime.now() - timedelta(days=random.randint(1, 2555)) \n",
    "                for _ in range(NUM_SUMINISTROS_ADV)\n",
    "            ]),\n",
    "            'fecha_baja': pd.to_datetime([\n",
    "                datetime.now() + timedelta(days=random.randint(-30, 365)) if random.random() < 0.05 else None\n",
    "                for _ in range(NUM_SUMINISTROS_ADV)\n",
    "            ]),\n",
    "            'estado_contrato': np.random.choice(['ACTIVO', 'SUSPENDIDO', 'BAJA'], \n",
    "                                              NUM_SUMINISTROS_ADV, p=[0.92, 0.05, 0.03]),\n",
    "            'tipo_suministro': np.random.choice(['RESIDENCIAL', 'COMERCIAL', 'INDUSTRIAL', 'AGRICOLA'], \n",
    "                                              NUM_SUMINISTROS_ADV, p=[0.6, 0.25, 0.1, 0.05]),\n",
    "            \n",
    "            # Potencia y tarifa\n",
    "            'potencia_contratada': np.random.lognormal(8.0, 1.0, NUM_SUMINISTROS_ADV),\n",
    "            'potencia_maxima_demandada': np.random.lognormal(8.2, 0.8, NUM_SUMINISTROS_ADV),\n",
    "            'tarifa_activa': np.random.choice(['2.0TD', '3.0TD', '6.1TD', '6.2TD'], \n",
    "                                            NUM_SUMINISTROS_ADV, p=[0.7, 0.2, 0.05, 0.05]),\n",
    "            'discriminacion_horaria': np.random.choice([True, False], NUM_SUMINISTROS_ADV, p=[0.4, 0.6]),\n",
    "            \n",
    "            # CNAE y actividad\n",
    "            'cnae': np.random.choice(['4711', '5210', '9820', '1071', '4540'], \n",
    "                                   NUM_SUMINISTROS_ADV, p=[0.3, 0.25, 0.2, 0.15, 0.1]),\n",
    "            'actividad_declarada': np.random.choice(['VIVIENDA', 'COMERCIO', 'OFICINA', 'INDUSTRIA', 'OTROS'], \n",
    "                                                  NUM_SUMINISTROS_ADV, p=[0.6, 0.2, 0.1, 0.05, 0.05]),\n",
    "            \n",
    "            # Facturaci√≥n y pagos\n",
    "            'facturacion_mensual_promedio': np.random.lognormal(4.5, 1.0, NUM_SUMINISTROS_ADV),\n",
    "            'dias_morosidad': np.random.exponential(5, NUM_SUMINISTROS_ADV),\n",
    "            'num_impagos': np.random.poisson(0.2, NUM_SUMINISTROS_ADV),\n",
    "            'metodo_pago': np.random.choice(['DOMICILIACION', 'TRANSFERENCIA', 'EFECTIVO'], \n",
    "                                          NUM_SUMINISTROS_ADV, p=[0.8, 0.15, 0.05]),\n",
    "            \n",
    "            # Consumo contractual\n",
    "            'consumo_anual_estimado': np.random.lognormal(7.0, 1.2, NUM_SUMINISTROS_ADV),\n",
    "            'consumo_anual_real': np.random.lognormal(7.1, 1.1, NUM_SUMINISTROS_ADV),\n",
    "            'desviacion_consumo_estimado': np.random.normal(0, 25, NUM_SUMINISTROS_ADV),\n",
    "            \n",
    "            # Modificaciones contractuales\n",
    "            'num_cambios_potencia': np.random.poisson(0.3, NUM_SUMINISTROS_ADV),\n",
    "            'num_cambios_tarifa': np.random.poisson(0.2, NUM_SUMINISTROS_ADV),\n",
    "            'ultima_modificacion': pd.to_datetime([\n",
    "                datetime.now() - timedelta(days=random.randint(30, 730)) if random.random() < 0.3 else None\n",
    "                for _ in range(NUM_SUMINISTROS_ADV)\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _generate_comercializadora_nodes(self) -> pd.DataFrame:\n",
    "        \"\"\"Genera nodos de comercializadoras\"\"\"\n",
    "        \n",
    "        nombres = ['ENDESA', 'IBERDROLA', 'NATURGY', 'VIESGO', 'EDP', 'HOLALUZ', 'FACTOR', 'LUCERA', \n",
    "                  'PEPEENERGY', 'SOMA', 'BASER', 'ALDRO', 'REPSOL', 'TOTALENERGIES', 'GALP', \n",
    "                  'GESTERNOVA', 'NEXUS', 'ACCIONA', 'CIDE', 'COMPETENCIA']\n",
    "        \n",
    "        data = {\n",
    "            'node_id': [f\"COM_{i:03d}\" for i in range(NUM_COMERCIALIZADORAS_ADV)],\n",
    "            'nombre': nombres[:NUM_COMERCIALIZADORAS_ADV],\n",
    "            'tipo_empresa': np.random.choice(['GRAN_EMPRESA', 'MEDIANA_EMPRESA', 'STARTUP'], \n",
    "                                           NUM_COMERCIALIZADORAS_ADV, p=[0.6, 0.3, 0.1]),\n",
    "            'NUM_CLIENTES_ADV_total': np.random.lognormal(10, 2, NUM_COMERCIALIZADORAS_ADV),\n",
    "            'cuota_mercado': np.random.exponential(5, NUM_COMERCIALIZADORAS_ADV),\n",
    "            'anos_operando': np.random.randint(1, 25, NUM_COMERCIALIZADORAS_ADV),\n",
    "            'rating_financiero': np.random.choice(['AAA', 'AA', 'A', 'BBB', 'BB'], \n",
    "                                                NUM_COMERCIALIZADORAS_ADV, p=[0.1, 0.2, 0.4, 0.2, 0.1]),\n",
    "            'num_reclamaciones': np.random.poisson(50, NUM_COMERCIALIZADORAS_ADV),\n",
    "            'tiempo_respuesta_promedio': np.random.exponential(3, NUM_COMERCIALIZADORAS_ADV),\n",
    "            'score_satisfaccion': np.random.beta(6, 2, NUM_COMERCIALIZADORAS_ADV) * 10,\n",
    "            'precios_competitivos': np.random.beta(5, 3, NUM_COMERCIALIZADORAS_ADV),\n",
    "            'productos_verdes': np.random.choice([True, False], NUM_COMERCIALIZADORAS_ADV, p=[0.7, 0.3]),\n",
    "            'servicios_digitales': np.random.choice([True, False], NUM_COMERCIALIZADORAS_ADV, p=[0.8, 0.2])\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _generate_ubicacion_nodes(self) -> pd.DataFrame:\n",
    "        \"\"\"Genera nodos de ubicaciones geogr√°ficas\"\"\"\n",
    "        \n",
    "        # Simular coordenadas en Espa√±a\n",
    "        lat_base, lon_base = 40.4168, -3.7038  # Madrid centro\n",
    "        \n",
    "        data = {\n",
    "            'node_id': [f\"UBI_{i:06d}\" for i in range(NUM_UBICACIONES_ADV)],\n",
    "            'latitud': np.random.normal(lat_base, 2.5, NUM_UBICACIONES_ADV),\n",
    "            'longitud': np.random.normal(lon_base, 3.0, NUM_UBICACIONES_ADV),\n",
    "            'coordenada_utm_x': np.random.uniform(300000, 800000, NUM_UBICACIONES_ADV),\n",
    "            'coordenada_utm_y': np.random.uniform(4200000, 4800000, NUM_UBICACIONES_ADV),\n",
    "            'codigo_postal': [f\"{random.randint(1, 52):02d}{random.randint(1, 999):03d}\" \n",
    "                            for _ in range(NUM_UBICACIONES_ADV)],\n",
    "            'provincia': np.random.choice(['MADRID', 'BARCELONA', 'VALENCIA', 'SEVILLA', 'BILBAO', 'ZARAGOZA'], \n",
    "                                       NUM_UBICACIONES_ADV, p=[0.3, 0.25, 0.15, 0.1, 0.1, 0.1]),\n",
    "            'tipo_zona': np.random.choice(['URBANA', 'RURAL', 'SEMIURBANA'], \n",
    "                                        NUM_UBICACIONES_ADV, p=[0.6, 0.2, 0.2]),\n",
    "            'densidad_poblacion': np.random.lognormal(6, 1.5, NUM_UBICACIONES_ADV),\n",
    "            'nivel_socioeconomico': np.random.choice(['ALTO', 'MEDIO_ALTO', 'MEDIO', 'MEDIO_BAJO', 'BAJO'], \n",
    "                                                   NUM_UBICACIONES_ADV, p=[0.1, 0.2, 0.4, 0.2, 0.1]),\n",
    "            'indice_criminalidad': np.random.exponential(3, NUM_UBICACIONES_ADV),\n",
    "            'accesibilidad_tecnica': np.random.choice(['FACIL', 'NORMAL', 'DIFICIL'], \n",
    "                                                    NUM_UBICACIONES_ADV, p=[0.6, 0.3, 0.1]),\n",
    "            'cobertura_comunicaciones': np.random.beta(8, 2, NUM_UBICACIONES_ADV) * 100,\n",
    "            'historial_fraudes': np.random.poisson(0.5, NUM_UBICACIONES_ADV),\n",
    "            'NUM_CONTADORES_ADV_zona': np.random.poisson(10, NUM_UBICACIONES_ADV),\n",
    "            'distancia_centro_transformacion': np.random.exponential(500, NUM_UBICACIONES_ADV),\n",
    "            'calidad_suministro': np.random.beta(9, 1, NUM_UBICACIONES_ADV) * 100\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _generate_concentrador_nodes(self) -> pd.DataFrame:\n",
    "        \"\"\"Genera nodos de concentradores de comunicaciones\"\"\"\n",
    "        \n",
    "        data = {\n",
    "            'node_id': [f\"CON_{i:04d}\" for i in range(NUM_CONCENTRADORES_ADV)],\n",
    "            'fabricante': np.random.choice(['ZIV', 'SAGITTARIUS', 'PRIME', 'LANDIS', 'ITRON'], \n",
    "                                         NUM_CONCENTRADORES_ADV),\n",
    "            'modelo_concentrador': np.random.choice(['DCU_500', 'DCU_1000', 'PRIME_Master', 'G3_Head'], \n",
    "                                                  NUM_CONCENTRADORES_ADV),\n",
    "            'version_software': [f\"v{random.randint(2,6)}.{random.randint(0,9)}\" \n",
    "                               for _ in range(NUM_CONCENTRADORES_ADV)],\n",
    "            'capacidad_maxima': np.random.choice([500, 1000, 2000, 5000], NUM_CONCENTRADORES_ADV),\n",
    "            'contadores_conectados': np.random.randint(10, 500, NUM_CONCENTRADORES_ADV),\n",
    "            'tasa_lectura_exitosa': np.random.beta(9, 1, NUM_CONCENTRADORES_ADV) * 100,\n",
    "            'latencia_promedio': np.random.exponential(2, NUM_CONCENTRADORES_ADV),\n",
    "            'uptime_porcentaje': np.random.beta(95, 5, NUM_CONCENTRADORES_ADV) * 100,\n",
    "            'num_reinicios_mes': np.random.poisson(1, NUM_CONCENTRADORES_ADV),\n",
    "            'errores_comunicacion': np.random.poisson(5, NUM_CONCENTRADORES_ADV),\n",
    "            'temperatura_operacion': np.random.normal(35, 10, NUM_CONCENTRADORES_ADV),\n",
    "            'nivel_bateria': np.random.beta(8, 2, NUM_CONCENTRADORES_ADV) * 100,\n",
    "            'calidad_senal_red': np.random.beta(7, 3, NUM_CONCENTRADORES_ADV) * 100,\n",
    "            'protocolo_principal': np.random.choice(['PLC_PRIME', 'PLC_G3', 'RF_169MHz', 'GPRS'], \n",
    "                                                  NUM_CONCENTRADORES_ADV, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "            'backup_comunicacion': np.random.choice([True, False], NUM_CONCENTRADORES_ADV, p=[0.7, 0.3]),\n",
    "            'ultima_actualizacion': pd.to_datetime([\n",
    "                datetime.now() - timedelta(days=random.randint(1, 365)) \n",
    "                for _ in range(NUM_CONCENTRADORES_ADV)\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _generate_cliente_nodes(self) -> pd.DataFrame:\n",
    "        \"\"\"Genera nodos de clientes\"\"\"\n",
    "        \n",
    "        data = {\n",
    "            'node_id': [f\"CLI_{i:06d}\" for i in range(NUM_CLIENTES_ADV)],\n",
    "            'tipo_cliente': np.random.choice(['PERSONA_FISICA', 'PERSONA_JURIDICA', 'ADMINISTRACION'], \n",
    "                                           NUM_CLIENTES_ADV, p=[0.8, 0.15, 0.05]),\n",
    "            'antiguedad_cliente': np.random.exponential(5, NUM_CLIENTES_ADV),\n",
    "            'NUM_SUMINISTROS_ADV': np.random.choice([1, 2, 3, 4, 5], NUM_CLIENTES_ADV, p=[0.7, 0.2, 0.05, 0.03, 0.02]),\n",
    "            'score_crediticio': np.random.beta(6, 4, NUM_CLIENTES_ADV) * 1000,\n",
    "            'ingresos_declarados': np.random.lognormal(10, 0.8, NUM_CLIENTES_ADV),\n",
    "            'edad_estimada': np.random.normal(45, 15, NUM_CLIENTES_ADV),\n",
    "            'canal_preferido': np.random.choice(['ONLINE', 'TELEFONO', 'PRESENCIAL', 'EMAIL'], \n",
    "                                              NUM_CLIENTES_ADV, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "            'num_reclamaciones': np.random.poisson(0.5, NUM_CLIENTES_ADV),\n",
    "            'num_cambios_comercializadora': np.random.poisson(1, NUM_CLIENTES_ADV),\n",
    "            'satisfaccion_cliente': np.random.beta(7, 3, NUM_CLIENTES_ADV) * 10,\n",
    "            'uso_servicios_digitales': np.random.choice([True, False], NUM_CLIENTES_ADV, p=[0.6, 0.4]),\n",
    "            'conciencia_energetica': np.random.beta(5, 5, NUM_CLIENTES_ADV) * 10,\n",
    "            'historial_impagos': np.random.poisson(0.3, NUM_CLIENTES_ADV),\n",
    "            'metodo_contacto_preferido': np.random.choice(['EMAIL', 'SMS', 'CARTA', 'TELEFONO'], \n",
    "                                                        NUM_CLIENTES_ADV, p=[0.5, 0.3, 0.1, 0.1]),\n",
    "            'programa_fidelizacion': np.random.choice([True, False], NUM_CLIENTES_ADV, p=[0.3, 0.7])\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _generate_transformador_nodes(self) -> pd.DataFrame:\n",
    "        \"\"\"Genera nodos de transformadores de distribuci√≥n\"\"\"\n",
    "        \n",
    "        data = {\n",
    "            'node_id': [f\"TRF_{i:04d}\" for i in range(NUM_TRANSFORMADORES_ADV)],\n",
    "            'potencia_nominal': np.random.choice([25, 50, 100, 160, 250, 400, 630], \n",
    "                                               NUM_TRANSFORMADORES_ADV, p=[0.1, 0.2, 0.25, 0.2, 0.15, 0.08, 0.02]),\n",
    "            'tension_primaria': np.random.choice([15000, 20000, 25000], NUM_TRANSFORMADORES_ADV, p=[0.4, 0.4, 0.2]),\n",
    "            'tension_secundaria': np.random.choice([400, 230], NUM_TRANSFORMADORES_ADV, p=[0.7, 0.3]),\n",
    "            'a√±o_fabricacion': np.random.randint(1980, 2024, NUM_TRANSFORMADORES_ADV),\n",
    "            'fabricante_transformador': np.random.choice(['SCHNEIDER', 'ABB', 'SIEMENS', 'ORMAZABAL', 'COOPER'], \n",
    "                                                       NUM_TRANSFORMADORES_ADV),\n",
    "            'tipo_refrigeracion': np.random.choice(['ACEITE', 'SECO', 'SILICONA'], \n",
    "                                                 NUM_TRANSFORMADORES_ADV, p=[0.6, 0.3, 0.1]),\n",
    "            'carga_actual_porcentaje': np.random.beta(6, 4, NUM_TRANSFORMADORES_ADV) * 100,\n",
    "            'carga_maxima_historica': np.random.beta(8, 2, NUM_TRANSFORMADORES_ADV) * 100,\n",
    "            'temperatura_operacion': np.random.normal(45, 15, NUM_TRANSFORMADORES_ADV),\n",
    "            'NUM_CONTADORES_ADV_alimentados': np.random.poisson(30, NUM_TRANSFORMADORES_ADV),\n",
    "            'perdidas_tecnicas': np.random.beta(2, 8, NUM_TRANSFORMADORES_ADV) * 5,\n",
    "            'ultima_revision': pd.to_datetime([\n",
    "                datetime.now() - timedelta(days=random.randint(30, 1095)) \n",
    "                for _ in range(NUM_TRANSFORMADORES_ADV)\n",
    "            ]),\n",
    "            'estado_conservacion': np.random.choice(['EXCELENTE', 'BUENO', 'REGULAR', 'MALO'], \n",
    "                                                  NUM_TRANSFORMADORES_ADV, p=[0.2, 0.5, 0.25, 0.05]),\n",
    "            'num_averias': np.random.poisson(0.5, NUM_TRANSFORMADORES_ADV),\n",
    "            'tiempo_fuera_servicio': np.random.exponential(2, NUM_TRANSFORMADORES_ADV),\n",
    "            'monitorizacion_remota': np.random.choice([True, False], NUM_TRANSFORMADORES_ADV, p=[0.4, 0.6]),\n",
    "            'sistema_proteccion': np.random.choice(['FUSIBLES', 'INTERRUPTOR', 'AUTOMATICO'], \n",
    "                                                 NUM_TRANSFORMADORES_ADV, p=[0.3, 0.4, 0.3])\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _generate_zona_nodes(self) -> pd.DataFrame:\n",
    "        \"\"\"Genera nodos de zonas de distribuci√≥n\"\"\"\n",
    "        \n",
    "        data = {\n",
    "            'node_id': [f\"ZON_{i:03d}\" for i in range(NUM_ZONAS_ADV)],\n",
    "            'nombre_zona': [f\"ZONA_{chr(65 + i//26)}{i%26 + 1}\" for i in range(NUM_ZONAS_ADV)],\n",
    "            'tipo_zona_electrica': np.random.choice(['URBANA_DENSA', 'URBANA', 'RURAL', 'INDUSTRIAL'], \n",
    "                                                  NUM_ZONAS_ADV, p=[0.2, 0.4, 0.3, 0.1]),\n",
    "            'NUM_TRANSFORMADORES_ADV': np.random.poisson(8, NUM_ZONAS_ADV),\n",
    "            'NUM_CONTADORES_ADV_zona': np.random.poisson(200, NUM_ZONAS_ADV),\n",
    "            'longitud_red_mt': np.random.exponential(10, NUM_ZONAS_ADV),  # km\n",
    "            'longitud_red_bt': np.random.exponential(25, NUM_ZONAS_ADV),  # km\n",
    "            'indice_calidad_suministro': np.random.beta(8, 2, NUM_ZONAS_ADV) * 10,\n",
    "            'tiempo_medio_interrupcion': np.random.exponential(2, NUM_ZONAS_ADV),  # horas/a√±o\n",
    "            'perdidas_tecnicas_zona': np.random.beta(3, 7, NUM_ZONAS_ADV) * 10,  # %\n",
    "            'perdidas_no_tecnicas': np.random.exponential(2, NUM_ZONAS_ADV),  # %\n",
    "            'densidad_carga': np.random.lognormal(3, 1, NUM_ZONAS_ADV),  # MW/km¬≤\n",
    "            'crecimiento_demanda': np.random.normal(2, 1, NUM_ZONAS_ADV),  # %/a√±o\n",
    "            'num_incidencias_mes': np.random.poisson(3, NUM_ZONAS_ADV),\n",
    "            'cobertura_telemedida': np.random.beta(8, 2, NUM_ZONAS_ADV) * 100,  # %\n",
    "            'antiguedad_red_promedio': np.random.normal(25, 10, NUM_ZONAS_ADV),  # a√±os\n",
    "            'inversion_mantenimiento': np.random.lognormal(8, 1, NUM_ZONAS_ADV),  # ‚Ç¨/a√±o\n",
    "            'personal_tecnico_asignado': np.random.poisson(2, NUM_ZONAS_ADV),\n",
    "            'vehiculos_disponibles': np.random.poisson(1, NUM_ZONAS_ADV),\n",
    "            'almacen_material': np.random.choice([True, False], NUM_ZONAS_ADV, p=[0.6, 0.4])\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _generate_heterogeneous_edges(self, node_data: Dict) -> Dict:\n",
    "        \"\"\"Genera todas las relaciones heterog√©neas entre tipos de nodos\"\"\"\n",
    "        \n",
    "        print(\"üîó Generando relaciones heterog√©neas...\")\n",
    "        \n",
    "        edge_data = {}\n",
    "        \n",
    "        # 1. CONTADOR -> SUMINISTRO (1:1)\n",
    "        edge_data[('contador', 'mide', 'suministro')] = self._create_contador_suministro_edges(\n",
    "            node_data['contador'], node_data['suministro']\n",
    "        )\n",
    "        \n",
    "        # 2. SUMINISTRO -> COMERCIALIZADORA (N:1)\n",
    "        edge_data[('suministro', 'contratado_con', 'comercializadora')] = self._create_suministro_comercializadora_edges(\n",
    "            node_data['suministro'], node_data['comercializadora']\n",
    "        )\n",
    "        \n",
    "        # 3. CONTADOR -> UBICACION (N:1)\n",
    "        edge_data[('contador', 'ubicado_en', 'ubicacion')] = self._create_contador_ubicacion_edges(\n",
    "            node_data['contador'], node_data['ubicacion']\n",
    "        )\n",
    "        \n",
    "        # 4. CONTADOR -> CONCENTRADOR (N:1)\n",
    "        edge_data[('contador', 'comunica_via', 'concentrador')] = self._create_contador_concentrador_edges(\n",
    "            node_data['contador'], node_data['concentrador']\n",
    "        )\n",
    "        \n",
    "        # 5. SUMINISTRO -> CLIENTE (N:1)\n",
    "        edge_data[('suministro', 'pertenece_a', 'cliente')] = self._create_suministro_cliente_edges(\n",
    "            node_data['suministro'], node_data['cliente']\n",
    "        )\n",
    "        \n",
    "        # 6. CONTADOR -> TRANSFORMADOR (N:1)\n",
    "        edge_data[('contador', 'alimentado_por', 'transformador')] = self._create_contador_transformador_edges(\n",
    "            node_data['contador'], node_data['transformador']\n",
    "        )\n",
    "        \n",
    "        # 7. TRANSFORMADOR -> ZONA (N:1)\n",
    "        edge_data[('transformador', 'pertenece_zona', 'zona')] = self._create_transformador_zona_edges(\n",
    "            node_data['transformador'], node_data['zona']\n",
    "        )\n",
    "        \n",
    "        # 8. UBICACION -> ZONA (N:1)\n",
    "        edge_data[('ubicacion', 'dentro_de', 'zona')] = self._create_ubicacion_zona_edges(\n",
    "            node_data['ubicacion'], node_data['zona']\n",
    "        )\n",
    "        \n",
    "        # 9. Relaciones intra-tipo\n",
    "        \n",
    "        # CONTADOR -> CONTADOR (proximidad geogr√°fica)\n",
    "        edge_data[('contador', 'cerca_de', 'contador')] = self._create_contador_contador_edges(\n",
    "            node_data['contador'], node_data['ubicacion']\n",
    "        )\n",
    "        \n",
    "        # CONTADOR -> CONTADOR (mismo modelo/marca)\n",
    "        edge_data[('contador', 'similar_a', 'contador')] = self._create_contador_similar_edges(\n",
    "            node_data['contador']\n",
    "        )\n",
    "        \n",
    "        # TRANSFORMADOR -> TRANSFORMADOR (misma zona)\n",
    "        edge_data[('transformador', 'conectado_a', 'transformador')] = self._create_transformador_transformador_edges(\n",
    "            node_data['transformador']\n",
    "        )\n",
    "        \n",
    "        # CLIENTE -> CLIENTE (m√∫ltiples suministros)\n",
    "        edge_data[('cliente', 'relacionado_con', 'cliente')] = self._create_cliente_cliente_edges(\n",
    "            node_data['cliente']\n",
    "        )\n",
    "        \n",
    "        return edge_data\n",
    "    \n",
    "    def _create_contador_suministro_edges(self, contadores: pd.DataFrame, suministros: pd.DataFrame) -> List:\n",
    "        \"\"\"Relaci√≥n 1:1 entre contadores y suministros\"\"\"\n",
    "        edges = []\n",
    "        for i in range(min(len(contadores), len(suministros))):\n",
    "            edges.append([i, i])  # Correspondencia 1:1\n",
    "        return edges\n",
    "    \n",
    "    def _create_suministro_comercializadora_edges(self, suministros: pd.DataFrame, comercializadoras: pd.DataFrame) -> List:\n",
    "        \"\"\"Relaci√≥n N:1 suministros -> comercializadoras\"\"\"\n",
    "        edges = []\n",
    "        for i in range(len(suministros)):\n",
    "            com_idx = np.random.randint(0, len(comercializadoras))\n",
    "            edges.append([i, com_idx])\n",
    "        return edges\n",
    "    \n",
    "    def _create_contador_ubicacion_edges(self, contadores: pd.DataFrame, ubicaciones: pd.DataFrame) -> List:\n",
    "        \"\"\"Relaci√≥n N:1 contadores -> ubicaciones\"\"\"\n",
    "        edges = []\n",
    "        for i in range(len(contadores)):\n",
    "            ubi_idx = np.random.randint(0, len(ubicaciones))\n",
    "            edges.append([i, ubi_idx])\n",
    "        return edges\n",
    "    \n",
    "    def _create_contador_concentrador_edges(self, contadores: pd.DataFrame, concentradores: pd.DataFrame) -> List:\n",
    "        \"\"\"Relaci√≥n N:1 contadores -> concentradores\"\"\"\n",
    "        edges = []\n",
    "        for i in range(len(contadores)):\n",
    "            # Simular distribuci√≥n no uniforme (algunos concentradores m√°s cargados)\n",
    "            prob_weights = np.random.exponential(1, len(concentradores))\n",
    "            prob_weights = prob_weights / prob_weights.sum()\n",
    "            conc_idx = np.random.choice(len(concentradores), p=prob_weights)\n",
    "            edges.append([i, conc_idx])\n",
    "        return edges\n",
    "    \n",
    "    def _create_suministro_cliente_edges(self, suministros: pd.DataFrame, clientes: pd.DataFrame) -> List:\n",
    "        \"\"\"Relaci√≥n N:1 suministros -> clientes (algunos clientes tienen m√∫ltiples suministros)\"\"\"\n",
    "        edges = []\n",
    "        cliente_idx = 0\n",
    "        for i in range(len(suministros)):\n",
    "            edges.append([i, cliente_idx])\n",
    "            # Probabilidad de que el siguiente suministro sea del mismo cliente\n",
    "            if np.random.random() > 0.85:  # 15% probabilidad de m√∫ltiples suministros\n",
    "                cliente_idx = min(cliente_idx + 1, len(clientes) - 1)\n",
    "        return edges\n",
    "    \n",
    "    def _create_contador_transformador_edges(self, contadores: pd.DataFrame, transformadores: pd.DataFrame) -> List:\n",
    "        \"\"\"Relaci√≥n N:1 contadores -> transformadores\"\"\"\n",
    "        edges = []\n",
    "        for i in range(len(contadores)):\n",
    "            # Distribuci√≥n realista: cada transformador alimenta ~25 contadores\n",
    "            trf_idx = i // 25 % len(transformadores)\n",
    "            edges.append([i, trf_idx])\n",
    "        return edges\n",
    "    \n",
    "    def _create_transformador_zona_edges(self, transformadores: pd.DataFrame, zonas: pd.DataFrame) -> List:\n",
    "        \"\"\"Relaci√≥n N:1 transformadores -> zonas\"\"\"\n",
    "        edges = []\n",
    "        for i in range(len(transformadores)):\n",
    "            zona_idx = i // 4 % len(zonas)  # ~4 transformadores por zona\n",
    "            edges.append([i, zona_idx])\n",
    "        return edges\n",
    "    \n",
    "    def _create_ubicacion_zona_edges(self, ubicaciones: pd.DataFrame, zonas: pd.DataFrame) -> List:\n",
    "        \"\"\"Relaci√≥n N:1 ubicaciones -> zonas\"\"\"\n",
    "        edges = []\n",
    "        for i in range(len(ubicaciones)):\n",
    "            zona_idx = i // 20 % len(zonas)  # ~20 ubicaciones por zona\n",
    "            edges.append([i, zona_idx])\n",
    "        return edges\n",
    "    \n",
    "    def _create_contador_contador_edges(self, contadores: pd.DataFrame, ubicaciones: pd.DataFrame) -> List:\n",
    "        \"\"\"Relaciones entre contadores por proximidad geogr√°fica\"\"\"\n",
    "        edges = []\n",
    "        \n",
    "        # Simular proximidad basada en ubicaciones\n",
    "        for i in range(len(contadores)):\n",
    "            # Cada contador se conecta con 2-5 vecinos cercanos\n",
    "            num_neighbors = np.random.randint(2, 6)\n",
    "            for _ in range(num_neighbors):\n",
    "                neighbor_idx = np.random.randint(0, len(contadores))\n",
    "                if neighbor_idx != i:\n",
    "                    edges.append([i, neighbor_idx])\n",
    "                    \n",
    "        return edges\n",
    "    \n",
    "    def _create_contador_similar_edges(self, contadores: pd.DataFrame) -> List:\n",
    "        \"\"\"Relaciones entre contadores de mismo modelo/marca\"\"\"\n",
    "        edges = []\n",
    "        \n",
    "        # Agrupar por marca y modelo\n",
    "        for marca in contadores['marca'].unique():\n",
    "            for modelo in contadores['modelo'].unique():\n",
    "                subset_indices = contadores[\n",
    "                    (contadores['marca'] == marca) & (contadores['modelo'] == modelo)\n",
    "                ].index.tolist()\n",
    "                \n",
    "                # Conectar contadores del mismo tipo\n",
    "                for i in range(len(subset_indices)):\n",
    "                    for j in range(i + 1, min(i + 5, len(subset_indices))):  # Limitar conexiones\n",
    "                        edges.append([subset_indices[i], subset_indices[j]])\n",
    "                        edges.append([subset_indices[j], subset_indices[i]])  # Bidireccional\n",
    "                        \n",
    "        return edges\n",
    "    \n",
    "    def _create_transformador_transformador_edges(self, transformadores: pd.DataFrame) -> List:\n",
    "        \"\"\"Relaciones entre transformadores en la misma red\"\"\"\n",
    "        edges = []\n",
    "        \n",
    "        # Conectar transformadores por grupos (simulando red el√©ctrica)\n",
    "        for i in range(0, len(transformadores), 10):  # Grupos de 10\n",
    "            group_end = min(i + 10, len(transformadores))\n",
    "            for j in range(i, group_end):\n",
    "                for k in range(j + 1, group_end):\n",
    "                    if np.random.random() < 0.3:  # 30% probabilidad de conexi√≥n\n",
    "                        edges.append([j, k])\n",
    "                        edges.append([k, j])  # Bidireccional\n",
    "                        \n",
    "        return edges\n",
    "    \n",
    "    def _create_cliente_cliente_edges(self, clientes: pd.DataFrame) -> List:\n",
    "        \"\"\"Relaciones entre clientes relacionados\"\"\"\n",
    "        edges = []\n",
    "        \n",
    "        # Simular relaciones familiares o empresariales\n",
    "        for i in range(len(clientes)):\n",
    "            if np.random.random() < 0.1:  # 10% probabilidad de tener relaci√≥n\n",
    "                related_idx = np.random.randint(0, len(clientes))\n",
    "                if related_idx != i:\n",
    "                    edges.append([i, related_idx])\n",
    "                    \n",
    "        return edges\n",
    "    \n",
    "    def _assign_fraud_labels(self, contadores: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Asigna etiquetas de fraude a contadores con patrones realistas\"\"\"\n",
    "        \n",
    "        NUM_CONTADORES_ADV = len(contadores)\n",
    "        labels = ['NORMAL'] * NUM_CONTADORES_ADV\n",
    "        \n",
    "        # Calcular n√∫mero de fraudes\n",
    "        num_fraudes = int(NUM_CONTADORES_ADV * FRAUD_PROBABILITY_ADV)\n",
    "        num_irregularidades = int(NUM_CONTADORES_ADV * IRREGULARITY_PROBABILITY_ADV)\n",
    "        \n",
    "        # Seleccionar √≠ndices para fraudes (con bias hacia ciertos patrones)\n",
    "        fraud_indices = []\n",
    "        irregularity_indices = []\n",
    "        \n",
    "        # Bias hacia contadores con ciertas caracter√≠sticas\n",
    "        fraud_candidates = contadores[\n",
    "            (contadores['consumo_promedio_diario'] < contadores['consumo_promedio_diario'].quantile(0.2)) |\n",
    "            (contadores['estado_comunicacion'] != 'ACTIVO') |\n",
    "            (contadores['anomalias_detectadas'] > 2) |\n",
    "            (contadores['resultado_ultima_inspeccion'] == 'ANOMALIA_MAYOR')\n",
    "        ].index.tolist()\n",
    "        \n",
    "        # Seleccionar fraudes de candidatos sospechosos (70%) y aleatorios (30%)\n",
    "        fraud_from_candidates = min(int(num_fraudes * 0.7), len(fraud_candidates))\n",
    "        fraud_indices.extend(np.random.choice(fraud_candidates, fraud_from_candidates, replace=False))\n",
    "        \n",
    "        remaining_fraud = num_fraudes - fraud_from_candidates\n",
    "        if remaining_fraud > 0:\n",
    "            available_indices = [i for i in range(NUM_CONTADORES_ADV) if i not in fraud_indices]\n",
    "            fraud_indices.extend(np.random.choice(available_indices, remaining_fraud, replace=False))\n",
    "        \n",
    "        # Seleccionar irregularidades (m√°s aleatorias)\n",
    "        available_for_irregularity = [i for i in range(NUM_CONTADORES_ADV) if i not in fraud_indices]\n",
    "        irregularity_indices = np.random.choice(available_for_irregularity, num_irregularidades, replace=False)\n",
    "        \n",
    "        # Asignar etiquetas\n",
    "        for idx in fraud_indices:\n",
    "            labels[idx] = 'FRAUDE'\n",
    "            \n",
    "        for idx in irregularity_indices:\n",
    "            labels[idx] = 'IRREGULARIDAD'\n",
    "        \n",
    "        # Modificar caracter√≠sticas de contadores fraudulentos para crear patrones\n",
    "        contadores = contadores.copy()\n",
    "        \n",
    "        for idx in fraud_indices:\n",
    "            # Fraudes tienden a tener consumo bajo y patrones an√≥malos\n",
    "            contadores.loc[idx, 'consumo_promedio_diario'] *= np.random.uniform(0.1, 0.4)\n",
    "            contadores.loc[idx, 'variabilidad_consumo'] *= np.random.uniform(0.2, 0.6)\n",
    "            contadores.loc[idx, 'anomalias_detectadas'] += np.random.randint(1, 5)\n",
    "            contadores.loc[idx, 'eventos_alarma'] += np.random.randint(2, 8)\n",
    "            \n",
    "        for idx in irregularity_indices:\n",
    "            # Irregularidades tienen patrones menos extremos\n",
    "            contadores.loc[idx, 'consumo_promedio_diario'] *= np.random.uniform(0.5, 0.8)\n",
    "            contadores.loc[idx, 'score_fiabilidad'] *= np.random.uniform(0.6, 0.9)\n",
    "            contadores.loc[idx, 'anomalias_detectadas'] += np.random.randint(1, 3)\n",
    "        \n",
    "        contadores['label'] = labels\n",
    "        \n",
    "        print(f\"‚úÖ Etiquetas asignadas: {labels.count('NORMAL')} Normal, {labels.count('FRAUDE')} Fraude, {labels.count('IRREGULARIDAD')} Irregularidad\")\n",
    "        \n",
    "        return contadores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b648b730-b13a-4565-a0df-46f6e64bacb4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.3. Data Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659d820-4e3e-4542-9f90-b6105e345ccb",
   "metadata": {},
   "source": [
    "We create a class in order to process the data. Take into account that our goal here is to have the data in PyTorch Geometric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b9ec41-7216-4060-b11f-6605314fc475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousGraphProcessor:\n",
    "    \"\"\"\n",
    "    Processor in order to convert the previous data into a PyTorch Geometric format\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.label_encoders = {}\n",
    "        \n",
    "    def create_heterogeneous_graph(self, data: Dict) -> HeteroData:\n",
    "        \"\"\"\n",
    "        Converts hetero data into a HeteroData object in PyTorch Geometric\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"üîÑ Procesando grafo heterog√©neo...\")\n",
    "        \n",
    "        hetero_data = HeteroData()\n",
    "        \n",
    "        # Process each type of node\n",
    "        for node_type, node_df in data['nodes'].items():\n",
    "            print(f\"   Procesando nodos tipo: {node_type}\")\n",
    "            \n",
    "            # Procesar caracter√≠sticas del nodo\n",
    "            x, feature_names = self._process_node_features(node_df, node_type)\n",
    "            hetero_data[node_type].x = torch.tensor(x, dtype=torch.float)\n",
    "            \n",
    "            # Guardar nombres de caracter√≠sticas para an√°lisis posterior\n",
    "            hetero_data[node_type].feature_names = feature_names\n",
    "            \n",
    "            # Para contadores, agregar etiquetas de fraude\n",
    "            if node_type == 'contador' and 'label' in node_df.columns:\n",
    "                labels = [FRAUD_CLASSES[label] for label in node_df['label']]\n",
    "                hetero_data[node_type].y = torch.tensor(labels, dtype=torch.long)\n",
    "            \n",
    "            print(f\"     ‚úÖ {len(node_df)} nodos, {x.shape[1]} caracter√≠sticas\")\n",
    "        \n",
    "        # Process heterogeneous relations:\n",
    "        for edge_type, edge_list in data['edges'].items():\n",
    "            if len(edge_list) > 0:\n",
    "                src_type, relation, dst_type = edge_type\n",
    "                edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "                hetero_data[src_type, relation, dst_type].edge_index = edge_index\n",
    "                print(f\"   ‚úÖ {relation}: {len(edge_list)} relaciones ({src_type} -> {dst_type})\")\n",
    "        \n",
    "        # Transformar a formato homog√©neo para algunos algoritmos\n",
    "        # hetero_data = T.ToHomogeneous()(hetero_data)\n",
    "        \n",
    "        print(f\"‚úÖ Grafo heterog√©neo creado con {len(data['nodes'])} tipos de nodos\")\n",
    "        \n",
    "        return hetero_data\n",
    "    \n",
    "    def _process_node_features(self, node_df: pd.DataFrame, node_type: str) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"Procesa caracter√≠sticas de un tipo espec√≠fico de nodo\"\"\"\n",
    "        \n",
    "        # Excluir columnas no-feature\n",
    "        exclude_cols = ['node_id', 'label']\n",
    "        feature_cols = [col for col in node_df.columns if col not in exclude_cols]\n",
    "        \n",
    "        processed_features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            if node_df[col].dtype in ['object', 'bool']:\n",
    "                # Variables categ√≥ricas\n",
    "                if node_df[col].dtype == 'bool':\n",
    "                    # Variables booleanas\n",
    "                    processed_features.append(node_df[col].astype(int).values.reshape(-1, 1))\n",
    "                    feature_names.append(f\"{col}\")\n",
    "                else:\n",
    "                    # Variables categ√≥ricas -> One-hot encoding\n",
    "                    encoded = pd.get_dummies(node_df[col], prefix=col)\n",
    "                    processed_features.append(encoded.values)\n",
    "                    feature_names.extend(encoded.columns.tolist())\n",
    "                    \n",
    "            elif node_df[col].dtype in ['datetime64[ns]']:\n",
    "                # Variables temporales -> caracter√≠sticas num√©ricas\n",
    "                days_since = (datetime.now() - node_df[col]).dt.days.fillna(0).values.reshape(-1, 1)\n",
    "                processed_features.append(days_since)\n",
    "                feature_names.append(f\"{col}_days_since\")\n",
    "                \n",
    "            else:\n",
    "                # Variables num√©ricas -> normalizaci√≥n\n",
    "                scaler_key = f\"{node_type}_{col}\"\n",
    "                if scaler_key not in self.scalers:\n",
    "                    self.scalers[scaler_key] = StandardScaler()\n",
    "                \n",
    "                values = node_df[col].fillna(node_df[col].median()).values.reshape(-1, 1)\n",
    "                normalized = self.scalers[scaler_key].fit_transform(values)\n",
    "                processed_features.append(normalized)\n",
    "                feature_names.append(f\"{col}_normalized\")\n",
    "        \n",
    "        # Concatenar todas las caracter√≠sticas\n",
    "        if processed_features:\n",
    "            final_features = np.concatenate(processed_features, axis=1)\n",
    "        else:\n",
    "            final_features = np.zeros((len(node_df), 1))\n",
    "            feature_names = ['dummy_feature']\n",
    "        \n",
    "        return final_features, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79e9265-f9f8-4011-b90a-a88cbb915b16",
   "metadata": {},
   "source": [
    "Now, the architecture, trainer and evaluator are going to be imported from the utils folder where we have those .pys stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f003b3-5366-4205-b4cb-4dda184aae71",
   "metadata": {},
   "source": [
    "## 5.4. Advanced GNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6ac8252-6e19-467c-ae65-2e7cfe8baf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.arquitecturas_GNN import HeterogeneousFraudGNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c3e81-01eb-4619-a936-724c6ac4950c",
   "metadata": {},
   "source": [
    "## 5.5. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50a0c23b-1938-4da3-9187-060a91ae2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.trainer import HeterogeneousFraudTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeab319-61ec-45b2-8458-a61d6db7c49d",
   "metadata": {},
   "source": [
    "## 5.6. Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80f3fa23-acb9-4a75-ac12-e122d95b365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluator import HeterogeneousEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450e394-5b0c-4a6d-ab1d-0517b38bd1e6",
   "metadata": {},
   "source": [
    "## 5.7. Pipeline creation (real data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fc40c78-871c-4e65-8b44-2b98e571a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousGraphProcessor:\n",
    "    \"\"\"\n",
    "    Procesador para convertir datos de Neo4j en formato PyTorch Geometric\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.label_encoders = {}\n",
    "        \n",
    "    def create_heterogeneous_graph(self, data: Dict) -> HeteroData:\n",
    "        \"\"\"\n",
    "        Convierte datos heterog√©neos en un objeto HeteroData de PyTorch Geometric\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"üîÑ Procesando grafo heterog√©neo desde Neo4j...\")\n",
    "        \n",
    "        hetero_data = HeteroData()\n",
    "        \n",
    "        # Procesar cada tipo de nodo\n",
    "        for node_type, node_df in data['nodes'].items():\n",
    "            if len(node_df) == 0:\n",
    "                print(f\"   ‚ö†Ô∏è Saltando nodos tipo: {node_type} (vac√≠o)\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"   Procesando nodos tipo: {node_type}\")\n",
    "            \n",
    "            # Procesar caracter√≠sticas del nodo\n",
    "            x, feature_names = self._process_node_features(node_df, node_type)\n",
    "            hetero_data[node_type].x = torch.tensor(x, dtype=torch.float)\n",
    "            \n",
    "            # Save names of the features for the analysis we will do after\n",
    "            hetero_data[node_type].feature_names = feature_names\n",
    "            \n",
    "            # For CONTADORES, aggregate fraud labels\n",
    "            if node_type == 'contador' and 'label' in node_df.columns:\n",
    "                labels = [FRAUD_CLASSES[label] for label in node_df['label']]\n",
    "                hetero_data[node_type].y = torch.tensor(labels, dtype=torch.long)\n",
    "            \n",
    "            print(f\"     ‚úÖ {len(node_df)} nodes, {x.shape[1]} features\")\n",
    "        \n",
    "        # Process heterogeneous relations\n",
    "        for edge_type, edge_list in data['edges'].items():\n",
    "            if len(edge_list) > 0:\n",
    "                src_type, relation, dst_type = edge_type\n",
    "                \n",
    "                # Verify that both type of node exits\n",
    "                if src_type in hetero_data.node_types and dst_type in hetero_data.node_types:\n",
    "                    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "                    hetero_data[src_type, relation, dst_type].edge_index = edge_index\n",
    "                    print(f\"   ‚úÖ {relation}: {len(edge_list)} relaciones ({src_type} -> {dst_type})\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è Saltando relaci√≥n {relation}: tipos de nodo faltantes\")\n",
    "        \n",
    "        print(f\"‚úÖ Heterogeneous graph created with {len(data['nodes'])} type of nodes\")\n",
    "        \n",
    "        return hetero_data\n",
    "    \n",
    "    def _process_node_features(self, node_df: pd.DataFrame, node_type: str) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"\n",
    "        Process one type of node's features.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Excluir columnas no-feature\n",
    "        exclude_cols = ['node_id', 'label', 'nis_rad', 'nis_expediente', 'codigo_comercializadora', 'concentrador_id']\n",
    "        feature_cols = [col for col in node_df.columns if col not in exclude_cols]\n",
    "        \n",
    "        processed_features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            try:\n",
    "                if node_df[col].dtype in ['object', 'bool']:\n",
    "                    # Categorical variables\n",
    "                    if node_df[col].dtype == 'bool':\n",
    "                        # Boolean variables\n",
    "                        processed_features.append(node_df[col].astype(int).values.reshape(-1, 1))\n",
    "                        feature_names.append(f\"{col}\")\n",
    "                    else:\n",
    "                        # Categorical variables -> One-hot encoding\n",
    "                        # Limit the number of categories in order to avoid too big dimensions\n",
    "                        unique_values = node_df[col].nunique()\n",
    "                        if unique_values > 10:\n",
    "                            # If there are many categories, we use only the top10\n",
    "                            top_categories = node_df[col].value_counts().head(10).index\n",
    "                            node_df_temp = node_df[col].copy()\n",
    "                            node_df_temp[~node_df_temp.isin(top_categories)] = 'OTHER'\n",
    "                            encoded = pd.get_dummies(node_df_temp, prefix=col)\n",
    "                        else:\n",
    "                            encoded = pd.get_dummies(node_df[col], prefix=col)\n",
    "                        \n",
    "                        processed_features.append(encoded.values)\n",
    "                        feature_names.extend(encoded.columns.tolist())\n",
    "                        \n",
    "                elif node_df[col].dtype in ['datetime64[ns]', '<M8[ns]']:\n",
    "                    # Temporal variables -> Numeric features\n",
    "                    try:\n",
    "                        days_since = (datetime.now() - pd.to_datetime(node_df[col])).dt.days.fillna(0).values.reshape(-1, 1)\n",
    "                        processed_features.append(days_since)\n",
    "                        feature_names.append(f\"{col}_days_since\")\n",
    "                    except:\n",
    "                        # If the temporal conversion fails, we use a by default value\n",
    "                        default_values = np.zeros((len(node_df), 1))\n",
    "                        processed_features.append(default_values)\n",
    "                        feature_names.append(f\"{col}_days_since\")\n",
    "                        \n",
    "                else:\n",
    "                    # Numerical features -> standard scaller. This is done in order to every data contribute the same\n",
    "                    scaler_key = f\"{node_type}_{col}\"\n",
    "                    if scaler_key not in self.scalers:\n",
    "                        self.scalers[scaler_key] = StandardScaler()\n",
    "                    \n",
    "                    # Convertir a num√©rico y manejar NaNs\n",
    "                    values = pd.to_numeric(node_df[col], errors='coerce').fillna(0).values.reshape(-1, 1)\n",
    "                    \n",
    "                    # Verificar si hay varianza en los datos\n",
    "                    if np.std(values) > 0:\n",
    "                        normalized = self.scalers[scaler_key].fit_transform(values)\n",
    "                    else:\n",
    "                        normalized = values  # Si no hay varianza, mantener los valores originales\n",
    "                    \n",
    "                    processed_features.append(normalized)\n",
    "                    feature_names.append(f\"{col}_normalized\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Error procesando columna {col}: {e}\")\n",
    "                # Create features by default in case of error\n",
    "                default_values = np.zeros((len(node_df), 1))\n",
    "                processed_features.append(default_values)\n",
    "                feature_names.append(f\"{col}_default\")\n",
    "        \n",
    "        # Concatenate all the features\n",
    "        if processed_features:\n",
    "            final_features = np.concatenate(processed_features, axis=1)\n",
    "        else:\n",
    "            final_features = np.zeros((len(node_df), 1))\n",
    "            feature_names = ['dummy_feature']\n",
    "        \n",
    "        # Verify dimensions\n",
    "        print(f\"     Caracter√≠sticas finales: {final_features.shape}\")\n",
    "        \n",
    "        return final_features, feature_names\n",
    "\n",
    "def main_heterogeneous_neo4j():\n",
    "    \"\"\"\n",
    "    Principal Pipeline that uses real data from Neo4j\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # 1. Load data from Neo4j\n",
    "    logger.info(\"üîå Connecting to Neo4j and loading data...\")\n",
    "    \n",
    "    try:\n",
    "        neo4j_loader = Neo4jDataLoader(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "        raw_data = neo4j_loader.load_heterogeneous_data()\n",
    "        neo4j_loader.close()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error connecting to Neo4j: {e}\")\n",
    "        return None\n",
    "        \n",
    "    # 2. Procesar a formato PyTorch Geometric\n",
    "    logger.info(\"üîÑ Processing into PyTorch Geometric format...\")\n",
    "    processor = HeterogeneousGraphProcessor()\n",
    "    hetero_data = processor.create_heterogeneous_graph(raw_data)\n",
    "    \n",
    "    # Verificar que tenemos datos de contadores\n",
    "    if 'contador' not in hetero_data.node_types:\n",
    "        logger.error(\"We couldn' find CONTADOR type of nodes. Abortting...\")\n",
    "        return None\n",
    "        \n",
    "    # 3. Split the data\n",
    "    logger.info(\"üìä Splitting the data...\")\n",
    "    num_contadores = hetero_data['contador'].x.size(0)\n",
    "    indices = torch.randperm(num_contadores)\n",
    "        \n",
    "    train_size = int(0.6 * num_contadores)\n",
    "    val_size = int(0.2 * num_contadores)\n",
    "        \n",
    "    train_mask = torch.zeros(num_contadores, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_contadores, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_contadores, dtype=torch.bool)\n",
    "        \n",
    "    train_mask[indices[:train_size]] = True\n",
    "    val_mask[indices[train_size:train_size + val_size]] = True\n",
    "    test_mask[indices[train_size + val_size:]] = True\n",
    "\n",
    "    # 4. Create heterogeneous model\n",
    "    logger.info(\"üß† Creating heterogeneous GNN model...\")\n",
    "        \n",
    "    # Adjust input dims in terms of real data:\n",
    "    for node_type in hetero_data.node_types:\n",
    "        actual_dim = hetero_data[node_type].x.size(1)\n",
    "        NODE_DIMS_ADV[node_type] = actual_dim\n",
    "        logger.info(f\"   {node_type}: {actual_dim} caracter√≠sticas\")\n",
    "\n",
    "    model = HeterogeneousFraudGNN(\n",
    "        metadata=hetero_data.metadata(),\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_classes=len(FRAUD_CLASSES),\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "        \n",
    "    # Total number of params\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    logger.info(f\"Params of the model: {total_params:,}\")\n",
    "        \n",
    "    # 5. Training\n",
    "    logger.info(\"üéì Training heterogeneous model...\")\n",
    "    trainer = HeterogeneousFraudTrainer(model, device)\n",
    "    hetero_data = hetero_data.to(device)\n",
    "    train_mask = train_mask.to(device)\n",
    "    val_mask = val_mask.to(device)\n",
    "    test_mask = test_mask.to(device)\n",
    "        \n",
    "    history = trainer.train(hetero_data, train_mask, val_mask)\n",
    "\n",
    "    # 6. Evaluation\n",
    "    logger.info(\"üìä Evaluating the model...\")\n",
    "    \n",
    "    # Load the best model in case it exists\n",
    "    best_model_path = 'models/best_hetero_fraud_model.pth'\n",
    "    if os.path.exists(best_model_path):\n",
    "        model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    \n",
    "    evaluator = HeterogeneousEvaluator(model, device)\n",
    "    test_results = evaluator.evaluate_detailed(hetero_data, test_mask)\n",
    "        \n",
    "    # 7. Attention analysis (if it is available)\n",
    "    attention_analysis = None\n",
    "    try:\n",
    "        attention_analysis = evaluator.analyze_attention_weights(hetero_data)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"We couldn't analyze the attention: {e}\")\n",
    "        \n",
    "    # 8. Show the results\n",
    "    logger.info(\"üìà FINAL RESULTS:\")\n",
    "    logger.info(f\"Best precision ih validation: {history['best_val_acc']:.4f}\")\n",
    "    logger.info(f\"ROC AUC: {test_results['roc_auc']:.4f}\")\n",
    "    logger.info(\"\\nClassification report:\")\n",
    "    print(test_results['classification_report'])\n",
    "        \n",
    "    if attention_analysis:\n",
    "        logger.info(\"\\nPesos de atenci√≥n por tipo de informaci√≥n:\")\n",
    "        for info_type, weight in attention_analysis.items():\n",
    "            logger.info(f\"  {info_type}: {weight:.3f}\")\n",
    "        \n",
    "    # 9. Visualizar resultados\n",
    "    plot_heterogeneous_results(history, test_results, attention_analysis)\n",
    "        \n",
    "    # 10. Detectar fraudes potenciales\n",
    "    logger.info(\"üîç Detectando fraudes potenciales...\")\n",
    "    all_predictions = evaluator.predict(hetero_data)\n",
    "    all_probabilities = evaluator.predict_proba(hetero_data)\n",
    "    \n",
    "    # Casos con alta probabilidad de fraude\n",
    "    potential_frauds = []\n",
    "    for i in range(len(all_predictions)):\n",
    "        if hetero_data['contador'].y[i] == 0:  # Etiquetado como normal\n",
    "            fraud_prob = all_probabilities[i][1] + all_probabilities[i][2]\n",
    "            if fraud_prob > 0.7:  # Alta probabilidad de fraude\n",
    "                # Buscar el NIS correspondiente\n",
    "                contador_nis = raw_data['nodes']['contador'].iloc[i]['nis_rad']\n",
    "                potential_frauds.append({\n",
    "                    'contador_idx': i,\n",
    "                    'nis_rad': contador_nis,\n",
    "                    'fraud_probability': fraud_prob,\n",
    "                    'predicted_class': all_predictions[i]\n",
    "                })\n",
    "    \n",
    "    logger.info(f\"Fraudes potenciales detectados: {len(potential_frauds)}\")\n",
    "    \n",
    "    # Show top-5 most suspicious cases\n",
    "    if potential_frauds:\n",
    "        potential_frauds.sort(key=lambda x: x['fraud_probability'], reverse=True)\n",
    "        logger.info(\"Top-5 most suspicious cases:\")\n",
    "        for i, case in enumerate(potential_frauds[:5], 1):\n",
    "            logger.info(f\"  {i}. NIS: {case['nis_rad']}, Prob: {case['fraud_probability']:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_results': test_results,\n",
    "        'attention_analysis': attention_analysis,\n",
    "        'potential_frauds': potential_frauds,\n",
    "        'hetero_data': hetero_data,\n",
    "        'raw_data': raw_data\n",
    "    }\n",
    "\n",
    "class HeterogeneousModelAnalyzer:\n",
    "    \"\"\"\n",
    "    Analizador avanzado para modelos heterog√©neos con datos de Neo4j\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, data, raw_data, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.data = data.to(device)\n",
    "        self.raw_data = raw_data\n",
    "        self.device = device\n",
    "        \n",
    "    def analyze_node_importance(self, node_type='contador', top_k=10):\n",
    "        \"\"\"\n",
    "        Analiza la importancia de nodos de un tipo espec√≠fico\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self.model(self.data.x_dict, self.data.edge_index_dict)\n",
    "            \n",
    "            if node_type == 'contador':\n",
    "                node_embeddings = out\n",
    "                node_labels = self.data['contador'].y if hasattr(self.data['contador'], 'y') else None\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "            # Calculate importance based on the norm of the embedding. This can be done with torch.norm\n",
    "            importance_scores = torch.norm(node_embeddings, dim=1)\n",
    "            \n",
    "            # Get the top-k most important nodes:\n",
    "            top_indices = torch.topk(importance_scores, top_k).indices\n",
    "            \n",
    "            results = []\n",
    "            for idx in top_indices:\n",
    "                # Looks for info of the CONTADOR in the original data\n",
    "                contador_info = self.raw_data['nodes']['contador'].iloc[idx.item()]\n",
    "                \n",
    "                results.append({\n",
    "                    'node_index': idx.item(),\n",
    "                    'nis_rad': contador_info['nis_rad'],\n",
    "                    'marca': contador_info.get('marca', 'N/A'),\n",
    "                    'modelo': contador_info.get('modelo', 'N/A'),\n",
    "                    'importance_score': importance_scores[idx].item(),\n",
    "                    'predicted_class': node_embeddings[idx].argmax().item(),\n",
    "                    'true_class': node_labels[idx].item() if node_labels is not None else None\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_fraud_risk_report(self, threshold=0.7):\n",
    "        \"\"\"\n",
    "        Generates a risk fraud report with detailed information.\n",
    "        \"\"\"\n",
    "        \n",
    "        evaluator = HeterogeneousEvaluator(self.model, self.device)\n",
    "        probabilities = evaluator.predict_proba(self.data)\n",
    "        \n",
    "        # Calculate risk scores\n",
    "        fraud_scores = probabilities[:, 1] + probabilities[:, 2]  # Fraude + Irregularidad\n",
    "        \n",
    "        high_risk_indices = np.where(fraud_scores > threshold)[0]\n",
    "        \n",
    "        report = {\n",
    "            'total_contadores': len(probabilities),\n",
    "            'high_risk_count': len(high_risk_indices),\n",
    "            'high_risk_percentage': len(high_risk_indices) / len(probabilities) * 100,\n",
    "            'average_fraud_score': np.mean(fraud_scores),\n",
    "            'high_risk_nodes': []\n",
    "        }\n",
    "        \n",
    "        for idx in high_risk_indices:\n",
    "            # Obtain detailed info about the CONTADOR\n",
    "            contador_info = self.raw_data['nodes']['contador'].iloc[idx]\n",
    "            \n",
    "            report['high_risk_nodes'].append({\n",
    "                'node_index': int(idx),\n",
    "                'nis_rad': contador_info['nis_rad'],\n",
    "                'marca': contador_info.get('marca', 'N/A'),\n",
    "                'modelo': contador_info.get('modelo', 'N/A'),\n",
    "                'ubicacion': contador_info.get('ubicacion', 'N/A'),\n",
    "                'fraud_probability': float(fraud_scores[idx]),\n",
    "                'predicted_class': int(probabilities[idx].argmax()),\n",
    "                'class_probabilities': {\n",
    "                    'normal': float(probabilities[idx][0]),\n",
    "                    'fraude': float(probabilities[idx][1]),\n",
    "                    'irregularidad': float(probabilities[idx][2])\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Sort fraud probabilities\n",
    "        report['high_risk_nodes'].sort(key=lambda x: x['fraud_probability'], reverse=True)\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f45250-4df3-44d1-b14d-224a310f16ce",
   "metadata": {},
   "source": [
    "## 5.8. Advanced analysis of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08b43d0b-43c4-4c07-964f-3b614c5180ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousModelAnalyzer:\n",
    "    \"\"\"Analizador avanzado para modelos heterog√©neos\"\"\"\n",
    "    \n",
    "    def __init__(self, model, data, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.data = data.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def analyze_node_importance(self, node_type='contador', top_k=10):\n",
    "        \"\"\"Analiza la importancia de nodos de un tipo espec√≠fico\"\"\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Obtener embeddings finales\n",
    "            out = self.model(self.data.x_dict, self.data.edge_index_dict)\n",
    "            \n",
    "            if node_type == 'contador':\n",
    "                node_embeddings = out\n",
    "                node_labels = self.data['contador'].y\n",
    "            else:\n",
    "                # Para otros tipos de nodos, necesitar√≠amos extraer sus embeddings\n",
    "                return None\n",
    "            \n",
    "            # Calcular importancia basada en la norma del embedding\n",
    "            importance_scores = torch.norm(node_embeddings, dim=1)\n",
    "            \n",
    "            # Obtener top-k nodos m√°s importantes\n",
    "            top_indices = torch.topk(importance_scores, top_k).indices\n",
    "            \n",
    "            results = []\n",
    "            for idx in top_indices:\n",
    "                results.append({\n",
    "                    'node_index': idx.item(),\n",
    "                    'importance_score': importance_scores[idx].item(),\n",
    "                    'predicted_class': node_embeddings[idx].argmax().item(),\n",
    "                    'true_class': node_labels[idx].item() if node_labels is not None else None\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_edge_importance(self, edge_type):\n",
    "        \"\"\"Analiza la importancia de diferentes tipos de edges\"\"\"\n",
    "        \n",
    "        # Implementaci√≥n simplificada\n",
    "        if edge_type in self.data.edge_index_dict:\n",
    "            edge_index = self.data.edge_index_dict[edge_type]\n",
    "            num_edges = edge_index.size(1)\n",
    "            \n",
    "            return {\n",
    "                'edge_type': edge_type,\n",
    "                'num_edges': num_edges,\n",
    "                'source_nodes': edge_index[0].unique().size(0),\n",
    "                'target_nodes': edge_index[1].unique().size(0),\n",
    "                'avg_degree': num_edges / edge_index[0].unique().size(0) if edge_index[0].unique().size(0) > 0 else 0\n",
    "            }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def generate_fraud_risk_report(self, threshold=0.7):\n",
    "        \"\"\"Genera reporte de riesgo de fraude\"\"\"\n",
    "        \n",
    "        evaluator = HeterogeneousEvaluator(self.model, self.device)\n",
    "        probabilities = evaluator.predict_proba(self.data)\n",
    "        \n",
    "        # Calcular scores de riesgo\n",
    "        fraud_scores = probabilities[:, 1] + probabilities[:, 2]  # Fraude + Irregularidad\n",
    "        \n",
    "        high_risk_indices = np.where(fraud_scores > threshold)[0]\n",
    "        \n",
    "        report = {\n",
    "            'total_contadores': len(probabilities),\n",
    "            'high_risk_count': len(high_risk_indices),\n",
    "            'high_risk_percentage': len(high_risk_indices) / len(probabilities) * 100,\n",
    "            'average_fraud_score': np.mean(fraud_scores),\n",
    "            'high_risk_nodes': []\n",
    "        }\n",
    "        \n",
    "        for idx in high_risk_indices:\n",
    "            report['high_risk_nodes'].append({\n",
    "                'node_index': int(idx),\n",
    "                'FRAUD_PROBABILITY_ADV': float(fraud_scores[idx]),\n",
    "                'predicted_class': int(probabilities[idx].argmax()),\n",
    "                'class_probabilities': probabilities[idx].tolist()\n",
    "            })\n",
    "        \n",
    "        # Ordenar por probabilidad de fraude\n",
    "        report['high_risk_nodes'].sort(key=lambda x: x['FRAUD_PROBABILITY_ADV'], reverse=True)\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117bafcb-c9ea-4575-8c6b-c00ab6952506",
   "metadata": {},
   "source": [
    "## 5.9. Pipeline execution (Real data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f19b220-0c41-4cae-964f-e12d5f26856d",
   "metadata": {},
   "source": [
    "In this step we will be launching the previously defined pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e4bcc8-ade9-423a-bad9-7ced291c0de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cpu\n",
      "INFO:__main__:üîå Connecting to Neo4j and loading data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING GNN HETEROGENEOUS MODEL WITH NEO4J DATA\n",
      "============================================================\n",
      "üîå Connecting to Neo4j and loading data ...\n",
      "üîó Loading relations from Neo4j...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üîÑ Processing into PyTorch Geometric format...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Etiquetas desde BD: 4933 Normal, 38 Fraude, 34 Irregularidad\n",
      "‚úÖ Cargados 10841 nodos y 64912 relaciones desde Neo4j\n",
      "   - contador: 5005 nodos\n",
      "   - suministro: 5005 nodos\n",
      "   - comercializadora: 21 nodos\n",
      "   - ubicacion: 504 nodos\n",
      "   - concentrador: 102 nodos\n",
      "   - expediente_fraude: 204 nodos\n",
      "üîÑ Procesando grafo heterog√©neo desde Neo4j...\n",
      "   Procesando nodos tipo: contador\n",
      "     Caracter√≠sticas finales: (5005, 64)\n",
      "     ‚úÖ 5005 nodes, 64 features\n",
      "   Procesando nodos tipo: suministro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üìä Splitting the data...\n",
      "INFO:__main__:üß† Creating heterogeneous GNN model...\n",
      "INFO:__main__:   contador: 64 caracter√≠sticas\n",
      "INFO:__main__:   suministro: 47 caracter√≠sticas\n",
      "INFO:__main__:   comercializadora: 11 caracter√≠sticas\n",
      "INFO:__main__:   ubicacion: 24 caracter√≠sticas\n",
      "INFO:__main__:   concentrador: 15 caracter√≠sticas\n",
      "INFO:__main__:   expediente_fraude: 55 caracter√≠sticas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Caracter√≠sticas finales: (5005, 47)\n",
      "     ‚úÖ 5005 nodes, 47 features\n",
      "   Procesando nodos tipo: comercializadora\n",
      "     Caracter√≠sticas finales: (21, 11)\n",
      "     ‚úÖ 21 nodes, 11 features\n",
      "   Procesando nodos tipo: ubicacion\n",
      "     Caracter√≠sticas finales: (504, 24)\n",
      "     ‚úÖ 504 nodes, 24 features\n",
      "   Procesando nodos tipo: concentrador\n",
      "     Caracter√≠sticas finales: (102, 15)\n",
      "     ‚úÖ 102 nodes, 15 features\n",
      "   Procesando nodos tipo: expediente_fraude\n",
      "     Caracter√≠sticas finales: (204, 55)\n",
      "     ‚úÖ 204 nodes, 55 features\n",
      "   ‚úÖ mide: 5010 relaciones (contador -> suministro)\n",
      "   ‚úÖ comunica_via: 5006 relaciones (contador -> concentrador)\n",
      "   ‚úÖ ubicado_en: 5005 relaciones (contador -> ubicacion)\n",
      "   ‚úÖ involucrado_en: 154 relaciones (contador -> expediente_fraude)\n",
      "   ‚úÖ contratado_con: 5005 relaciones (suministro -> comercializadora)\n",
      "   ‚úÖ cerca_de: 15030 relaciones (contador -> contador)\n",
      "   ‚úÖ similar_a: 29702 relaciones (contador -> contador)\n",
      "‚úÖ Heterogeneous graph created with 6 type of nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Params of the model: 218,248\n",
      "INFO:__main__:üéì Training heterogeneous model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéì Iniciando entrenamiento por 300 √©pocas...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configure seeds\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    \n",
    "    print(\"üöÄ STARTING GNN HETEROGENEOUS MODEL WITH NEO4J DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create the directory \"models\" in case it douesn't exist\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # Execute the principal pipeline\n",
    "    results = main_heterogeneous_neo4j()\n",
    "    \n",
    "    if results is not None:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üéØ HETEROGENEOUS TRAINING COMPLETED\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üìä ROC AUC: {results['test_results']['roc_auc']:.4f}\")\n",
    "        print(f\"üéØ Best Val Accuracy: {results['history']['best_val_acc']:.4f}\")\n",
    "        print(f\"üîç Potential fraud: {len(results['potential_frauds'])}\")\n",
    "        print(f\"üíæ Model saved in: models/best_hetero_fraud_model.pth\")\n",
    "        \n",
    "        # # Additional analysis\n",
    "        # analyzer = HeterogeneousModelAnalyzer(\n",
    "        #     results['model'], \n",
    "        #     results['hetero_data'], \n",
    "        # )\n",
    "        \n",
    "        # # Analyze nodes importance\n",
    "        # important_nodes = analyzer.analyze_node_importance(top_k=5)\n",
    "        # print(f\"\\nüîù Top 5 nodos m√°s importantes:\")\n",
    "        # for i, node in enumerate(important_nodes, 1):\n",
    "        #     print(f\"  {i}. NIS: {node['nis_rad']} | {node['marca']} {node['modelo']} | Score: {node['importance_score']:.3f}\")\n",
    "        \n",
    "        # # Generate risk report\n",
    "        # risk_report = analyzer.generate_fraud_risk_report(threshold=0.7)\n",
    "        # print(f\"\\n‚ö†Ô∏è  Risk report:\")\n",
    "        # print(f\"  Total CONTADORES: {risk_report['total_contadores']:,}\")\n",
    "        # print(f\"  High risk: {risk_report['high_risk_count']} ({risk_report['high_risk_percentage']:.1f}%)\")\n",
    "        # print(f\"  Mean score value: {risk_report['average_fraud_score']:.3f}\")\n",
    "        \n",
    "        # if risk_report['high_risk_nodes']:\n",
    "        #     print(f\"\\nüö® Top 3 contadores de mayor riesgo:\")\n",
    "        #     for i, node in enumerate(risk_report['high_risk_nodes'][:3], 1):\n",
    "        #         print(f\"  {i}. NIS: {node['nis_rad']} | {node['marca']} {node['modelo']} | Riesgo: {node['fraud_probability']:.3f}\")\n",
    "        \n",
    "        # print(\"\\n‚úÖ ANALYSIS COMPLETED\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå ERROR: The analysis couldn't be completed\")\n",
    "        print(\"Verify the Neo4j connection and that data exist in the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644b7ec-26fb-4a7b-9d58-0322f4c9c79d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
