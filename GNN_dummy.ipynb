{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abbaa0dc-5ede-415d-a210-4d421d21bbd9",
   "metadata": {},
   "source": [
    "# 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5084a81a-243f-47ad-b7a3-0c2f42e26c8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (5.28.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from neo4j) (2024.1)\n",
      "Requirement already satisfied: torch in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: torch_geometric in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch_geometric) (3.11.10)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch_geometric) (2025.3.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch_geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch_geometric) (3.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from torch_geometric) (2.32.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp->torch_geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\alarrinoar\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch_geometric) (3.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from requests->torch_geometric) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from requests->torch_geometric) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from requests->torch_geometric) (2025.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\alarrinoar\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->torch_geometric) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install neo4j\n",
    "!pip install torch\n",
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a9af8b-d261-456d-959c-d5f1e7b0ab9b",
   "metadata": {},
   "source": [
    "# 2. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939ff8df-8bfb-43a3-baa0-638594954231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdcc8bb-cfac-432d-8c07-8571b15d198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from typing import Dict, List\n",
    "import warnings\n",
    "from config import *\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c718520-2ce9-4d3b-b98f-a240183d0bb2",
   "metadata": {},
   "source": [
    "# 3. Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cd1b8a4-3025-4afb-8e78-8441c0664ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = \"bolt://localhost:7687\"\n",
    "USERNAME = \"neo4j\"\n",
    "PASSWORD = \"password\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff0660c-cbeb-445d-9f88-f82de5460bf0",
   "metadata": {},
   "source": [
    "# 4. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6619468-d180-43aa-819f-0aa4d44518b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query, parameters=None):\n",
    "    \"\"\"\n",
    "    Function used to run a Cypher query against a neo4j graph database.\n",
    "\n",
    "    Parameters:\n",
    "        - query: The query itself.\n",
    "        - parameters: \n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, parameters)\n",
    "        return [record for record in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a9eb43-7da9-40c8-b182-7530577a9fb3",
   "metadata": {},
   "source": [
    "# 5. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f49092-fc4a-4823-9b96-f4290da56476",
   "metadata": {},
   "source": [
    "Vamos a parar la parte de la extracción y pegamos abajo el código que nos proporciona claude para crear el esqueleto del modelado de la red neuronal de grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ca8bf-8fef-4ce4-a9df-a46eb4fb044b",
   "metadata": {},
   "source": [
    "**Librerías que necesitamos -> torch, torch_geometric, sklearn, numpy, pandas, seaborn**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80411e1f-1d70-4ea5-8530-70747a5e6e5a",
   "metadata": {},
   "source": [
    "## 5.2. Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2d6cb-14ad-4906-b03a-929f2a6c205f",
   "metadata": {},
   "source": [
    "We perform the data processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "967e0ccc-aa28-44d6-aa9c-f9aeb8395b6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3096986611.py, line 191)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 191\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mself.labels = labels\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "class FraudGraphDataProcessor:\n",
    "    \"\"\"\n",
    "    Procesador de datos para crear grafos de detección de fraudes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler() # Para normalizar características numéricas para evitar que una característica domine sobre otras.\n",
    "        self.label_encoder = LabelEncoder() # Para codificar etiquetas\n",
    "        self.node_features = None # Guardará las características procesadas\n",
    "        self.edge_index = None # Guardará las conexiones del grafo\n",
    "        self.labels = None # Guardará las etiquetas de clase\n",
    "        \n",
    "    def load_neo4j_data(self, neo4j_connection) -> Dict:\n",
    "        \"\"\"\n",
    "        Carga datos desde Neo4j (adaptable a tu conexión)\n",
    "        \"\"\"\n",
    "        # Consulta para obtener nodos CONTADOR con sus características (marca, modelo, potencia, ...)\n",
    "        contador_query = \"\"\"\n",
    "        MATCH (c:CONTADOR)\n",
    "        OPTIONAL MATCH (c)-[:INVOLUCRADO_EN_FRAUDE]->(ef:EXPEDIENTE_FRAUDE)\n",
    "        OPTIONAL MATCH (c)-[:MIDE_CONSUMO_DE]->(s:SUMINISTRO)\n",
    "        OPTIONAL MATCH (c)-[:INSTALADO_EN]->(u:UBICACION)\n",
    "        OPTIONAL MATCH (c)-[:GENERA_MEDICION]->(m:MEDICION)\n",
    "        RETURN \n",
    "            c.nis_rad as node_id,\n",
    "            c.marca_contador as marca,\n",
    "            c.modelo_contador as modelo,\n",
    "            c.estado_tg as estado_tg,\n",
    "            c.telegest_activo as telegest_activo,\n",
    "            c.potencia_maxima as potencia_maxima,\n",
    "            c.fases_contador as fases,\n",
    "            s.potencia_contratada as potencia_contratada,\n",
    "            s.estado_contrato as estado_contrato,\n",
    "            u.coordenada_x as coord_x,\n",
    "            u.coordenada_y as coord_y,\n",
    "            avg(m.energia_activa) as consumo_promedio,\n",
    "            stdDev(m.energia_activa) as consumo_variabilidad,\n",
    "            count(m) as num_mediciones,\n",
    "            ef.tipo_anomalia as label,\n",
    "            ef.clasificacion_fraude as tipo_fraude,\n",
    "            ef.valoracion_total as impacto_economico\n",
    "        \"\"\"\n",
    "        \n",
    "        # Consulta para obtener relaciones:\n",
    "        # - proximidad geográfica (contadores a menos de 5km)\n",
    "        # - Misma comercializadora (misma empresa suministradora)\n",
    "        # - Mismo modelo (características técnicas similares)\n",
    "        # - Mismo concentrador (Infraestructura compartida)\n",
    "        edges_query = \"\"\"\n",
    "        MATCH (c1:CONTADOR)-[:INSTALADO_EN]->(u:UBICACION)<-[:INSTALADO_EN]-(c2:CONTADOR)\n",
    "        WHERE c1 <> c2\n",
    "        AND point.distance(\n",
    "            point({x: u.coordenada_x, y: u.coordenada_y}),\n",
    "            point({x: u.coordenada_x, y: u.coordenada_y})\n",
    "        ) < 5000\n",
    "        RETURN c1.nis_rad as source, c2.nis_rad as target, 'CERCANO' as relation_type\n",
    "        \n",
    "        UNION\n",
    "        \n",
    "        MATCH (c1:CONTADOR)-[:MIDE_CONSUMO_DE]->(s:SUMINISTRO)-[:CONTRATADO_CON]->(com:COMERCIALIZADORA)\n",
    "              <-[:CONTRATADO_CON]-(s2:SUMINISTRO)<-[:MIDE_CONSUMO_DE]-(c2:CONTADOR)\n",
    "        WHERE c1 <> c2\n",
    "        RETURN c1.nis_rad as source, c2.nis_rad as target, 'MISMA_COMERCIALIZADORA' as relation_type\n",
    "        \"\"\"\n",
    "        \n",
    "        # Simulación de datos. Más adelante lo modificaremos con nuestra conexión real\n",
    "        return self._simulate_data()\n",
    "    \n",
    "    def _simulate_data(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Simula datos basados en el esquema de Neo4j. Creamos datos sintéticos para testing y desarrollo cuando no \n",
    "        tenemos acceso a neo4j\n",
    "        \"\"\"\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        \n",
    "        # Crear nodos de contadores, en este caso crearemos 10500:\n",
    "        n_nodes = 10000\n",
    "        n_fraud = 500  # Setearemos un 5% fraudes\n",
    "        \n",
    "        # Características de nodos\n",
    "        node_data = {\n",
    "            'node_id': [f\"NIS_{i:06d}\" for i in range(n_nodes)],\n",
    "            'marca': np.random.choice(['MARCA_A', 'MARCA_B', 'MARCA_C'], n_nodes),\n",
    "            'modelo': np.random.choice(['MOD_1', 'MOD_2', 'MOD_3'], n_nodes),\n",
    "            'estado_tg': np.random.choice(['INTEGRADO', 'NO_INTEGRADO'], n_nodes, p=[0.8, 0.2]),\n",
    "            'telegest_activo': np.random.choice([True, False], n_nodes, p=[0.9, 0.1]),\n",
    "            'potencia_maxima': np.random.normal(5000, 2000, n_nodes),\n",
    "            'potencia_contratada': np.random.normal(3000, 1500, n_nodes),\n",
    "            'coord_x': np.random.uniform(400000, 600000, n_nodes),\n",
    "            'coord_y': np.random.uniform(4400000, 4800000, n_nodes),\n",
    "            'consumo_promedio': np.random.exponential(50, n_nodes),\n",
    "            'consumo_variabilidad': np.random.exponential(20, n_nodes),\n",
    "            'num_mediciones': np.random.poisson(100, n_nodes),\n",
    "        }\n",
    "        \n",
    "        # Crear etiquetas (la mayoría normal, algunos fraudes)\n",
    "        labels = ['NORMAL'] * n_nodes\n",
    "        fraud_indices = np.random.choice(n_nodes, n_fraud, replace=False)\n",
    "        \n",
    "        for i in fraud_indices:\n",
    "            labels[i] = np.random.choice(['FRAUDE', 'IRREGULARIDAD'], p=[0.7, 0.3])\n",
    "            # Hacer que los fraudes tengan patrones diferentes\n",
    "            if labels[i] == 'FRAUDE':\n",
    "                node_data['consumo_promedio'][i] *= 0.3  # Consumo muy bajo\n",
    "                node_data['consumo_variabilidad'][i] *= 0.5  # Menor variabilidad\n",
    "            \n",
    "        node_data['label'] = labels\n",
    "        \n",
    "        # Crear las relaciones entre nodos\n",
    "        edges = []\n",
    "        \n",
    "        # Bordes por proximidad geográfica\n",
    "        for i in range(n_nodes):\n",
    "            for j in range(i+1, min(i+20, n_nodes)):  # Limitar para eficiencia\n",
    "                dist = np.sqrt((node_data['coord_x'][i] - node_data['coord_x'][j])**2 + \n",
    "                              (node_data['coord_y'][i] - node_data['coord_y'][j])**2)\n",
    "                if dist < 5000:  # 5km\n",
    "                    edges.append([i, j])\n",
    "                    edges.append([j, i])  # Borde bidireccional\n",
    "        \n",
    "        # Bordes por similaridad técnica\n",
    "        for i in range(n_nodes):\n",
    "            for j in range(i+1, n_nodes):\n",
    "                if (node_data['marca'][i] == node_data['marca'][j] and \n",
    "                    node_data['modelo'][i] == node_data['modelo'][j] and\n",
    "                    np.random.random() < 0.1):  # 10% de probabilidad\n",
    "                    edges.append([i, j])\n",
    "                    edges.append([j, i])\n",
    "        \n",
    "        return {\n",
    "            'nodes': pd.DataFrame(node_data),\n",
    "            'edges': np.array(edges) if edges else np.array([]).reshape(0, 2)\n",
    "        }\n",
    "    \n",
    "    def create_graph_data(self, data: Dict) -> Data:\n",
    "        \"\"\"\n",
    "        Convierte datos en formato Data de PyTorch Geometric.\n",
    "        \"\"\"\n",
    "        \n",
    "        nodes_df = data['nodes']\n",
    "        edges = data['edges']\n",
    "        \n",
    "        # Mapear node_ids a índices\n",
    "        node_to_idx = {node_id: idx for idx, node_id in enumerate(nodes_df['node_id'])}\n",
    "        \n",
    "        # Preparar características de nodos\n",
    "        feature_columns = [\n",
    "            'potencia_maxima', 'potencia_contratada', 'coord_x', 'coord_y',\n",
    "            'consumo_promedio', 'consumo_variabilidad', 'num_mediciones'\n",
    "        ]\n",
    "        \n",
    "        # Codificar características categóricas con one hot encoding en este caso:\n",
    "        marca_encoded = pd.get_dummies(nodes_df['marca'], prefix='marca')\n",
    "        modelo_encoded = pd.get_dummies(nodes_df['modelo'], prefix='modelo')\n",
    "        estado_tg_encoded = pd.get_dummies(nodes_df['estado_tg'], prefix='estado_tg')\n",
    "        \n",
    "        # Características binarias\n",
    "        telegest_feature = nodes_df['telegest_activo'].astype(int)\n",
    "        \n",
    "        # Combinar todas las características\n",
    "        numeric_features = self.scaler.fit_transform(nodes_df[feature_columns])\n",
    "\n",
    "        # combinamos todas las características:\n",
    "        node_features = np.concatenate([\n",
    "            numeric_features,\n",
    "            marca_encoded.values,\n",
    "            modelo_encoded.values,\n",
    "            estado_tg_encoded.values,\n",
    "            telegest_feature.values.reshape(-1, 1)\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Preparar etiquetas\n",
    "        labels = [FRAUD_CLASSES[label] for label in nodes_df['label']]\n",
    "        \n",
    "        # Crear tensores\n",
    "        x = torch.tensor(node_features, dtype=torch.float) # características\n",
    "        y = torch.tensor(labels, dtype=torch.long) # etiquetas\n",
    "        \n",
    "        # Crear edge_index\n",
    "        if len(edges) > 0:\n",
    "            # primera fila nodos origen y segunda fila nodo destino\n",
    "            edge_index = torch.tensor(edges.T, dtype=torch.long)\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        \n",
    "        # Crear objeto Data final:\n",
    "        graph_data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        \n",
    "        # Guardar información para uso posterior\n",
    "        self.node_features = node_features\n",
    "         self.labels = labels\n",
    "        self.feature_names = (feature_columns + \n",
    "                             list(marca_encoded.columns) + \n",
    "                             list(modelo_encoded.columns) + \n",
    "                             list(estado_tg_encoded.columns) + \n",
    "                             ['telegest_activo'])\n",
    "        \n",
    "        return graph_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae50a4-e1e7-4ce2-aa8c-97241a32c600",
   "metadata": {},
   "source": [
    "## 5.3. Modelo GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809c023-8be9-4e06-9019-5b5ca394bec0",
   "metadata": {},
   "source": [
    "Vamos a construir la arquitectura, el esqueleto de la red neuronal GNN que será o bien una GCN o bien una GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccc37c4-cc7c-4255-84b2-d2aeb75fd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arquitectura_GNN_homo import FraudGNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf39e1e-189b-4b48-985d-5712c27dd37f",
   "metadata": {},
   "source": [
    "## 5.4. Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5e209-bf9c-4c03-bd33-a36aad963d49",
   "metadata": {},
   "source": [
    "Construcción de la clase utilizada para entrenar el modelo GNN definido en 5.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b344f-c3c0-46e9-8361-39a6bd11dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import FraudTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd351ac-ae41-40d9-a8d4-8b84bd4a2159",
   "metadata": {},
   "source": [
    "## 5.5. Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da2579c-6703-4f79-b629-d7b903c9f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import FraudEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318ea417-81e2-475d-975d-1ef6a9b54ef0",
   "metadata": {},
   "source": [
    "## 5.6. Construcción del Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489fc4d-5f7e-4e5c-a407-60e30e7d6125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6. PIPELINE PRINCIPAL\n",
    "# ==========================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Pipeline principal de entrenamiento\"\"\"\n",
    "    \n",
    "    # Configurar dispositivo\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "    \n",
    "    # 1. Cargar y procesar datos\n",
    "    print(\"1. Cargando datos...\")\n",
    "    processor = FraudGraphDataProcessor()\n",
    "    raw_data = processor.load_neo4j_data(None)  # Usar datos simulados\n",
    "    graph_data = processor.create_graph_data(raw_data)\n",
    "    \n",
    "    print(f\"   - Nodos: {graph_data.num_nodes}\")\n",
    "    print(f\"   - Bordes: {graph_data.num_edges}\")\n",
    "    print(f\"   - Características: {graph_data.num_node_features}\")\n",
    "    \n",
    "    # 2. Dividir datos\n",
    "    print(\"2. Dividiendo datos...\")\n",
    "    num_nodes = graph_data.num_nodes\n",
    "    # Use randperm in order to reorder the indexes\n",
    "    indices = torch.randperm(num_nodes)\n",
    "    \n",
    "    train_size = int(0.6 * num_nodes)\n",
    "    val_size = int(0.2 * num_nodes)\n",
    "    \n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    # Just to select train, test and val data:\n",
    "    train_mask[indices[:train_size]] = True\n",
    "    val_mask[indices[train_size:train_size + val_size]] = True\n",
    "    test_mask[indices[train_size + val_size:]] = True\n",
    "    \n",
    "    # 3. Crear modelo\n",
    "    print(\"3. Creando modelo...\")\n",
    "    model = FraudGNN(\n",
    "        input_dim=graph_data.num_node_features,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_classes=len(FRAUD_CLASSES),\n",
    "        model_type='GCN'  # o 'GAT'\n",
    "    )\n",
    "    # p.numel devuelve el número de elementos que tiene un tensor.\n",
    "    print(f\"   - Parámetros del modelo: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # 4. Entrenar\n",
    "    print(\"4. Entrenando modelo...\")\n",
    "    trainer = FraudTrainer(model, device)\n",
    "    graph_data = graph_data.to(device)\n",
    "    train_mask = train_mask.to(device)\n",
    "    val_mask = val_mask.to(device)\n",
    "    test_mask = test_mask.to(device)\n",
    "    \n",
    "    history = trainer.train(graph_data, train_mask, val_mask)\n",
    "    \n",
    "    # 5. Evaluar\n",
    "    print(\"5. Evaluando modelo...\")\n",
    "    model.load_state_dict(torch.load('best_fraud_model.pth'))\n",
    "    evaluator = FraudEvaluator(model, device)\n",
    "    \n",
    "    test_results = evaluator.evaluate_detailed(graph_data, test_mask)\n",
    "    \n",
    "    # 6. Mostrar resultados\n",
    "    print(\"6. Resultados:\")\n",
    "    print(f\"   - Mejor precisión de validación: {history['best_val_acc']:.4f}\")\n",
    "    print(f\"   - ROC AUC: {test_results['roc_auc']:.4f}\")\n",
    "    print(\"\\nReporte de clasificación:\")\n",
    "    print(test_results['classification_report'])\n",
    "    \n",
    "    # 7. Visualizar\n",
    "    print(\"7. Generando visualizaciones...\")\n",
    "    evaluator.plot_training_history(history)\n",
    "    evaluator.plot_confusion_matrix(test_results['confusion_matrix'])\n",
    "    \n",
    "    # 8. Detectar fraudes potenciales\n",
    "    print(\"8. Detectando fraudes potenciales...\")\n",
    "    all_predictions = evaluator.predict(graph_data)\n",
    "    all_probabilities = evaluator.predict_proba(graph_data)\n",
    "    \n",
    "    # Casos con alta probabilidad de fraude pero sin etiqueta\n",
    "    unlabeled_fraud_candidates = []\n",
    "    for i in range(len(all_predictions)):\n",
    "        if graph_data.y[i] == 0:  # Etiquetado como normal\n",
    "            fraud_prob = all_probabilities[i][1] + all_probabilities[i][2]\n",
    "            if fraud_prob > 0.7:  # Alta probabilidad de fraude\n",
    "                unlabeled_fraud_candidates.append({\n",
    "                    'node_index': i,\n",
    "                    'fraud_probability': fraud_prob,\n",
    "                    'predicted_class': all_predictions[i]\n",
    "                })\n",
    "    \n",
    "    print(f\"   - Casos sospechosos encontrados: {len(unlabeled_fraud_candidates)}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_results': test_results,\n",
    "        'fraud_candidates': unlabeled_fraud_candidates,\n",
    "        'processor': processor\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ejecutar pipeline\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    results = main()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ENTRENAMIENTO COMPLETADO\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Modelo guardado en: best_fraud_model.pth\")\n",
    "    print(f\"Fraudes potenciales detectados: {len(results['fraud_candidates'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622196a-e3fd-44c4-b21b-6ecf97bb5038",
   "metadata": {},
   "source": [
    "## 5.7. Integración con Neo4j real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780fedee-cc00-4520-99a1-59682b4c7b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 7. INTEGRACIÓN CON NEO4J REAL\n",
    "# ==========================================\n",
    "\n",
    "class Neo4jGraphLoader:\n",
    "    \"\"\"Cargador específico para datos reales de Neo4j\"\"\"\n",
    "    \n",
    "    def __init__(self, uri: str, user: str, password: str):\n",
    "        try:\n",
    "            self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "            # Test de conexión\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(\"RETURN 1 as test\")\n",
    "                test_val = result.single()[\"test\"]\n",
    "                if test_val == 1:\n",
    "                    print(\"✅ Conexión a Neo4j establecida correctamente\")\n",
    "                else:\n",
    "                    raise Exception(\"Error en test de conexión\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"❌ Error conectando a Neo4j: {e}\")\n",
    "        \n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "            print(\"🔌 Conexión a Neo4j cerrada\")\n",
    "        \n",
    "    def load_fraud_graph_data(self) -> Dict:\n",
    "        \"\"\"Carga datos reales desde tu base de datos Neo4j\"\"\"\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            # Consulta optimizada para obtener todas las características de nodos\n",
    "            nodes_query = \"\"\"\n",
    "            MATCH (c:CONTADOR)\n",
    "            OPTIONAL MATCH (c)-[:INVOLUCRADO_EN_FRAUDE]->(ef:EXPEDIENTE_FRAUDE)\n",
    "            OPTIONAL MATCH (c)-[:MIDE_CONSUMO_DE]->(s:SUMINISTRO)\n",
    "            OPTIONAL MATCH (c)-[:INSTALADO_EN]->(u:UBICACION)\n",
    "            OPTIONAL MATCH (c)-[:CONECTADO_A]->(conc:CONCENTRADOR)\n",
    "            OPTIONAL MATCH (c)-[:GENERA_MEDICION]->(m:MEDICION)\n",
    "            OPTIONAL MATCH (c)-[:GENERA_EVENTO]->(e:EVENTO)\n",
    "            OPTIONAL MATCH (c)-[:INSPECCIONADO_EN]->(i:INSPECCION)\n",
    "            \n",
    "            WITH c, ef, s, u, conc,\n",
    "                 count(DISTINCT m) as num_mediciones,\n",
    "                 avg(m.energia_activa) as consumo_promedio,\n",
    "                 stdDev(m.energia_activa) as consumo_variabilidad,\n",
    "                 min(m.timestamp_medicion) as primera_medicion,\n",
    "                 max(m.timestamp_medicion) as ultima_medicion,\n",
    "                 count(DISTINCT e) as num_eventos,\n",
    "                 count(DISTINCT i) as num_inspecciones,\n",
    "                 collect(DISTINCT e.tipo_reporte) as tipos_eventos\n",
    "                 \n",
    "            RETURN \n",
    "                c.nis_rad as node_id,\n",
    "                c.numero_contador as numero_contador,\n",
    "                c.marca_contador as marca,\n",
    "                c.modelo_contador as modelo,\n",
    "                c.tipo_aparato as tipo_aparato,\n",
    "                c.estado_tg as estado_tg,\n",
    "                c.telegest_activo as telegest_activo,\n",
    "                c.potencia_maxima as potencia_maxima,\n",
    "                c.fases_contador as fases,\n",
    "                c.tension as tension,\n",
    "                c.version_firmware as version_firmware,\n",
    "                \n",
    "                s.estado_contrato as estado_contrato,\n",
    "                s.potencia_contratada as potencia_contratada,\n",
    "                s.tension_suministro as tension_suministro,\n",
    "                s.tipo_punto as tipo_punto,\n",
    "                s.cnae as cnae,\n",
    "                s.tarifa_activa as tarifa,\n",
    "                s.comercializadora_codigo as comercializadora,\n",
    "                \n",
    "                u.coordenada_x as coord_x,\n",
    "                u.coordenada_y as coord_y,\n",
    "                u.codigo_postal as codigo_postal,\n",
    "                u.area_ejecucion as area_ejecucion,\n",
    "                \n",
    "                conc.estado_comunicacion as estado_comunicacion,\n",
    "                conc.version_concentrador as version_concentrador,\n",
    "                \n",
    "                num_mediciones,\n",
    "                consumo_promedio,\n",
    "                consumo_variabilidad,\n",
    "                primera_medicion,\n",
    "                ultima_medicion,\n",
    "                num_eventos,\n",
    "                num_inspecciones,\n",
    "                tipos_eventos,\n",
    "                \n",
    "                ef.tipo_anomalia as label,\n",
    "                ef.clasificacion_fraude as tipo_fraude,\n",
    "                ef.valoracion_total as impacto_economico,\n",
    "                ef.energia_facturada as energia_facturada,\n",
    "                ef.fecha_inicio_anomalia as fecha_inicio_fraude,\n",
    "                ef.fecha_fin_anomalia as fecha_fin_fraude\n",
    "            \"\"\"\n",
    "            \n",
    "            # Consulta para relaciones del grafo\n",
    "            edges_query = \"\"\"\n",
    "            // Relaciones por proximidad geográfica (< 2km)\n",
    "            MATCH (c1:CONTADOR)-[:INSTALADO_EN]->(u1:UBICACION)\n",
    "            MATCH (c2:CONTADOR)-[:INSTALADO_EN]->(u2:UBICACION)\n",
    "            WHERE c1 <> c2\n",
    "            AND point.distance(\n",
    "                point({x: u1.coordenada_x, y: u1.coordenada_y}),\n",
    "                point({x: u2.coordenada_x, y: u2.coordenada_y})\n",
    "            ) < 2000\n",
    "            RETURN c1.nis_rad as source, c2.nis_rad as target, \n",
    "                   'PROXIMIDAD_GEOGRAFICA' as relation_type,\n",
    "                   point.distance(\n",
    "                       point({x: u1.coordenada_x, y: u1.coordenada_y}),\n",
    "                       point({x: u2.coordenada_x, y: u2.coordenada_y})\n",
    "                   ) as distancia\n",
    "            \n",
    "            UNION\n",
    "            \n",
    "            // Relaciones por misma comercializadora\n",
    "            MATCH (c1:CONTADOR)-[:MIDE_CONSUMO_DE]->(s1:SUMINISTRO)\n",
    "            MATCH (c2:CONTADOR)-[:MIDE_CONSUMO_DE]->(s2:SUMINISTRO)\n",
    "            WHERE c1 <> c2 \n",
    "            AND s1.comercializadora_codigo = s2.comercializadora_codigo\n",
    "            RETURN c1.nis_rad as source, c2.nis_rad as target,\n",
    "                   'MISMA_COMERCIALIZADORA' as relation_type,\n",
    "                   s1.comercializadora_codigo as comercializadora\n",
    "            \n",
    "            UNION\n",
    "            \n",
    "            // Relaciones por misma marca y modelo\n",
    "            MATCH (c1:CONTADOR), (c2:CONTADOR)\n",
    "            WHERE c1 <> c2\n",
    "            AND c1.marca_contador = c2.marca_contador\n",
    "            AND c1.modelo_contador = c2.modelo_contador\n",
    "            RETURN c1.nis_rad as source, c2.nis_rad as target,\n",
    "                   'MISMO_MODELO' as relation_type,\n",
    "                   c1.marca_contador + '_' + c1.modelo_contador as modelo\n",
    "            \n",
    "            UNION\n",
    "            \n",
    "            // Relaciones por mismo concentrador\n",
    "            MATCH (c1:CONTADOR)-[:CONECTADO_A]->(conc:CONCENTRADOR)<-[:CONECTADO_A]-(c2:CONTADOR)\n",
    "            WHERE c1 <> c2\n",
    "            RETURN c1.nis_rad as source, c2.nis_rad as target,\n",
    "                   'MISMO_CONCENTRADOR' as relation_type,\n",
    "                   conc.concentrador_id as concentrador_id\n",
    "            \"\"\"\n",
    "            \n",
    "            # Ejecutar consultas\n",
    "            nodes_result = session.run(nodes_query)\n",
    "            edges_result = session.run(edges_query)\n",
    "            \n",
    "            # Convertir a DataFrames\n",
    "            nodes_df = pd.DataFrame([dict(record) for record in nodes_result])\n",
    "            edges_df = pd.DataFrame([dict(record) for record in edges_result])\n",
    "            \n",
    "            return {\n",
    "                'nodes': nodes_df,\n",
    "                'edges': edges_df\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f78031-9489-4556-9ac7-5ab3ebf5582c",
   "metadata": {},
   "source": [
    "## 5.8. Análisis temporal avanzado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f91a8e-2a05-449f-bcaf-1e75f578f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 8. ANÁLISIS TEMPORAL AVANZADO\n",
    "# ==========================================\n",
    "\n",
    "class TemporalFraudAnalyzer:\n",
    "    \"\"\"Análisis temporal para detección de fraudes\"\"\"\n",
    "    \n",
    "    def __init__(self, neo4j_loader: Neo4jGraphLoader):\n",
    "        self.loader = neo4j_loader\n",
    "        \n",
    "    def extract_temporal_features(self, nis_rad: str, days_back: int = 365) -> Dict:\n",
    "        \"\"\"Extrae características temporales específicas para un contador\"\"\"\n",
    "        \n",
    "        with self.loader.driver.session() as session:\n",
    "            query = \"\"\"\n",
    "            MATCH (c:CONTADOR {nis_rad: $nis_rad})-[:GENERA_MEDICION]->(m:MEDICION)\n",
    "            WHERE m.timestamp_medicion >= datetime() - duration('P' + $days_back + 'D')\n",
    "            WITH c, m\n",
    "            ORDER BY m.timestamp_medicion\n",
    "            \n",
    "            WITH c, collect(m) as mediciones\n",
    "            \n",
    "            // Calcular características temporales\n",
    "            WITH c, mediciones,\n",
    "                 [m IN mediciones | m.energia_activa] as consumos,\n",
    "                 [i IN range(1, size(mediciones)-1) | \n",
    "                    mediciones[i].energia_activa - mediciones[i-1].energia_activa] as diferencias\n",
    "            \n",
    "            RETURN \n",
    "                // Estadísticas básicas\n",
    "                avg([m IN mediciones | m.energia_activa]) as consumo_promedio,\n",
    "                stdDev([m IN mediciones | m.energia_activa]) as consumo_std,\n",
    "                min([m IN mediciones | m.energia_activa]) as consumo_min,\n",
    "                max([m IN mediciones | m.energia_activa]) as consumo_max,\n",
    "                \n",
    "                // Detección de cambios bruscos\n",
    "                size([d IN diferencias WHERE abs(d) > 100]) as cambios_bruscos,\n",
    "                max([d IN diferencias | abs(d)]) as max_cambio,\n",
    "                \n",
    "                // Patrones de resistencia (indicador de manipulación)\n",
    "                avg([m IN mediciones | m.resistencia_r1]) as resistencia_r1_promedio,\n",
    "                stdDev([m IN mediciones | m.resistencia_r1]) as resistencia_r1_std,\n",
    "                \n",
    "                // Consistencia temporal\n",
    "                size(mediciones) as total_mediciones,\n",
    "                duration.between(mediciones[0].timestamp_medicion, \n",
    "                               mediciones[-1].timestamp_medicion).days as periodo_dias\n",
    "            \"\"\"\n",
    "            \n",
    "            result = session.run(query, nis_rad=nis_rad, days_back=days_back)\n",
    "            record = result.single()\n",
    "            \n",
    "            if record:\n",
    "                return dict(record)\n",
    "            return {}\n",
    "    \n",
    "    def detect_consumption_anomalies(self, threshold_std: float = 3.0) -> List[Dict]:\n",
    "        \"\"\"Detecta anomalías en patrones de consumo\"\"\"\n",
    "        \n",
    "        with self.loader.driver.session() as session:\n",
    "            query = \"\"\"\n",
    "            MATCH (c:CONTADOR)-[:GENERA_MEDICION]->(m:MEDICION)\n",
    "            WHERE m.timestamp_medicion >= datetime() - duration('P90D')\n",
    "            \n",
    "            WITH c, \n",
    "                 avg(m.energia_activa) as consumo_promedio,\n",
    "                 stdDev(m.energia_activa) as consumo_std,\n",
    "                 collect(m.energia_activa) as consumos,\n",
    "                 count(m) as num_mediciones\n",
    "            \n",
    "            WHERE consumo_std > 0 AND num_mediciones > 10\n",
    "            \n",
    "            // Identificar mediciones anómalas\n",
    "            WITH c, consumo_promedio, consumo_std, consumos,\n",
    "                 [consumo IN consumos WHERE \n",
    "                    abs(consumo - consumo_promedio) > $threshold_std * consumo_std] as anomalias\n",
    "            \n",
    "            WHERE size(anomalias) > 0\n",
    "            \n",
    "            RETURN c.nis_rad as nis_rad,\n",
    "                   consumo_promedio,\n",
    "                   consumo_std,\n",
    "                   size(anomalias) as num_anomalias,\n",
    "                   size(consumos) as total_mediciones,\n",
    "                   (toFloat(size(anomalias)) / size(consumos)) as porcentaje_anomalias\n",
    "            \n",
    "            ORDER BY porcentaje_anomalias DESC\n",
    "            \"\"\"\n",
    "            \n",
    "            result = session.run(query, threshold_std=threshold_std)\n",
    "            return [dict(record) for record in result]\n",
    "\n",
    "# ==========================================\n",
    "# 9. MODELO GNN HÍBRIDO CON SERIES TEMPORALES\n",
    "# ==========================================\n",
    "\n",
    "class HybridTemporalGNN(nn.Module):\n",
    "    \"\"\"GNN híbrido que combina características estáticas y temporales\"\"\"\n",
    "    \n",
    "    def __init__(self, static_dim: int, temporal_dim: int, hidden_dim: int = 64, \n",
    "                 num_classes: int = 3, sequence_length: int = 30):\n",
    "        super(HybridTemporalGNN, self).__init__()\n",
    "        \n",
    "        # Encoder para características estáticas (GNN)\n",
    "        self.static_conv1 = GCNConv(static_dim, hidden_dim)\n",
    "        self.static_conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Encoder para series temporales (LSTM/GRU)\n",
    "        self.temporal_encoder = nn.LSTM(\n",
    "            temporal_dim, hidden_dim, batch_first=True, num_layers=2, dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Capa de atención para combinar información\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, batch_first=True)\n",
    "        \n",
    "        # Clasificador final\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, static_x, temporal_x, edge_index, batch=None):\n",
    "        # Procesar características estáticas con GNN\n",
    "        static_out = F.relu(self.static_conv1(static_x, edge_index))\n",
    "        static_out = F.dropout(static_out, training=self.training)\n",
    "        static_out = self.static_conv2(static_out, edge_index)\n",
    "        \n",
    "        # Procesar series temporales con LSTM\n",
    "        temporal_out, _ = self.temporal_encoder(temporal_x)\n",
    "        temporal_out = temporal_out[:, -1, :]  # Tomar último estado\n",
    "        \n",
    "        # Combinar con atención\n",
    "        combined_features = torch.cat([\n",
    "            static_out.unsqueeze(1), \n",
    "            temporal_out.unsqueeze(1)\n",
    "        ], dim=1)\n",
    "        \n",
    "        attended_features, _ = self.attention(\n",
    "            combined_features, combined_features, combined_features\n",
    "        )\n",
    "        \n",
    "        # Concatenar características finales\n",
    "        final_features = torch.cat([\n",
    "            attended_features[:, 0, :],  # Estáticas\n",
    "            attended_features[:, 1, :]   # Temporales\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Clasificar\n",
    "        output = self.classifier(final_features)\n",
    "        return F.log_softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725a567-96c4-4095-8d56-f81136d96605",
   "metadata": {},
   "source": [
    "## 5.9. Monitoreo en tiempo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8142f1-1699-4741-9b8e-7a1afdfeded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 10. SISTEMA DE MONITOREO EN TIEMPO REAL\n",
    "# ==========================================\n",
    "\n",
    "class RealTimeFraudMonitor:\n",
    "    \"\"\"Monitor en tiempo real para detección de fraudes\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, neo4j_loader: Neo4jGraphLoader,\n",
    "                 threshold_fraud: float = 0.7):\n",
    "        self.model = torch.load(model_path)\n",
    "        self.model.eval()\n",
    "        self.neo4j_loader = neo4j_loader\n",
    "        self.threshold_fraud = threshold_fraud\n",
    "        self.alert_history = []\n",
    "        \n",
    "    def check_new_measurements(self, hours_back: int = 24) -> List[Dict]:\n",
    "        \"\"\"Revisa mediciones recientes en busca de patrones fraudulentos\"\"\"\n",
    "        \n",
    "        with self.neo4j_loader.driver.session() as session:\n",
    "            query = \"\"\"\n",
    "            MATCH (c:CONTADOR)-[:GENERA_MEDICION]->(m:MEDICION)\n",
    "            WHERE m.timestamp_medicion >= datetime() - duration('PT' + $hours_back + 'H')\n",
    "            \n",
    "            // Agrupar por contador y calcular cambios\n",
    "            WITH c, collect(m ORDER BY m.timestamp_medicion) as mediciones\n",
    "            WHERE size(mediciones) >= 2\n",
    "            \n",
    "            WITH c, mediciones,\n",
    "                 mediciones[-1].energia_activa as ultimo_consumo,\n",
    "                 mediciones[-2].energia_activa as penultimo_consumo,\n",
    "                 avg([m IN mediciones | m.energia_activa]) as consumo_promedio_reciente\n",
    "            \n",
    "            // Detectar caídas bruscas o patrones anómalos\n",
    "            WHERE ultimo_consumo < (penultimo_consumo * 0.3)  // Caída >70%\n",
    "               OR ultimo_consumo < (consumo_promedio_reciente * 0.2)  // Muy bajo vs promedio\n",
    "            \n",
    "            RETURN c.nis_rad as nis_rad,\n",
    "                   ultimo_consumo,\n",
    "                   penultimo_consumo,\n",
    "                   consumo_promedio_reciente,\n",
    "                   ((penultimo_consumo - ultimo_consumo) / penultimo_consumo) as porcentaje_caida\n",
    "            \n",
    "            ORDER BY porcentaje_caida DESC\n",
    "            \"\"\"\n",
    "            \n",
    "            result = session.run(query, hours_back=hours_back)\n",
    "            suspicious_cases = [dict(record) for record in result]\n",
    "            \n",
    "            # Evaluar cada caso con el modelo GNN\n",
    "            alerts = []\n",
    "            for case in suspicious_cases:\n",
    "                fraud_probability = self._evaluate_fraud_probability(case['nis_rad'])\n",
    "                \n",
    "                if fraud_probability > self.threshold_fraud:\n",
    "                    alert = {\n",
    "                        'timestamp': pd.Timestamp.now(),\n",
    "                        'nis_rad': case['nis_rad'],\n",
    "                        'fraud_probability': fraud_probability,\n",
    "                        'trigger': 'CAIDA_BRUSCA_CONSUMO',\n",
    "                        'details': case,\n",
    "                        'priority': 'HIGH' if fraud_probability > 0.9 else 'MEDIUM'\n",
    "                    }\n",
    "                    alerts.append(alert)\n",
    "                    self.alert_history.append(alert)\n",
    "            \n",
    "            return alerts\n",
    "    \n",
    "    def _evaluate_fraud_probability(self, nis_rad: str) -> float:\n",
    "        \"\"\"Evalúa probabilidad de fraude para un contador específico\"\"\"\n",
    "        # Aquí implementarías la evaluación con el modelo GNN\n",
    "        # Por simplicidad, retorna un valor simulado\n",
    "        return np.random.random()\n",
    "    \n",
    "    def generate_daily_report(self) -> Dict:\n",
    "        \"\"\"Genera reporte diario de actividad fraudulenta\"\"\"\n",
    "        \n",
    "        recent_alerts = [alert for alert in self.alert_history \n",
    "                        if alert['timestamp'] > pd.Timestamp.now() - pd.Timedelta(days=1)]\n",
    "        \n",
    "        return {\n",
    "            'fecha': pd.Timestamp.now().date(),\n",
    "            'total_alertas': len(recent_alerts),\n",
    "            'alertas_alta_prioridad': len([a for a in recent_alerts if a['priority'] == 'HIGH']),\n",
    "            'alertas_media_prioridad': len([a for a in recent_alerts if a['priority'] == 'MEDIUM']),\n",
    "            'tipos_triggers': pd.Series([a['trigger'] for a in recent_alerts]).value_counts().to_dict(),\n",
    "            'contadores_afectados': list(set([a['nis_rad'] for a in recent_alerts])),\n",
    "            'probabilidad_fraude_promedio': np.mean([a['fraud_probability'] for a in recent_alerts]) if recent_alerts else 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f874d0-692c-4a9e-bc99-3b29cd07bda3",
   "metadata": {},
   "source": [
    "## 5.10. Uso completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beabf76b-35a7-4e78-8871-6d854e2c519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 11. EJEMPLO DE USO COMPLETO\n",
    "# ==========================================\n",
    "\n",
    "def run_production_pipeline():\n",
    "    \"\"\"Pipeline completo para producción\"\"\"\n",
    "    \n",
    "    print(\"🚀 INICIANDO PIPELINE DE DETECCIÓN DE FRAUDES GNN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Conectar a Neo4j\n",
    "    print(\"1. Conectando a Neo4j...\")\n",
    "    neo4j_loader = Neo4jGraphLoader(\n",
    "        uri=\"bolt://localhost:7687\",\n",
    "        user=\"neo4j\", \n",
    "        password=\"your_password\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # 2. Cargar datos reales\n",
    "        print(\"2. Cargando datos desde Neo4j...\")\n",
    "        raw_data = neo4j_loader.load_fraud_graph_data()\n",
    "        print(f\"   ✅ Cargados {len(raw_data['nodes'])} nodos y {len(raw_data['edges'])} relaciones\")\n",
    "        \n",
    "        # 3. Procesar datos para GNN\n",
    "        print(\"3. Procesando datos para GNN...\")\n",
    "        processor = FraudGraphDataProcessor()\n",
    "        graph_data = processor.create_graph_data(raw_data)\n",
    "        print(f\"   ✅ Grafo creado: {graph_data.num_nodes} nodos, {graph_data.num_edges} bordes\")\n",
    "        \n",
    "        # 4. Entrenar modelo (si no existe)\n",
    "        model_path = 'production_fraud_model.pth'\n",
    "        if not os.path.exists(model_path):\n",
    "            print(\"4. Entrenando modelo GNN...\")\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            model = FraudGNN(\n",
    "                input_dim=graph_data.num_node_features,\n",
    "                hidden_dim=128,\n",
    "                num_classes=3,\n",
    "                model_type='GAT'  # Usar GAT para mejor rendimiento\n",
    "            )\n",
    "            \n",
    "            trainer = FraudTrainer(model, device)\n",
    "            \n",
    "            # División de datos\n",
    "            num_nodes = graph_data.num_nodes\n",
    "            indices = torch.randperm(num_nodes)\n",
    "            train_size = int(0.7 * num_nodes)\n",
    "            val_size = int(0.15 * num_nodes)\n",
    "            \n",
    "            train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "            val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "            test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "            \n",
    "            train_mask[indices[:train_size]] = True\n",
    "            val_mask[indices[train_size:train_size + val_size]] = True\n",
    "            test_mask[indices[train_size + val_size:]] = True\n",
    "            \n",
    "            # Entrenar\n",
    "            graph_data = graph_data.to(device)\n",
    "            history = trainer.train(graph_data, train_mask, val_mask)\n",
    "            \n",
    "            # Guardar modelo\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"   ✅ Modelo entrenado y guardado en {model_path}\")\n",
    "        \n",
    "        # 5. Configurar monitoreo en tiempo real\n",
    "        print(\"5. Configurando monitoreo en tiempo real...\")\n",
    "        monitor = RealTimeFraudMonitor(model_path, neo4j_loader)\n",
    "        \n",
    "        # 6. Ejecutar detección inicial\n",
    "        print(\"6. Ejecutando detección de fraudes...\")\n",
    "        alerts = monitor.check_new_measurements(hours_back=48)\n",
    "        print(f\"   ⚠️  {len(alerts)} alertas de fraude detectadas\")\n",
    "        \n",
    "        for alert in alerts[:5]:  # Mostrar primeras 5\n",
    "            print(f\"   🚨 {alert['nis_rad']}: {alert['fraud_probability']:.3f} \"\n",
    "                  f\"({alert['priority']}) - {alert['trigger']}\")\n",
    "        \n",
    "        # 7. Generar reporte\n",
    "        print(\"7. Generando reporte diario...\")\n",
    "        daily_report = monitor.generate_daily_report()\n",
    "        print(f\"   📊 Total alertas: {daily_report['total_alertas']}\")\n",
    "        print(f\"   🔴 Alta prioridad: {daily_report['alertas_alta_prioridad']}\")\n",
    "        print(f\"   🟡 Media prioridad: {daily_report['alertas_media_prioridad']}\")\n",
    "        \n",
    "        print(\"\\n✅ PIPELINE COMPLETADO EXITOSAMENTE\")\n",
    "        return {\n",
    "            'alerts': alerts,\n",
    "            'report': daily_report,\n",
    "            'model_path': model_path,\n",
    "            'monitor': monitor\n",
    "        }\n",
    "        \n",
    "    finally:\n",
    "        neo4j_loader.close()\n",
    "\n",
    "# Para ejecutar en producción:\n",
    "# results = run_production_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed55d8b5-bda4-4de7-a658-6848249d6ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
